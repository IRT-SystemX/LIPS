{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to create an augmented simulator and how to train it. \n",
    "\n",
    "As always on these notebooks, we use the `Benchmark1` to demonstrate how to perform this task.\n",
    "\n",
    "On the first section, we explain how to use a model that is already available on this reposotiry. The second section is dedicated to the explanation of what is needed to create a different kind of `AugmentedSimulator` with a different Neural Network archiecture with a customized loss etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the benchmark dataset\n",
    "\n",
    "As always the first step is always to load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lips.neurips_benchmark import NeuripsBenchmark1\n",
    "path_benchmark = os.path.join(\"reference_data\")\n",
    "neurips_benchmark1 = NeuripsBenchmark1(path_benchmark=path_benchmark,\n",
    "                                       load_data_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune an available model\n",
    "\n",
    "In this section we explain how to tune an available model. We take the example of the `FullyConnectedAS` that is an available fully connected neural network.\n",
    "\n",
    "This section supposes that you already have a \"model\" (for example based on neural networks) that meets the `AugmentedSimulator` interface. If you do not have it already, the next section will cover the main principles.\n",
    "\n",
    "**NB** The creation of the 'augmented_simulator' depends on each type of 'augmented_simulator'. \n",
    "\n",
    "The first step is to create the class you want to use, with the meta parameters you want to test. For this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = os.path.join(\"trained_models\")\n",
    "if not os.path.exists(path_save):\n",
    "    os.mkdir(path_save)\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from lips.augmented_simulators import FullyConnectedAS\n",
    "\n",
    "# the three lines bellow might be familiar to the tensorflow users. They tell tensorflow to not take all\n",
    "# the GPU video RAM for the model.\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for el in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(el, True)\n",
    "\n",
    "my_simulator = FullyConnectedAS(name=\"test_FullyConnectedAS\",\n",
    "                                # `attr_x` represents the variables of the dataset you want to use to predict \n",
    "                                # the output.\n",
    "                                attr_x=(\"prod_p\", \"prod_v\", \"load_p\", \"load_q\", \"line_status\", \"topo_vect\"),\n",
    "                                # `attr_y` represents the variables of the dataset you want to predict\n",
    "                                # we predict everything needed, you can try to change them if you want, add some \n",
    "                                # others etc.\n",
    "                                attr_y=(\"a_or\", \"a_ex\", \"p_or\", \"p_ex\", \"q_or\", \"q_ex\", \"prod_q\", \"load_v\", \"v_or\", \"v_ex\"),\n",
    "                                # `sizes_layer` represents the size of each hidden layer in the neural network. The number\n",
    "                                # of layers is determined by the length of this list, for example\n",
    "                                sizes_layer=(200, 200, 200),\n",
    "                                # `lr` is the learning rate\n",
    "                                lr=3e-4, \n",
    "                                # `layer` is the type of keras layer you want to use. We don't recommend to \n",
    "                                # change it\n",
    "                                layer=Dense,\n",
    "                                # `layer_act` is the activation function you want to use after each layer\n",
    "                                layer_act=\"relu\",\n",
    "                                # `loss` is the training loss\n",
    "                                loss=\"mse\",  # loss used to train the model\n",
    "                                # `batch_size` is the size of the batch for training\n",
    "                                batch_size=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to train it. For example here we will train it for 200 epochs.\n",
    "\n",
    "**NB** You are responsible to use the correct dataset for training your model ! You can make experiments by training on the `test` set or on the `test_ood_topo` set if you want but we don't recommend you do to so !\n",
    "\n",
    "**NB** This code is generic and should work for all `AugementedSimulator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1404\n",
      "Epoch 2/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0136\n",
      "Epoch 3/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0079\n",
      "Epoch 4/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0056\n",
      "Epoch 5/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0044\n",
      "Epoch 6/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0036\n",
      "Epoch 7/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0030\n",
      "Epoch 8/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0026\n",
      "Epoch 9/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0023\n",
      "Epoch 10/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0021\n",
      "Epoch 11/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0019\n",
      "Epoch 12/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0017\n",
      "Epoch 13/200\n",
      "782/782 [==============================] - 1s 836us/step - loss: 0.0016\n",
      "Epoch 14/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0015\n",
      "Epoch 15/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0014\n",
      "Epoch 16/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0013\n",
      "Epoch 17/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0012\n",
      "Epoch 18/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0012\n",
      "Epoch 19/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0011\n",
      "Epoch 20/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0011\n",
      "Epoch 21/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0010\n",
      "Epoch 22/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 9.8675e-04\n",
      "Epoch 23/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 9.6524e-04\n",
      "Epoch 24/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 9.3391e-04\n",
      "Epoch 25/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 8.9422e-04\n",
      "Epoch 26/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 8.8237e-04\n",
      "Epoch 27/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 8.4460e-04\n",
      "Epoch 28/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 8.2857e-04\n",
      "Epoch 29/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 8.1561e-04\n",
      "Epoch 30/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 7.9513e-04\n",
      "Epoch 31/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 7.6786e-04\n",
      "Epoch 32/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 7.5376e-04\n",
      "Epoch 33/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 7.3683e-04\n",
      "Epoch 34/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 7.4053e-04\n",
      "Epoch 35/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 7.1006e-04\n",
      "Epoch 36/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 6.9784e-04\n",
      "Epoch 37/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 6.9338e-04\n",
      "Epoch 38/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 6.7334e-04\n",
      "Epoch 39/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 6.6233e-04\n",
      "Epoch 40/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 6.5308e-04\n",
      "Epoch 41/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 6.3960e-04\n",
      "Epoch 42/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 6.3764e-04\n",
      "Epoch 43/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 6.2287e-04\n",
      "Epoch 44/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 6.1882e-04\n",
      "Epoch 45/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 6.0126e-04\n",
      "Epoch 46/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 6.0371e-04\n",
      "Epoch 47/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.8063e-04\n",
      "Epoch 48/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.8881e-04\n",
      "Epoch 49/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.6677e-04\n",
      "Epoch 50/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.8196e-04\n",
      "Epoch 51/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.5339e-04\n",
      "Epoch 52/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.5476e-04\n",
      "Epoch 53/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.5236e-04\n",
      "Epoch 54/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.3744e-04\n",
      "Epoch 55/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.3624e-04\n",
      "Epoch 56/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.2672e-04\n",
      "Epoch 57/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.2559e-04\n",
      "Epoch 58/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.2332e-04\n",
      "Epoch 59/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.0806e-04\n",
      "Epoch 60/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.0772e-04\n",
      "Epoch 61/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.9387e-04\n",
      "Epoch 62/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.9708e-04\n",
      "Epoch 63/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 5.0372e-04\n",
      "Epoch 64/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.8197e-04\n",
      "Epoch 65/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.9133e-04\n",
      "Epoch 66/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.7358e-04\n",
      "Epoch 67/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.7306e-04\n",
      "Epoch 68/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.7924e-04\n",
      "Epoch 69/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.7263e-04\n",
      "Epoch 70/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.6396e-04\n",
      "Epoch 71/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.5089e-04\n",
      "Epoch 72/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.7838e-04\n",
      "Epoch 73/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.4712e-04\n",
      "Epoch 74/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.4390e-04\n",
      "Epoch 75/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.4481e-04\n",
      "Epoch 76/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.3885e-04\n",
      "Epoch 77/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.3965e-04\n",
      "Epoch 78/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.2979e-04\n",
      "Epoch 79/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.3850e-04\n",
      "Epoch 80/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.2449e-04\n",
      "Epoch 81/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.2389e-04\n",
      "Epoch 82/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.6170e-04\n",
      "Epoch 83/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.0804e-04\n",
      "Epoch 84/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.0765e-04\n",
      "Epoch 85/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.1595e-04\n",
      "Epoch 86/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.1857e-04\n",
      "Epoch 87/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.0626e-04\n",
      "Epoch 88/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.0570e-04\n",
      "Epoch 89/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.9840e-04\n",
      "Epoch 90/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.0350e-04\n",
      "Epoch 91/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 4.0407e-04\n",
      "Epoch 92/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.9303e-04\n",
      "Epoch 93/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.8932e-04\n",
      "Epoch 94/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.8930e-04\n",
      "Epoch 95/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.8607e-04\n",
      "Epoch 96/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 1ms/step - loss: 3.8773e-04\n",
      "Epoch 97/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.7908e-04\n",
      "Epoch 98/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.8917e-04\n",
      "Epoch 99/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.6859e-04\n",
      "Epoch 100/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.7683e-04\n",
      "Epoch 101/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.7338e-04\n",
      "Epoch 102/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.7450e-04\n",
      "Epoch 103/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.6917e-04\n",
      "Epoch 104/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.6679e-04\n",
      "Epoch 105/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.6586e-04\n",
      "Epoch 106/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.6306e-04\n",
      "Epoch 107/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.5732e-04\n",
      "Epoch 108/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.5917e-04\n",
      "Epoch 109/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.6332e-04\n",
      "Epoch 110/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.5505e-04\n",
      "Epoch 111/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.5926e-04\n",
      "Epoch 112/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.5524e-04\n",
      "Epoch 113/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.5491e-04\n",
      "Epoch 114/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.4318e-04\n",
      "Epoch 115/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.4629e-04\n",
      "Epoch 116/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.4697e-04\n",
      "Epoch 117/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.4345e-04\n",
      "Epoch 118/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.4076e-04\n",
      "Epoch 119/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.3703e-04\n",
      "Epoch 120/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.4838e-04\n",
      "Epoch 121/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.4036e-04\n",
      "Epoch 122/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.3797e-04\n",
      "Epoch 123/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.3239e-04\n",
      "Epoch 124/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.3734e-04\n",
      "Epoch 125/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.2834e-04\n",
      "Epoch 126/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.2609e-04\n",
      "Epoch 127/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.2745e-04\n",
      "Epoch 128/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.3066e-04\n",
      "Epoch 129/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.2357e-04\n",
      "Epoch 130/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.2919e-04\n",
      "Epoch 131/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.2118e-04\n",
      "Epoch 132/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.2457e-04\n",
      "Epoch 133/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.1272e-04\n",
      "Epoch 134/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.2309e-04\n",
      "Epoch 135/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.1392e-04\n",
      "Epoch 136/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.1368e-04\n",
      "Epoch 137/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.1283e-04\n",
      "Epoch 138/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.1225e-04\n",
      "Epoch 139/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.1811e-04\n",
      "Epoch 140/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.0553e-04\n",
      "Epoch 141/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.1149e-04\n",
      "Epoch 142/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.0536e-04\n",
      "Epoch 143/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.0516e-04\n",
      "Epoch 144/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.9965e-04\n",
      "Epoch 145/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.0492e-04\n",
      "Epoch 146/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.0699e-04\n",
      "Epoch 147/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.9906e-04\n",
      "Epoch 148/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.0184e-04\n",
      "Epoch 149/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.0237e-04\n",
      "Epoch 150/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.9760e-04\n",
      "Epoch 151/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.9846e-04\n",
      "Epoch 152/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.1336e-04\n",
      "Epoch 153/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.8396e-04\n",
      "Epoch 154/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.9666e-04\n",
      "Epoch 155/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.8979e-04\n",
      "Epoch 156/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.9089e-04\n",
      "Epoch 157/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.9086e-04\n",
      "Epoch 158/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.8079e-04\n",
      "Epoch 159/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.9476e-04\n",
      "Epoch 160/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.8105e-04\n",
      "Epoch 161/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.8910e-04\n",
      "Epoch 162/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.8299e-04\n",
      "Epoch 163/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.8312e-04\n",
      "Epoch 164/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.8202e-04\n",
      "Epoch 165/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.8477e-04\n",
      "Epoch 166/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.7638e-04\n",
      "Epoch 167/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.7577e-04\n",
      "Epoch 168/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.8301e-04\n",
      "Epoch 169/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.8545e-04\n",
      "Epoch 170/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.7827e-04\n",
      "Epoch 171/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.7495e-04\n",
      "Epoch 172/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.7642e-04\n",
      "Epoch 173/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.7320e-04\n",
      "Epoch 174/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.7875e-04\n",
      "Epoch 175/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.7009e-04\n",
      "Epoch 176/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.7303e-04\n",
      "Epoch 177/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.7146e-04\n",
      "Epoch 178/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.7229e-04\n",
      "Epoch 179/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.6893e-04\n",
      "Epoch 180/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.6523e-04\n",
      "Epoch 181/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.7280e-04\n",
      "Epoch 182/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.6431e-04\n",
      "Epoch 183/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.6651e-04\n",
      "Epoch 184/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.6762e-04\n",
      "Epoch 185/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.6119e-04\n",
      "Epoch 186/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.6412e-04\n",
      "Epoch 187/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.5857e-04\n",
      "Epoch 188/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.6967e-04\n",
      "Epoch 189/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 1ms/step - loss: 2.6632e-04\n",
      "Epoch 190/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.5667e-04\n",
      "Epoch 191/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.5660e-04\n",
      "Epoch 192/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.6129e-04\n",
      "Epoch 193/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.5635e-04\n",
      "Epoch 194/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.5506e-04\n",
      "Epoch 195/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.6178e-04\n",
      "Epoch 196/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.5137e-04\n",
      "Epoch 197/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.5355e-04\n",
      "Epoch 198/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.5359e-04\n",
      "Epoch 199/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.6203e-04\n",
      "Epoch 200/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.5214e-04\n"
     ]
    }
   ],
   "source": [
    "my_simulator.train(neurips_benchmark1.train_dataset, nb_iter=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then you can save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_simulator.save(path_save)\n",
    "my_simulator.save_metadata(path_save)\n",
    "# TODO code \"save\", \"save_weights\" and \"save_metadata\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or evaluate it on the test dataset as in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/Documents/powerflow_klu/lightsim2grid/_aux_add_trafo.py:33: UserWarning: There were some Nan in the pp_net.trafo[\"tap_neutral\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_neutral\\\"], they have been replaced by 0\")\n",
      "/home/benjamin/Documents/powerflow_klu/lightsim2grid/_aux_add_trafo.py:41: UserWarning: There were some Nan in the pp_net.trafo[\"tap_step_percent\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_step_percent\\\"], they have been replaced by 0\")\n",
      "/home/benjamin/Documents/powerflow_klu/lightsim2grid/_aux_add_trafo.py:46: UserWarning: There were some Nan in the pp_net.trafo[\"tap_pos\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_pos\\\"], they have been replaced by 0\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Basic verifier *************\n",
      "2.859% of lines does not respect the positivity of currents (Amp) at origin\n",
      "Concerned lines with corresponding number of negative current values at their origin:\n",
      " {3: 1916, 0: 515, 12: 397, 8: 388, 15: 362, 13: 323, 10: 322, 7: 312, 6: 273, 14: 199, 19: 181, 16: 124, 9: 106, 5: 97, 17: 66, 2: 55, 1: 48, 11: 29, 4: 6}\n",
      "----------------------------------------------\n",
      "2.782% of lines does not respect the positivity of currents (Amp) at extremity\n",
      "Concerned lines with corresponding number of negative current values at their extremity:\n",
      " {3: 1861, 0: 478, 8: 392, 12: 384, 13: 346, 15: 346, 7: 329, 10: 307, 14: 258, 6: 257, 19: 139, 16: 106, 5: 85, 9: 84, 17: 61, 1: 39, 2: 38, 11: 29, 4: 25}\n",
      "----------------------------------------------\n",
      "3.424% of lines does not respect the positivity of voltages (Kv) at origin\n",
      "Concerned lines with corresponding number of negative voltage values at their origin:\n",
      " {3: 2032, 5: 553, 0: 476, 15: 405, 4: 356, 17: 331, 9: 301, 13: 291, 19: 286, 16: 276, 10: 262, 11: 248, 14: 243, 12: 203, 8: 189, 1: 150, 7: 127, 2: 70, 6: 48}\n",
      "----------------------------------------------\n",
      "3.479% of lines does not respect the positivity of voltages (Kv) at extremity\n",
      "Concerned lines with corresponding number of negative voltage values at their extremity:\n",
      " {3: 2070, 5: 513, 0: 486, 4: 418, 15: 401, 17: 356, 14: 313, 13: 307, 19: 306, 9: 299, 10: 291, 16: 246, 12: 202, 8: 202, 11: 187, 1: 125, 7: 108, 2: 68, 6: 59}\n",
      "----------------------------------------------\n",
      "21.957% of lines does not respect the positivity of loss (Mw)\n",
      "Concerned lines with corresponding number of negative loss values:\n",
      " {19: 6097, 13: 5391, 6: 4544, 4: 3885, 17: 3837, 18: 3694, 15: 3604, 16: 3506, 3: 1890, 10: 1740, 12: 1266, 1: 986, 14: 807, 2: 697, 11: 525, 5: 346, 0: 326, 9: 296, 7: 240, 8: 236}\n",
      "----------------------------------------------\n",
      "Prediction in presence of line disconnection. Problem encountered !\n",
      "----------------------------------------------\n",
      "************* Check loss *************\n",
      "Number of failed cases is 841 and the proportion is 8.410% : \n",
      "************* Check Energy Conservation *************\n",
      "Number of failed cases is 9983 and the proportion is 99.830% : \n",
      "************* Check kirchhoff's current law *************\n",
      "93.42% not verify the Kirchhoff's current law at 0.01 tolerance\n",
      "************* Basic verifier *************\n",
      "4.109% of lines does not respect the positivity of currents (Amp) at origin\n",
      "Concerned lines with corresponding number of negative current values at their origin:\n",
      " {3: 1085, 13: 574, 4: 570, 5: 562, 6: 557, 11: 551, 14: 520, 2: 463, 1: 446, 17: 445, 10: 387, 0: 378, 7: 331, 16: 289, 8: 261, 12: 240, 9: 233, 19: 211, 15: 115}\n",
      "----------------------------------------------\n",
      "4.184% of lines does not respect the positivity of currents (Amp) at extremity\n",
      "Concerned lines with corresponding number of negative current values at their extremity:\n",
      " {3: 1087, 1: 610, 14: 584, 4: 579, 13: 577, 11: 550, 6: 535, 5: 506, 2: 468, 17: 460, 0: 386, 10: 350, 7: 313, 16: 284, 8: 261, 9: 240, 12: 236, 19: 228, 15: 115}\n",
      "----------------------------------------------\n",
      "1.918% of lines does not respect the positivity of voltages (Kv) at origin\n",
      "Concerned lines with corresponding number of negative voltage values at their origin:\n",
      " {3: 1856, 1: 254, 17: 239, 5: 176, 0: 175, 14: 162, 16: 151, 4: 128, 7: 128, 6: 112, 15: 98, 10: 95, 19: 63, 9: 45, 13: 40, 11: 38, 8: 37, 12: 28, 2: 10}\n",
      "----------------------------------------------\n",
      "1.888% of lines does not respect the positivity of voltages (Kv) at extremity\n",
      "Concerned lines with corresponding number of negative voltage values at their extremity:\n",
      " {3: 1841, 1: 253, 17: 221, 0: 173, 5: 170, 14: 163, 4: 141, 16: 115, 7: 113, 6: 99, 15: 95, 10: 92, 19: 63, 12: 54, 8: 53, 9: 45, 13: 41, 11: 35, 2: 10}\n",
      "----------------------------------------------\n",
      "34.640% of lines does not respect the positivity of loss (Mw)\n",
      "Concerned lines with corresponding number of negative loss values:\n",
      " {16: 6056, 6: 5736, 5: 4881, 17: 4864, 18: 4292, 15: 4195, 4: 4157, 10: 4140, 19: 3981, 13: 3738, 2: 3665, 14: 2635, 8: 2572, 0: 2546, 11: 2492, 1: 2190, 3: 2028, 12: 2000, 9: 1713, 7: 1399}\n",
      "----------------------------------------------\n",
      "Prediction in presence of line disconnection. Problem encountered !\n",
      "----------------------------------------------\n",
      "************* Check loss *************\n",
      "Number of failed cases is 2179 and the proportion is 21.790% : \n",
      "************* Check Energy Conservation *************\n",
      "Number of failed cases is 9997 and the proportion is 99.970% : \n",
      "************* Check kirchhoff's current law *************\n",
      "99.26% not verify the Kirchhoff's current law at 0.01 tolerance\n"
     ]
    }
   ],
   "source": [
    "metrics_per_dataset = neurips_benchmark1.evaluate_augmented_simulator(my_simulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once saved, if you want to reuse it you can do exactly as we did in the previous notebook:\n",
    "```python\n",
    "relaoded_simulator = FullyConnectedAS(name=\"test_FullyConnectedAS\")  # the name should match! The other things are loaded from the directory\n",
    "relaoded_simulator.load_metadata(path_baselines)\n",
    "relaoded_simulator.init()\n",
    "relaoded_simulator.restore(path_baselines)\n",
    "```\n",
    "\n",
    "And you are good to go !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code another type of \"augmented_simulator\"\n",
    "\n",
    "We provide only a single type of augmented simulator as of now. More baselines will be added with, we hope the growth of the community.\n",
    "\n",
    "Coding another type of \"augmented simulator\" is not difficult. Finding one that work well for all the criteria is of course a different challenge.\n",
    "\n",
    "Basically, an augmented simulator should:\n",
    "\n",
    "- inherit from `AugmentedSimulator`\n",
    "- implements the methods `save_metadata` and `load_metadata`\n",
    "- implements the methods `save` and `restore`\n",
    "- implements the method `init` that can for example, create the neural network from its meta parameters\n",
    "- implements the method `train` that will train the augmented simulator for a given number of steps (it is not mandatory to make such function, some `AugmentedSimulator` might not require any training at all)\n",
    "- implements the method `predict` method that will make some predictions and return a dictionnary containing the predictions for all variable types.\n",
    "\n",
    "More information is given on the documentation. And a fully working example is given in the `FullyConnectedAS` class.\n",
    "\n",
    "This is it, nothing more is required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
