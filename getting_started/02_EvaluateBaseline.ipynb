{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to demonstrate how we can evaluate the results of a baseline on a given benchmark.\n",
    "\n",
    "It will be split into two part. The first part will focus on the evaluation of a baseline that does not requires any training (the `DCApproximatrionAC`). On the second part, we will show how to load a baseline (or any other `AugmentedSimulator`) and evaluate it on a `Benchmark` of our choice.\n",
    "\n",
    "As for the first notebook, we demonstrate this capability for the case of `NeuripsBenchmark1`.\n",
    "\n",
    "**NB** This notebook supposes that the data for the benchmark are already available. If they are not, please generate them or download them.\n",
    "\n",
    "**NB** The `DCApproximatrionAC` baseline requires the `grid2op` python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pprint import pprint\n",
    "from lips.benchmark.powergridBenchmark import PowerGridBenchmark\n",
    "from lips.utils import get_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate required paths\n",
    "LIPS_PATH = pathlib.Path().resolve().parent\n",
    "DATA_PATH = LIPS_PATH / \"reference_data\" / \"powergrid\" / \"l2rpn_case14_sandbox\"\n",
    "BENCH_CONFIG_PATH = LIPS_PATH / \"configurations\" / \"powergrid\" / \"benchmarks\" / \"l2rpn_case14_sandbox.ini\"\n",
    "SIM_CONFIG_PATH = LIPS_PATH / \"configurations\" / \"powergrid\" / \"simulators\"\n",
    "BASELINES_PATH = LIPS_PATH / \"trained_baselines\" / \"powergrid\"\n",
    "EVALUATION_PATH = LIPS_PATH / \"evaluation_results\" / \"PowerGrid\"\n",
    "LOG_PATH = LIPS_PATH / \"lips_logs.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Benchmark1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial step: load the dataset\n",
    "\n",
    "A common dataset will be used for evaluate the two augmented simulator. This initial step aims at loading it once and for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark1 = PowerGridBenchmark(benchmark_name=\"Benchmark1\",\n",
    "                                benchmark_path=DATA_PATH,\n",
    "                                load_data_set=True,\n",
    "                                log_path=LOG_PATH,\n",
    "                                config_path=BENCH_CONFIG_PATH\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark name:  Benchmark1\n",
      "Environment name:  l2rpn_case14_sandbox\n",
      "Output attributes:  ('a_or', 'a_ex')\n",
      "Evaluation criteria: \n",
      "{'IndRed': ['TIME_INF'],\n",
      " 'ML': ['MSE_avg', 'MAE_avg', 'mape_90_avg', 'TIME_INF'],\n",
      " 'OOD': ['MSE_avg', 'MAE_avg', 'mape_90_avg', 'TIME_INF'],\n",
      " 'Physics': ['CURRENT_POS']}\n"
     ]
    }
   ],
   "source": [
    "# to verify the config is loaded appropriately for this benchmark\n",
    "print(\"Benchmark name: \", benchmark1.config.section_name)\n",
    "print(\"Environment name: \", benchmark1.config.get_option(\"env_name\"))\n",
    "print(\"Output attributes: \", benchmark1.config.get_option(\"attr_y\"))\n",
    "print(\"Evaluation criteria: \")\n",
    "pprint(benchmark1.config.get_option(\"eval_dict\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DC approximation\n",
    "\n",
    "We remind that the `grid2op` library is required for this part. You can install it with `pip install grid2op` if you do not have it already.\n",
    "\n",
    "First we will create the \"augmented simulator\". As opposed to the second model we will expose here, this method require access to a powergrid. This is one of the reason we need grid2op. \n",
    "\n",
    "The way to load each `AugmentedSimulator` is specific. Here for example we load the DCApproximation that will use the same powergrid as the one used to generate the data in the previous Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next few lines are specific for each benchmark and each `AugmentedSimulator`\n",
    "import grid2op\n",
    "import warnings\n",
    "from lips.physical_simulator.dcApproximationAS import DCApproximationAS\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    env = grid2op.make(benchmark1.config.get_option(\"env_name\"), test=True)\n",
    "    grid_path = pathlib.Path(env.get_path_env()) / \"grid.json\"\n",
    "\n",
    "dc_sim = DCApproximationAS(name=\"dc_approximation\", \n",
    "                           benchmark_name=\"Benchmark1\",\n",
    "                           config_path=BENCH_CONFIG_PATH,\n",
    "                           grid_path=grid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is load, there is a common interface to evaluate its performance, on a dataset. This is showed in the cell bellow where we evaluate a physics based simulator `DCApproximation` on these two dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluate dc: 100%|█████████████████████████████████████████████████████| 10000/10000 [03:23<00:00, 49.26it/s]\n",
      "evaluate dc: 100%|█████████████████████████████████████████████████████| 10000/10000 [03:21<00:00, 49.59it/s]\n",
      "evaluate dc: 100%|█████████████████████████████████████████████████████| 10000/10000 [03:18<00:00, 50.36it/s]\n"
     ]
    }
   ],
   "source": [
    "EVAL_SAVE_PATH = get_path(EVALUATION_PATH, benchmark1)\n",
    "dc_metrics_per_dataset = benchmark1.evaluate_simulator(augmented_simulator=dc_sim,\n",
    "                                                       dataset=\"all\", # other values : \"val\", \"test\", \"test_ood_topo\"\n",
    "                                                       save_path=EVAL_SAVE_PATH,\n",
    "                                                       save_predictions=True\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-related performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE90 for a_or:  0.1408511172541828\n",
      "MAPE90 for a_ex:  0.13240412951825123\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPE90 for a_or: \", dc_metrics_per_dataset[\"test\"][\"ML\"][\"mape_90_avg\"][\"a_or\"])\n",
    "print(\"MAPE90 for a_ex: \", dc_metrics_per_dataset[\"test\"][\"ML\"][\"mape_90_avg\"][\"a_ex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOD Generalization evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE90 for a_or:  0.1608534769924919\n",
      "MAPE90 for a_ex:  0.149238631980091\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPE90 for a_or: \", dc_metrics_per_dataset[\"test_ood_topo\"][\"ML\"][\"mape_90_avg\"][\"a_or\"])\n",
    "print(\"MAPE90 for a_ex: \", dc_metrics_per_dataset[\"test_ood_topo\"][\"ML\"][\"mape_90_avg\"][\"a_ex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics compliances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CURRENT_POS'])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_metrics_per_dataset[\"test\"][\"Physics\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : when there is no violation, create although the key and force it to take value zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Current positivity violation: {}\n"
     ]
    }
   ],
   "source": [
    "print(\"1) Current positivity violation:\", dc_metrics_per_dataset[\"test\"][\"Physics\"][\"CURRENT_POS\"])#[\"a_or\"][\"Violation_proportion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOD Generalization Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Current positivity violation: {}\n"
     ]
    }
   ],
   "source": [
    "print(\"1) Current positivity violation:\", dc_metrics_per_dataset[\"test_ood_topo\"][\"Physics\"][\"CURRENT_POS\"])#[\"a_or\"][\"Violation_proportion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it is possible to study the metrics on the different dataset. For example, if we want the \"MSE\" error on the \"test\" dataset (with a similar distribution as the training one):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A learned baseline \"augmented simulator\"\n",
    "\n",
    "Along with some dataset, we provide also some baseline (from a trained neural network). This baseline is made of a fully connected neural network that takes the available input of the powergrid and tries to predict all the output of the simulator.\n",
    "\n",
    "The fully connected neural network is made of XXX layer each with YYY units.\n",
    "\n",
    "It is learned for KKK epochs on the training set of the `Benchmark1`.\n",
    "\n",
    "**NB** These baselines are not yet fully trained, and some hyper parameters still need to be optimized. We intend on doing that before the official release of the benchmark for the Neurips conference.\n",
    "\n",
    "First we need to load the baseline and initialize it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.augmented_simulators.tensorflow_models import TfFullyConnected\n",
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "# rebuild the baseline architecture\n",
    "tf_fc = TfFullyConnected(name=\"tf_fc\",\n",
    "                         bench_config_path=BENCH_CONFIG_PATH,\n",
    "                         bench_config_name=\"Benchmark1\",\n",
    "                         sim_config_path=SIM_CONFIG_PATH / \"tf_fc.ini\",\n",
    "                         sim_config_name=\"DEFAULT\",\n",
    "                         scaler=StandardScaler,\n",
    "                         log_path=LOG_PATH)\n",
    "\n",
    "LOAD_PATH = get_path(BASELINES_PATH, benchmark1)\n",
    "tf_fc.restore(LOAD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, as for the DC approximation, we can evaluate it on the test datasets of the benchmark.\n",
    "\n",
    "This is done with the same command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-14 19:11:16.036827: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n"
     ]
    }
   ],
   "source": [
    "# TODO: log the losses\n",
    "# EVAL_SAVE_PATH = get_path(EVALUATION_PATH, benchmark1)\n",
    "tf_fc_metrics = benchmark1.evaluate_simulator(augmented_simulator=tf_fc,\n",
    "                                              eval_batch_size=10000,\n",
    "                                              dataset=\"all\",\n",
    "                                              shuffle=False,\n",
    "                                              save_path=None,\n",
    "                                              save_predictions=False\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the two augmented simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can compare the two \"augmented simulators\". For example, if we want to compare the MAPE90 (mean absolute percentage error compute for last 10% quantile) on the test dataset (with a distribution similar to the training distribution) for currents (A) at two extremity of power lines, we might compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC Approximation\n",
      "Dataset : test\n",
      "MAPE90     : {'a_or': 0.2054102762347278, 'a_ex': 0.18943869874874716}\n",
      "MSE_avg    : {'a_or': 91723.53006148033, 'a_ex': 131732.91079220167}\n",
      "MAE_avg    : {'a_or': 114.3059446624192, 'a_ex': 143.16579138645758}\n",
      "Dataset : test_ood_topo\n",
      "mape_90_avg : {'a_or': 0.2227085624215881, 'a_ex': 0.20711185760666956}\n",
      "MSE_avg    : {'a_or': 107658.54513807358, 'a_ex': 155649.85692985042}\n",
      "MAE_avg    : {'a_or': 123.67567242785628, 'a_ex': 155.63737545351296}\n"
     ]
    }
   ],
   "source": [
    "ML_metrics = \"ML\"\n",
    "dataset_name = \"test\"\n",
    "print(\"DC Approximation\")\n",
    "print(f\"Dataset : {dataset_name}\")\n",
    "print(\"{:<10} : {}\".format(\"MAPE90\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"mape_90_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MSE_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"MSE_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MAE_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"MAE_avg\"]))\n",
    "dataset_name = \"test_ood_topo\"\n",
    "print(f\"Dataset : {dataset_name}\")\n",
    "print(\"{:<10} : {}\".format(\"mape_90_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"mape_90_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MSE_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"MSE_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MAE_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"MAE_avg\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully Connected Augmented Simulator\n",
      "Dataset : test\n",
      "mape_90_avg : {'a_or': 0.005971284397153569, 'a_ex': 0.005959986222812791}\n",
      "MSE_avg    : {'a_or': 21.65915870666504, 'a_ex': 44.23374557495117}\n",
      "MAE_avg    : {'a_or': 2.7974419593811035, 'a_ex': 3.8893446922302246}\n",
      "Dataset : test_ood_topo\n",
      "mape_90_avg : {'a_or': 0.17957805279569528, 'a_ex': 0.17880120699511942}\n",
      "MSE_avg    : {'a_or': 12364.6455078125, 'a_ex': 23608.12109375}\n",
      "MAE_avg    : {'a_or': 52.66007614135742, 'a_ex': 74.55868530273438}\n"
     ]
    }
   ],
   "source": [
    "ML_metrics = \"ML\"\n",
    "dataset_name = \"test\"\n",
    "print(\"Fully Connected Augmented Simulator\")\n",
    "print(f\"Dataset : {dataset_name}\")\n",
    "print(\"{:<10} : {}\".format(\"mape_90_avg\", tf_fc_metrics[dataset_name][ML_metrics][\"mape_90_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MSE_avg\", tf_fc_metrics[dataset_name][ML_metrics][\"MSE_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MAE_avg\", tf_fc_metrics[dataset_name][ML_metrics][\"MAE_avg\"]))\n",
    "dataset_name = \"test_ood_topo\"\n",
    "print(f\"Dataset : {dataset_name}\")\n",
    "print(\"{:<10} : {}\".format(\"mape_90_avg\", tf_fc_metrics[dataset_name][ML_metrics][\"mape_90_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MSE_avg\", tf_fc_metrics[dataset_name][ML_metrics][\"MSE_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MAE_avg\", tf_fc_metrics[dataset_name][ML_metrics][\"MAE_avg\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physic compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23% of currents at the origin side of power lines violate the current positivity.\n"
     ]
    }
   ],
   "source": [
    "physic_compliances = \"Physics\"\n",
    "dataset_name = \"test\"\n",
    "current_violation = tf_fc_metrics[dataset_name][physic_compliances][\"CURRENT_POS\"][\"a_or\"][\"Violation_proportion\"]\n",
    "print(\"{:.2f}% of currents at the origin side of power lines violate the current positivity.\".format(current_violation*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of negative current values (Amp) : 0.02\n"
     ]
    }
   ],
   "source": [
    "current_error = tf_fc_metrics[dataset_name][physic_compliances][\"CURRENT_POS\"][\"a_or\"][\"Violation_proportion\"]\n",
    "print(\"The sum of negative current values (Amp) : {:.2f}\".format(current_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industrial readiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.033661603927612305"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_fc.predict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134.44738698005676"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_sim._predict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.7556830085814"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_sim._raw_grid_simulator.comp_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-16 10:33:38.986746: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-16 10:33:38.987008: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "benchmark2 = PowerGridBenchmark(benchmark_name=\"Benchmark2\",\n",
    "                                benchmark_path=DATA_PATH,\n",
    "                                load_data_set=True,\n",
    "                                log_path=LOG_PATH,\n",
    "                                config_path=BENCH_CONFIG_PATH\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark name:  Benchmark2\n",
      "Environment name:  l2rpn_case14_sandbox\n",
      "Output attributes:  ('a_or', 'a_ex', 'p_or', 'p_ex', 'v_or', 'v_ex')\n",
      "Evaluation criteria: \n",
      "{'IndRed': ['TIME_INF'],\n",
      " 'ML': ['MSE_avg', 'MAE_avg', 'mape_avg', 'mape_90_avg', 'TIME_INF'],\n",
      " 'OOD': ['MSE_avg', 'MAE_avg', 'mape_avg', 'mape_90_avg', 'TIME_INF'],\n",
      " 'Physics': ['CURRENT_POS',\n",
      "             'VOLTAGE_POS',\n",
      "             'LOSS_POS',\n",
      "             'DISC_LINES',\n",
      "             'CHECK_LOSS',\n",
      "             'CHECK_GC',\n",
      "             'CHECK_LC',\n",
      "             'CHECK_VOLTAGE_EQ']}\n"
     ]
    }
   ],
   "source": [
    "# to verify the config is loaded appropriately for this benchmark\n",
    "print(\"Benchmark name: \", benchmark2.config.section_name)\n",
    "print(\"Environment name: \", benchmark2.config.get_option(\"env_name\"))\n",
    "print(\"Output attributes: \", benchmark2.config.get_option(\"attr_y\"))\n",
    "print(\"Evaluation criteria: \")\n",
    "pprint(benchmark2.config.get_option(\"eval_dict\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next few lines are specific for each benchmark and each `AugmentedSimulator`\n",
    "import grid2op\n",
    "import warnings\n",
    "from lips.physical_simulator.dcApproximationAS import DCApproximationAS\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    env = grid2op.make(benchmark2.config.get_option(\"env_name\"), test=True)\n",
    "    grid_path = pathlib.Path(env.get_path_env()) / \"grid.json\"\n",
    "\n",
    "dc_sim = DCApproximationAS(name=\"dc_approximation\", \n",
    "                           benchmark_name=\"Benchmark2\",\n",
    "                           config_path=BENCH_CONFIG_PATH,\n",
    "                           grid_path=grid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_SAVE_PATH = get_path(EVALUATION_PATH, benchmark2)\n",
    "dc_metrics_per_dataset_bench2 = benchmark2.evaluate_simulator(augmented_simulator=dc_sim,\n",
    "                                                              dataset=\"all\", # other values : \"val\", \"test\", \"test_ood_topo\"\n",
    "                                                              save_path=EVAL_SAVE_PATH,\n",
    "                                                              save_predictions=True\n",
    "                                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-related performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE90 for a_or:  0.1575609916428136\n",
      "MAPE90 for a_ex:  0.14730832052856613\n",
      "MAPE for p_or:  0.0921423999325571\n",
      "MAPE for p_ex:  0.08787188867809301\n",
      "MAE for p_or:  1.140607839753558\n",
      "MAE for p_ex:  1.026569282669907\n",
      "MAPE for v_or:  0.020920751203534725\n",
      "MAPE for v_ex:  0.033166301338939726\n",
      "MAE for v_or:  0.6651214754772188\n",
      "MAE for v_ex:  0.9825881892442705\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPE90 for a_or: \", dc_metrics_per_dataset_bench2[\"test\"][\"ML\"][\"mape_90_avg\"][\"a_or\"])\n",
    "print(\"MAPE90 for a_ex: \", dc_metrics_per_dataset_bench2[\"test\"][\"ML\"][\"mape_90_avg\"][\"a_ex\"])\n",
    "print(\"MAPE for p_or: \", dc_metrics_per_dataset_bench2[\"test\"][\"ML\"][\"mape_avg\"][\"p_or\"])\n",
    "print(\"MAPE for p_ex: \", dc_metrics_per_dataset_bench2[\"test\"][\"ML\"][\"mape_avg\"][\"p_ex\"])\n",
    "print(\"MAE for p_or: \", dc_metrics_per_dataset_bench2[\"test\"][\"ML\"][\"MAE_avg\"][\"p_or\"])\n",
    "print(\"MAE for p_ex: \", dc_metrics_per_dataset_bench2[\"test\"][\"ML\"][\"MAE_avg\"][\"p_ex\"])\n",
    "print(\"MAPE for v_or: \", dc_metrics_per_dataset_bench2[\"test\"][\"ML\"][\"mape_avg\"][\"v_or\"])\n",
    "print(\"MAPE for v_ex: \", dc_metrics_per_dataset_bench2[\"test\"][\"ML\"][\"mape_avg\"][\"v_ex\"])\n",
    "print(\"MAE for v_or: \", dc_metrics_per_dataset_bench2[\"test\"][\"ML\"][\"MAE_avg\"][\"v_or\"])\n",
    "print(\"MAE for v_ex: \", dc_metrics_per_dataset_bench2[\"test\"][\"ML\"][\"MAE_avg\"][\"v_ex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOD Generalization evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE90 for a_or:  0.16061949592695715\n",
      "MAPE90 for a_ex:  0.15209429645818998\n",
      "MAPE for p_or:  0.08811211862205569\n",
      "MAPE for p_ex:  0.08532031022804058\n",
      "MAE for p_or:  1.1736193180608905\n",
      "MAE for p_ex:  1.0713623671817072\n",
      "MAPE for v_or:  0.021399578333099696\n",
      "MAPE for v_ex:  0.03433434072231821\n",
      "MAE for v_or:  0.7879805879878998\n",
      "MAE for v_ex:  1.119801747493744\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPE90 for a_or: \", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"ML\"][\"mape_90_avg\"][\"a_or\"])\n",
    "print(\"MAPE90 for a_ex: \", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"ML\"][\"mape_90_avg\"][\"a_ex\"])\n",
    "print(\"MAPE for p_or: \", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"ML\"][\"mape_avg\"][\"p_or\"])\n",
    "print(\"MAPE for p_ex: \", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"ML\"][\"mape_avg\"][\"p_ex\"])\n",
    "print(\"MAE for p_or: \", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"ML\"][\"MAE_avg\"][\"p_or\"])\n",
    "print(\"MAE for p_ex: \", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"ML\"][\"MAE_avg\"][\"p_ex\"])\n",
    "print(\"MAPE for v_or: \", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"ML\"][\"mape_avg\"][\"v_or\"])\n",
    "print(\"MAPE for v_ex: \", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"ML\"][\"mape_avg\"][\"v_ex\"])\n",
    "print(\"MAE for v_or: \", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"ML\"][\"MAE_avg\"][\"v_or\"])\n",
    "print(\"MAE for v_ex: \", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"ML\"][\"MAE_avg\"][\"v_ex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics compliances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CURRENT_POS', 'VOLTAGE_POS', 'LOSS_POS', 'DISC_LINES', 'CHECK_LOSS', 'CHECK_GC', 'CHECK_LC', 'CHECK_VOLTAGE_EQ'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_metrics_per_dataset_bench2[\"test\"][\"Physics\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : when there is no violation, create although the key and force it to take value zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Current positivity violation: {}\n",
      "2) Voltage positivity violation: {}\n",
      "3) Loss positivity violation: {}\n",
      "4) Disconnected lines violation: {}\n",
      "5) Violation of loss to be between [1,4]% of production: 0.0\n",
      "6) Violation of global conservation: 100.0% and its weighted mape: 0.9999998971074707\n",
      "7) Violation of local conservation: 7.142857142857142% and its weighted mape: 0.01564590303591114\n",
      "8) Violation proportion of voltage equality at subs: 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"1) Current positivity violation:\", dc_metrics_per_dataset_bench2[\"test\"][\"Physics\"][\"CURRENT_POS\"])#[\"a_or\"][\"Violation_proportion\"]\n",
    "print(\"2) Voltage positivity violation:\", dc_metrics_per_dataset_bench2[\"test\"][\"Physics\"][\"VOLTAGE_POS\"])\n",
    "print(\"3) Loss positivity violation:\", dc_metrics_per_dataset_bench2[\"test\"][\"Physics\"][\"LOSS_POS\"])\n",
    "print(\"4) Disconnected lines violation:\", dc_metrics_per_dataset_bench2[\"test\"][\"Physics\"][\"DISC_LINES\"])\n",
    "print(\"5) Violation of loss to be between [1,4]% of production:\", dc_metrics_per_dataset_bench2[\"test\"][\"Physics\"][\"CHECK_LOSS\"][\"violation_percentage\"])\n",
    "print(\"6) Violation of global conservation: {}% and its weighted mape: {}\".format(dc_metrics_per_dataset_bench2[\"test\"][\"Physics\"][\"CHECK_GC\"][\"violation_percentage\"], dc_metrics_per_dataset_bench2[\"test\"][\"Physics\"][\"CHECK_GC\"][\"wmape\"]))\n",
    "print(\"7) Violation of local conservation: {}% and its weighted mape: {}\".format(dc_metrics_per_dataset_bench2[\"test\"][\"Physics\"][\"CHECK_LC\"][\"violation_percentage\"], dc_metrics_per_dataset_bench2[\"test\"][\"Physics\"][\"CHECK_LC\"][\"mape\"]))\n",
    "print(\"8) Violation proportion of voltage equality at subs:\", dc_metrics_per_dataset_bench2[\"test\"][\"Physics\"][\"CHECK_VOLTAGE_EQ\"][0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOD Generalization Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Current positivity violation: {}\n",
      "2) Voltage positivity violation: {}\n",
      "3) Loss positivity violation: {}\n",
      "4) Disconnected lines violation: {}\n",
      "5) Violation of loss to be between [1,4]% of production: 0.0\n",
      "6) Violation of global conservation: 100.0% and its weighted mape: 1.0000000047503754\n",
      "7) Violation of local conservation: 7.142857142857142% and its weighted mape: 0.017686318493588492\n",
      "8) Violation proportion of voltage equality at subs: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"1) Current positivity violation:\", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"Physics\"][\"CURRENT_POS\"])#[\"a_or\"][\"Violation_proportion\"]\n",
    "print(\"2) Voltage positivity violation:\", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"Physics\"][\"VOLTAGE_POS\"])\n",
    "print(\"3) Loss positivity violation:\", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"Physics\"][\"LOSS_POS\"])\n",
    "print(\"4) Disconnected lines violation:\", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"Physics\"][\"DISC_LINES\"])\n",
    "print(\"5) Violation of loss to be between [1,4]% of production:\", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"Physics\"][\"CHECK_LOSS\"][\"violation_percentage\"])\n",
    "print(\"6) Violation of global conservation: {}% and its weighted mape: {}\".format(dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"Physics\"][\"CHECK_GC\"][\"violation_percentage\"], dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"Physics\"][\"CHECK_GC\"][\"wmape\"]))\n",
    "print(\"7) Violation of local conservation: {}% and its weighted mape: {}\".format(dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"Physics\"][\"CHECK_LC\"][\"violation_percentage\"], dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"Physics\"][\"CHECK_LC\"][\"mape\"]))\n",
    "print(\"8) Violation proportion of voltage equality at subs:\", dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"Physics\"][\"CHECK_VOLTAGE_EQ\"][0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industrial Readiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time for test dataset: 82.65s\n",
      "Inference time for OOD dataset: 83.23s\n"
     ]
    }
   ],
   "source": [
    "print(f'Inference time for test dataset: {dc_metrics_per_dataset_bench2[\"test\"][\"IndRed\"][\"TIME_INF\"]:.2f}s')\n",
    "print(f'Inference time for OOD dataset: {dc_metrics_per_dataset_bench2[\"test_ood_topo\"][\"IndRed\"][\"TIME_INF\"]:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.augmented_simulators.tensorflow_models import TfFullyConnected\n",
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "# rebuild the baseline architecture\n",
    "tf_fc = TfFullyConnected(name=\"tf_fc\",\n",
    "                         bench_config_path=BENCH_CONFIG_PATH,\n",
    "                         bench_config_name=\"Benchmark2\",\n",
    "                         sim_config_path=SIM_CONFIG_PATH / \"tf_fc.ini\",\n",
    "                         sim_config_name=\"DEFAULT\",\n",
    "                         scaler=StandardScaler,\n",
    "                         log_path=LOG_PATH)\n",
    "\n",
    "# LOAD_PATH = get_path(BASELINES_PATH, benchmark2)\n",
    "# tf_fc.restore(LOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc.train(train_dataset=benchmark2.train_dataset, val_dataset=benchmark2.val_dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [01:44<00:00, 95.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [01:41<00:00, 98.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [01:37<00:00, 102.52it/s]\n"
     ]
    }
   ],
   "source": [
    "tf_fc_metrics = benchmark2.evaluate_simulator(augmented_simulator=tf_fc,\n",
    "                                              eval_batch_size=10000,\n",
    "                                              dataset=\"all\",\n",
    "                                              shuffle=False,\n",
    "                                              save_path=None,\n",
    "                                              save_predictions=False\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Current positivity violation: 0.0145525\n",
      "2) Voltage positivity violation: 0.01344\n",
      "3) Loss positivity violation: 0.334525\n",
      "4) Disconnected lines violation: {}\n",
      "5) Violation of loss to be between [1,4]% of production: 2.04\n",
      "6) Violation of global conservation: 99.97% and its weighted mape: 0.32163846492767334\n",
      "7) Violation of local conservation: 98.97785714285715% and its weighted mape: 0.039185661898096955\n",
      "8) Violation proportion of voltage equality at subs: 0.995625\n"
     ]
    }
   ],
   "source": [
    "print(\"1) Current positivity violation:\", (tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"CURRENT_POS\"][\"a_or\"][\"Violation_proportion\"]+tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"CURRENT_POS\"][\"a_ex\"][\"Violation_proportion\"])/2)#[\"a_or\"][\"Violation_proportion\"]\n",
    "print(\"2) Voltage positivity violation:\", (tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"VOLTAGE_POS\"][\"v_or\"][\"Violation_proportion\"]+tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"VOLTAGE_POS\"][\"v_ex\"][\"Violation_proportion\"])/2)\n",
    "print(\"3) Loss positivity violation:\", tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"LOSS_POS\"][\"violation_proportion\"])\n",
    "print(\"4) Disconnected lines violation:\", tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"DISC_LINES\"])\n",
    "print(\"5) Violation of loss to be between [1,4]% of production:\", tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"CHECK_LOSS\"][\"violation_percentage\"])\n",
    "print(\"6) Violation of global conservation: {}% and its weighted mape: {}\".format(tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"CHECK_GC\"][\"violation_percentage\"], tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"CHECK_GC\"][\"wmape\"]))\n",
    "print(\"7) Violation of local conservation: {}% and its weighted mape: {}\".format(tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"CHECK_LC\"][\"violation_percentage\"], tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"CHECK_LC\"][\"mape\"]))\n",
    "print(\"8) Violation proportion of voltage equality at subs:\", tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"CHECK_VOLTAGE_EQ\"][0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc_values': array([[ 0.93359566, -1.87155533, -1.41511726, ...,  0.4112649 ,\n",
       "          0.44258046, -0.56777906],\n",
       "        [ 0.41781998,  0.98659134,  1.32159805, ..., -0.2284466 ,\n",
       "          0.37139964,  0.31681108],\n",
       "        [-0.86855888, -3.11248207, -0.11334896, ...,  0.65899277,\n",
       "         -0.02189493, -0.27778149],\n",
       "        ...,\n",
       "        [-0.87533188,  0.91049194,  0.57787037, ...,  0.12562943,\n",
       "          0.46206784, -0.59373856],\n",
       "        [ 1.54883957, -0.09885025, -0.13272953, ...,  0.11185193,\n",
       "         -0.23499465, -0.79366875],\n",
       "        [ 1.89224243,  0.84216881,  0.35978317, ...,  0.6466496 ,\n",
       "          1.34231901, -0.70930004]]),\n",
       " 'violation_percentage': 98.97785714285715,\n",
       " 'mae': 0.8387612063547331,\n",
       " 'mape': 0.039185661898096955}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"CHECK_LC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.41328454,  0.91084516, -1.91290545, ...,  0.33432364,\n",
       "       -0.97183847,  0.43700492])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum(tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"CHECK_LC\"][\"lc_values\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gc_values': array([ 3.4132667 ,  0.91083765, -1.9129095 , ...,  0.33430815,\n",
       "        -0.97188854,  0.43698454], dtype=float32),\n",
       " 'violation_percentage': 99.97,\n",
       " 'mae': 1.7046846,\n",
       " 'wmape': 0.32163846}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_fc_metrics[\"test_ood_topo\"][\"Physics\"][\"CHECK_GC\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lips",
   "language": "python",
   "name": "lips"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
