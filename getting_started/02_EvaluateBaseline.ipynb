{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the moment the directory should be set to the parent to be able to use the LIPS package\n",
    "import os\n",
    "os.chdir(os.path.pardir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to demonstrate how we can evaluate the results of a baseline on a given benchmark.\n",
    "\n",
    "It will be split into two part. The first part will focus on the evaluation of a baseline that does not requires any training (the `DCApproximatrionAC`). On the second part, we will show how to load a baseline (or any other `AugmentedSimulator`) and evaluate it on a `Benchmark` of our choice.\n",
    "\n",
    "As for the first notebook, we demonstrate this capability for the case of `NeuripsBenchmark1`.\n",
    "\n",
    "**NB** This notebook supposes that the data for the benchmark are already available. If they are not, please generate them or download them.\n",
    "\n",
    "**NB** The `DCApproximatrionAC` baseline requires the `grid2op` python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Benchmark1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial step: load the dataset\n",
    "\n",
    "A common dataset will be used for evaluate the two augmented simulator. This initial step aims at loading it once and for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 15:48:18.989554: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2022-01-07 15:48:18.989611: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from lips.benchmark import PowerGridBenchmark\n",
    "path_benchmark = os.path.join(\"reference_data\")\n",
    "log_path = os.path.abspath(os.path.join(\"lips\",\"logger\",\"logs.log\"))\n",
    "benchmark1 = PowerGridBenchmark(benchmark_name=\"Benchmark1\",\n",
    "                                path_benchmark=path_benchmark,\n",
    "                                load_data_set=True,\n",
    "                                log_path=log_path\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DC approximation\n",
    "\n",
    "We remind that the `grid2op` library is required for this part. You can install it with `pip install grid2op` if you do not have it already.\n",
    "\n",
    "First we will create the \"augmented simulator\". As opposed to the second model we will expose here, this method require access to a powergrid. This is one of the reason we need grid2op. \n",
    "\n",
    "The way to load each `AugmentedSimulator` is specific. Here for example we load the DCApproximation that will use the same powergrid as the one used to generate the data in the previous Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next few lines are specific for each benchmark and each `AugmentedSimulator`\n",
    "import grid2op\n",
    "import warnings\n",
    "from lips.physical_simulator.dcApproximationAS import DCApproximationAS\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    env = grid2op.make(\"l2rpn_case14_sandbox\", test=True)\n",
    "    grid_path = os.path.join(env.get_path_env(), \"grid.json\")\n",
    "\n",
    "dc_sim = DCApproximationAS(name=\"dc_approximation\", \n",
    "                           benchmark_name=\"Benchmark1\",\n",
    "                           path_config=None, # use default config path\n",
    "                           grid_path=grid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is load, there is a common interface to evaluate its performance, on a dataset. This is showed in the cell bellow where we evaluate a physics based simulator `DCApproximation` on these two dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_metrics_per_dataset = benchmark1.evaluate_simulator(augmented_simulator=dc_sim,\n",
    "                                                       dataset=\"all\"\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prod_p', 'prod_v', 'load_p', 'load_q', 'line_status', 'topo_vect', 'a_or', 'a_ex'])\n",
      "dict_keys(['val', 'test', 'test_ood_topo'])\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"val\"\n",
    "# use dc simulator `_observations` private variable to trace all the observations used\n",
    "print(dc_sim._observations[\"val\"].keys())\n",
    "# use dc simulator `_flow` private variable for computed flow values \n",
    "print(dc_sim._flow.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it is possible to study the metrics on the different dataset. For example, if we want the \"MSE\" error on the \"test\" dataset (with a similar distribution as the training one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val', 'test', 'test_ood_topo'])\n"
     ]
    }
   ],
   "source": [
    "# see the evaluated datasets\n",
    "print(dc_metrics_per_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE90     : {'a_or': 0.16308450692718962, 'a_ex': 0.1464899336666497}\n",
      "NRMSE_avg  : {'a_or': 0.08072826942694072, 'a_ex': 0.07632464035218221}\n",
      "MAE_avg    : {'a_or': 84.77337833139086, 'a_ex': 107.36099085275112}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"test\"\n",
    "ML_metrics = 0\n",
    "print(\"{:<10} : {}\".format(\"MAPE90\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"mape90\"]))\n",
    "print(\"{:<10} : {}\".format(\"NRMSE_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"NRMSE_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MAE_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"MAE_avg\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE90     : {'a_or': 0.6199380111893823, 'a_ex': 0.6128878148489871}\n",
      "NRMSE_avg  : {'a_or': 0.24516475287804304, 'a_ex': 0.24667614062546903}\n",
      "MAE_avg    : {'a_or': 184.18927002976574, 'a_ex': 256.52591287868444}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"test_ood_topo\"\n",
    "print(\"{:<10} : {}\".format(\"MAPE90\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"mape90\"]))\n",
    "print(\"{:<10} : {}\".format(\"NRMSE_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"NRMSE_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MAE_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"MAE_avg\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A learned baseline \"augmented simulator\"\n",
    "\n",
    "Along with some dataset, we provide also some baseline (from a trained neural network). This baseline is made of a fully connected neural network that takes the available input of the powergrid and tries to predict all the output of the simulator.\n",
    "\n",
    "The fully connected neural network is made of XXX layer each with YYY units.\n",
    "\n",
    "It is learned for KKK epochs on the training set of the `Benchmark1`.\n",
    "\n",
    "**NB** These baselines are not yet fully trained, and some hyper parameters still need to be optimized. We intend on doing that before the official release of the benchmark for the Neurips conference.\n",
    "\n",
    "First we need to load the baseline and initialize it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_baselines = os.path.join(\"trained_baselines\")\n",
    "from lips.augmented_simulators import FullyConnectedAS\n",
    "\n",
    "# recreate the baseline\n",
    "fc_augmented_sim = FullyConnectedAS(name=\"Baseline_FullyConnected\")\n",
    "\n",
    "# TODO create a wrapper for these 3 calls\n",
    "fc_augmented_sim.load_metadata(path_baselines)\n",
    "fc_augmented_sim.init()\n",
    "fc_augmented_sim.restore(path_baselines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, as for the DC approximation, we can evaluate it on the test datasets of the benchmark.\n",
    "\n",
    "This is done with the same command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A log file including some verifications is created at root directory with the name logs.log\n"
     ]
    }
   ],
   "source": [
    "fc_metrics_per_dataset = neurips_benchmark1.evaluate_augmented_simulator(fc_augmented_sim, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12818050384521484"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_augmented_sim._predict_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the two augmented simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can compare the two \"augmented simulators\". For example, if we want to compare the MAPE90 (mean absolute percentage error compute for last 10% quantile) on the test dataset (with a distribution similar to the training distribution) for currents (A) at two extremity of power lines, we might compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a_or': 0.00880006226699487, 'a_ex': 0.008783799864951779}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_metrics = 0\n",
    "fc_metrics_per_dataset[\"test\"][ML_metrics]['mape90']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a_or': 0.1776475031579627, 'a_ex': 0.1608028851306038}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_metrics_per_dataset[\"test\"][ML_metrics]['mape90']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the same quantity but for the \"out of distribution (due to topology)\" distribution we can have a look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a_or': 0.20694255606968653, 'a_ex': 0.20816347558174791}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_metrics_per_dataset[\"test_ood_topo\"][ML_metrics]['mape90']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a_or': 0.1776475031579627, 'a_ex': 0.1608028851306038}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_metrics_per_dataset[\"test\"][ML_metrics]['mape90']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physic compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02305"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physic_compliances = 1\n",
    "fc_metrics_per_dataset[\"test\"][physic_compliances][\"BasicVerifications\"][\"currents\"][\"a_or\"][\"Violation_proportion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11866.515"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_metrics_per_dataset[\"test\"][physic_compliances][\"BasicVerifications\"][\"currents\"][\"a_or\"][\"Error\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark3 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rte",
   "language": "python",
   "name": "rte"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
