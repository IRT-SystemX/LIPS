{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to demonstrate how we can evaluate the results of a baseline on a given benchmark.\n",
    "\n",
    "It will be split into two part. The first part will focus on the evaluation of a baseline that does not requires any training (the `DCApproximatrionAC`). On the second part, we will show how to load a baseline (or any other `AugmentedSimulator`) and evaluate it on a `Benchmark` of our choice.\n",
    "\n",
    "As for the first notebook, we demonstrate this capability for the case of `NeuripsBenchmark1`.\n",
    "\n",
    "**NB** This notebook supposes that the data for the benchmark are already available. If they are not, please generate them or download them.\n",
    "\n",
    "**NB** The `DCApproximatrionAC` baseline requires the `grid2op` python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pprint import pprint\n",
    "from lips.benchmark import PowerGridBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate required paths\n",
    "LIPS_PATH = pathlib.Path().resolve().parent\n",
    "DATA_PATH = LIPS_PATH / \"reference_data\"\n",
    "LOG_PATH = LIPS_PATH / \"lips_logs.log\"\n",
    "CONFIG_PATH = LIPS_PATH / \"lips\" / \"config\" / \"conf.ini\"\n",
    "BASELINES_PATH = LIPS_PATH / \"trained_baselines\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Benchmark1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial step: load the dataset\n",
    "\n",
    "A common dataset will be used for evaluate the two augmented simulator. This initial step aims at loading it once and for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark1 = PowerGridBenchmark(benchmark_name=\"Benchmark1\",\n",
    "                                benchmark_path=DATA_PATH,\n",
    "                                load_data_set=True,\n",
    "                                log_path=LOG_PATH,\n",
    "                                config_path=CONFIG_PATH\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark name:  Benchmark1\n",
      "Environment name:  l2rpn_case14_sandbox\n",
      "Output attributes:  ('a_or', 'a_ex')\n",
      "Evaluation criteria: \n",
      "{'IndRed': [],\n",
      " 'ML': ['MSE_avg', 'MAE_avg', 'mape_90_avg'],\n",
      " 'OOD': ['MSE_avg', 'MAE_avg', 'mape_90_avg'],\n",
      " 'Physics': ['CURRENT_POS']}\n"
     ]
    }
   ],
   "source": [
    "# to verify the config is loaded appropriately for this benchmark\n",
    "print(\"Benchmark name: \", benchmark1.config.section_name)\n",
    "print(\"Environment name: \", benchmark1.config.get_option(\"env_name\"))\n",
    "print(\"Output attributes: \", benchmark1.config.get_option(\"attr_y\"))\n",
    "print(\"Evaluation criteria: \")\n",
    "pprint(benchmark1.config.get_option(\"eval_dict\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DC approximation\n",
    "\n",
    "We remind that the `grid2op` library is required for this part. You can install it with `pip install grid2op` if you do not have it already.\n",
    "\n",
    "First we will create the \"augmented simulator\". As opposed to the second model we will expose here, this method require access to a powergrid. This is one of the reason we need grid2op. \n",
    "\n",
    "The way to load each `AugmentedSimulator` is specific. Here for example we load the DCApproximation that will use the same powergrid as the one used to generate the data in the previous Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next few lines are specific for each benchmark and each `AugmentedSimulator`\n",
    "import grid2op\n",
    "import warnings\n",
    "from lips.physical_simulator.dcApproximationAS import DCApproximationAS\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    env = grid2op.make(benchmark1.config.get_option(\"env_name\"), test=True)\n",
    "    grid_path = pathlib.Path(env.get_path_env()) / \"grid.json\"\n",
    "\n",
    "dc_sim = DCApproximationAS(name=\"dc_approximation\", \n",
    "                           benchmark_name=\"Benchmark1\",\n",
    "                           config_path=None, # use default config path\n",
    "                           grid_path=grid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is load, there is a common interface to evaluate its performance, on a dataset. This is showed in the cell bellow where we evaluate a physics based simulator `DCApproximation` on these two dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_metrics_per_dataset = benchmark1.evaluate_simulator(augmented_simulator=dc_sim,\n",
    "                                                       dataset=\"all\" # other values : \"val\", \"test\", \"test_ood_topo\"\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': {'IndRed': {},\n",
      "          'ML': {'MAE_avg': {'a_ex': 107.61548189784273,\n",
      "                             'a_or': 84.97437099584019},\n",
      "                 'MSE_avg': {'a_ex': 72934.1281314372,\n",
      "                             'a_or': 49687.964377169934},\n",
      "                 'mape_90_avg': {'a_ex': 0.14643601581756005,\n",
      "                                 'a_or': 0.16305198078356278}},\n",
      "          'Physics': {'CURRENT_POS': {}}},\n",
      " 'test_ood_topo': {'IndRed': {},\n",
      "                   'ML': {'MAE_avg': {'a_ex': 115.29069542441187,\n",
      "                                      'a_or': 90.98358689257782},\n",
      "                          'MSE_avg': {'a_ex': 84768.24530659153,\n",
      "                                      'a_or': 57543.84358850798},\n",
      "                          'mape_90_avg': {'a_ex': 0.16242165476667145,\n",
      "                                          'a_or': 0.1796451285591881}},\n",
      "                   'Physics': {'CURRENT_POS': {}}},\n",
      " 'val': {'IndRed': {},\n",
      "         'ML': {'MAE_avg': {'a_ex': 106.66064066851729,\n",
      "                            'a_or': 84.34903333650664},\n",
      "                'MSE_avg': {'a_ex': 72049.00478794311,\n",
      "                            'a_or': 49269.07913815063},\n",
      "                'mape_90_avg': {'a_ex': 0.1487931272336917,\n",
      "                                'a_or': 0.1657331818964689}},\n",
      "         'Physics': {'CURRENT_POS': {}}}}\n"
     ]
    }
   ],
   "source": [
    "pprint(dc_metrics_per_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it is possible to study the metrics on the different dataset. For example, if we want the \"MSE\" error on the \"test\" dataset (with a similar distribution as the training one):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A learned baseline \"augmented simulator\"\n",
    "\n",
    "Along with some dataset, we provide also some baseline (from a trained neural network). This baseline is made of a fully connected neural network that takes the available input of the powergrid and tries to predict all the output of the simulator.\n",
    "\n",
    "The fully connected neural network is made of XXX layer each with YYY units.\n",
    "\n",
    "It is learned for KKK epochs on the training set of the `Benchmark1`.\n",
    "\n",
    "**NB** These baselines are not yet fully trained, and some hyper parameters still need to be optimized. We intend on doing that before the official release of the benchmark for the Neurips conference.\n",
    "\n",
    "First we need to load the baseline and initialize it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.augmented_simulators.fullyConnectedAS import FullyConnectedAS\n",
    "\n",
    "# rebuild the baseline architecture\n",
    "fc_augmented_sim = FullyConnectedAS(name=\"FullyConnectedAS\",\n",
    "                                    benchmark_name=\"Benchmark1\",\n",
    "                                    log_path=LOG_PATH\n",
    "                                   )\n",
    "\n",
    "# TODO create a wrapper for these 3 calls\n",
    "fc_augmented_sim.load_metadata(BASELINES_PATH)\n",
    "fc_augmented_sim.init()\n",
    "fc_augmented_sim.restore(BASELINES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, as for the DC approximation, we can evaluate it on the test datasets of the benchmark.\n",
    "\n",
    "This is done with the same command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_metrics_per_dataset = benchmark1.evaluate_simulator(augmented_simulator=fc_augmented_sim,\n",
    "                                                       dataset=\"all\",\n",
    "                                                       batch_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the two augmented simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can compare the two \"augmented simulators\". For example, if we want to compare the MAPE90 (mean absolute percentage error compute for last 10% quantile) on the test dataset (with a distribution similar to the training distribution) for currents (A) at two extremity of power lines, we might compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC Approximation\n",
      "Dataset : test\n",
      "MAPE90     : {'a_or': 0.16305198078356278, 'a_ex': 0.14643601581756005}\n",
      "MSE_avg    : {'a_or': 49687.964377169934, 'a_ex': 72934.1281314372}\n",
      "MAE_avg    : {'a_or': 84.97437099584019, 'a_ex': 107.61548189784273}\n",
      "Dataset : test_ood_topo\n",
      "mape_90_avg : {'a_or': 0.1796451285591881, 'a_ex': 0.16242165476667145}\n",
      "MSE_avg    : {'a_or': 57543.84358850798, 'a_ex': 84768.24530659153}\n",
      "MAE_avg    : {'a_or': 90.98358689257782, 'a_ex': 115.29069542441187}\n"
     ]
    }
   ],
   "source": [
    "ML_metrics = \"ML\"\n",
    "dataset_name = \"test\"\n",
    "print(\"DC Approximation\")\n",
    "print(f\"Dataset : {dataset_name}\")\n",
    "print(\"{:<10} : {}\".format(\"MAPE90\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"mape_90_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MSE_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"MSE_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MAE_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"MAE_avg\"]))\n",
    "dataset_name = \"test_ood_topo\"\n",
    "print(f\"Dataset : {dataset_name}\")\n",
    "print(\"{:<10} : {}\".format(\"mape_90_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"mape_90_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MSE_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"MSE_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MAE_avg\", dc_metrics_per_dataset[dataset_name][ML_metrics][\"MAE_avg\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully Connected Augmented Simulator\n",
      "Dataset : test\n",
      "mape_90_avg : {'a_or': 0.007147388017315423, 'a_ex': 0.006962988795339291}\n",
      "MSE_avg    : {'a_or': 10.514204978942871, 'a_ex': 23.054763793945312}\n",
      "MAE_avg    : {'a_or': 1.9447065591812134, 'a_ex': 2.7763805389404297}\n",
      "Dataset : test_ood_topo\n",
      "mape_90_avg : {'a_or': 0.19447579884433752, 'a_ex': 0.19415184161617227}\n",
      "MSE_avg    : {'a_or': 8378.7890625, 'a_ex': 14869.296875}\n",
      "MAE_avg    : {'a_or': 40.592567443847656, 'a_ex': 56.149513244628906}\n"
     ]
    }
   ],
   "source": [
    "ML_metrics = \"ML\"\n",
    "dataset_name = \"test\"\n",
    "print(\"Fully Connected Augmented Simulator\")\n",
    "print(f\"Dataset : {dataset_name}\")\n",
    "print(\"{:<10} : {}\".format(\"mape_90_avg\", fc_metrics_per_dataset[dataset_name][ML_metrics][\"mape_90_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MSE_avg\", fc_metrics_per_dataset[dataset_name][ML_metrics][\"MSE_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MAE_avg\", fc_metrics_per_dataset[dataset_name][ML_metrics][\"MAE_avg\"]))\n",
    "dataset_name = \"test_ood_topo\"\n",
    "print(f\"Dataset : {dataset_name}\")\n",
    "print(\"{:<10} : {}\".format(\"mape_90_avg\", fc_metrics_per_dataset[dataset_name][ML_metrics][\"mape_90_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MSE_avg\", fc_metrics_per_dataset[dataset_name][ML_metrics][\"MSE_avg\"]))\n",
    "print(\"{:<10} : {}\".format(\"MAE_avg\", fc_metrics_per_dataset[dataset_name][ML_metrics][\"MAE_avg\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physic compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.69% of currents at the origin side of power lines violate the current positivity.\n"
     ]
    }
   ],
   "source": [
    "physic_compliances = \"Physics\"\n",
    "dataset_name = \"test\"\n",
    "current_violation = fc_metrics_per_dataset[dataset_name][physic_compliances][\"CURRENT_POS\"][\"a_or\"][\"Violation_proportion\"]\n",
    "print(\"{:.2f}% of currents at the origin side of power lines violate the current positivity.\".format(current_violation*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of negative current values (Amp) : 0.03\n"
     ]
    }
   ],
   "source": [
    "current_error = fc_metrics_per_dataset[dataset_name][physic_compliances][\"CURRENT_POS\"][\"a_or\"][\"Violation_proportion\"]\n",
    "print(\"The sum of negative current values (Amp) : {:.2f}\".format(current_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industrial readiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13227033615112305"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_augmented_sim.predict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262.6392412185669"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_sim._predict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138.35747891198844"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_sim._raw_grid_simulator.comp_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.benchmark import PowerGridBenchmark\n",
    "path_benchmark = os.path.join(\"reference_data\")\n",
    "log_path = os.path.abspath(os.path.join(\"lips\",\"logger\",\"logs.log\"))\n",
    "benchmark2 = PowerGridBenchmark(benchmark_name=\"Benchmark2\",\n",
    "                                path_benchmark=path_benchmark,\n",
    "                                load_data_set=True,\n",
    "                                log_path=log_path\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark3 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lips_gitlab",
   "language": "python",
   "name": "lips_gitlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
