{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design new scenarios (benchmarks)\n",
    "In order to design new benchmarks in the context of power networks, a configuration file should be created from which the benchmark is initiated. The LIPS platform via `ConfigManager` class ease this operation, and this notebook shows how to use its different functionalities.\n",
    "\n",
    "The figure below presents the scheme of benchmarking platform which is composed of three distinct parts : \n",
    "* DataSet: Generate some dataset for training and evaluation \n",
    "* Benchmark: coordinates between different parts and allows train and evaluate an agumented simulator \n",
    "* Evaluation: once the benchmark done, it allow to evaluate the performance with respect to various point of views\n",
    "\n",
    "![title](img/Benchmarking_scheme_v2.png)\n",
    "\n",
    "In this notebook we concentrate on the middle module where a scenario should be defined and configured, and for which we could generate some data in `Notebook 01`, and on the basis of which we can evalute some baseline methods (see `Notebook 02`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Read an existing config](#first-bullet)\n",
    "* [Create a new Benchmark](#second-bullet)\n",
    "* [Create a new configuration file](#create-new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_for the moment, I return to the parent directory to be able to access the lips modules, this cell is not necessary if the lips package is installed in future_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pprint import pprint\n",
    "from lips.config import ConfigManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read an existing config <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an object of `ConfigManager`class. By indicating an existing benchmark name, its options will be restored by the config manager. You can also indicate a path to load an existing configuration file or to store a new configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, for demonstration purpose we select a path to an existing configuration file for power grid use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIPS_PATH = pathlib.Path().resolve().parent\n",
    "CONFIG_PATH = LIPS_PATH / \"configurations\" / \"powergrid\" / \"benchmarks\" / \"l2rpn_case14_sandbox.ini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a config using the indicated configuration path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bench1 = ConfigManager(section_name=\"Benchmark1\",\n",
    "                          path=CONFIG_PATH\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the list of available benchmarks (aka config sections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bench1.config.sections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the options available for a selected benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have selected `Benchmark1` as the section name, its options could be accessed easily using provided functions. We can print all the parameters set for this benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm_bench1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access the option values\n",
    "One can also access the options using `get_option` function and by indicating the desired option key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set input features for `Benchmark1` of power grid use case. In power grid use case, in addition to injections (`prod_p`, `prod_v`, `load_p`, `load_q`), we consider also two supplementary features refering to the grid topology and the connectivity of the power lines (`topo_vect`, `line_status`). \n",
    "\n",
    "These supplementary variables are used as input for simple augmented simulators as `FullyConnected` neural network, however, they intervene in latent space in more sophisticated architectures as `LeapNet`. More from these architecture are provided in next notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bench1.get_option(\"attr_x\") + cm_bench1.get_option(\"attr_tau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of outputs `Benchmark1` of power grid use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bench1.get_option(\"attr_y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of required evaluation criteria for `Benchmark1` of power grid use case. As we can observe, there are 4 different categories of evaluation criteria which are defined in our proposed benchmarking pipeline, which are:\n",
    "\n",
    "- `ML`: Machine learning related metrics (computing the accuracy of augmented simulators);\n",
    "- `Physics`: physics compliances which verify the physics laws (equations) on the predictions of an augmented simulator;\n",
    "- `IndRed`: Industrial Readiness which verifies whether the proposed augmented simulators could be exploited in industriy;\n",
    "- `OOD`: which verifies the out-of-distribution generalization capacity of the augmented simulators.\n",
    "\n",
    "These metrics could vary from benchmark to benchmark wrt to the complexity and outputs of each benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bench1.get_option(\"eval_dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the evaluation metrics for another benchmark (`Benchmark2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bench2 = ConfigManager(section_name=\"Benchmark2\",\n",
    "                          path=CONFIG_PATH\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, this benchmark includes more output variables to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(cm_bench2.get_option(\"attr_y\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are more physics compliances that should be verified for this more complex benchmark, as it includes more outputs to predicted using an augmented simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(cm_bench2.get_option(\"eval_dict\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add/Edit the options\n",
    "We can edit the existing options for the selected benchmark (aka config section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bench1.edit_config_option('attr_y', \"('a_or')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bench1.get_option(\"attr_y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write this modification to `config.ini` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bench1._write_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove an option and update the config file \n",
    "We can also remove an option that is not required anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add a new option first, using the same `edit_config_option` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bench1.edit_config_option('new_attr', \"('theta_or')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm_bench1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can remove this newly created option using `remove_config_option` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bench1.remove_config_option(option=\"new_attr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm_bench1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new section for a new scenario <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define a complete set of options for a new benchmark, if requried. It could be started by instantiating the `ConfigManager` class and giving a new name to the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfigManager(section_name=\"Benchmark_new\",\n",
    "                   path=CONFIG_PATH\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create the configuration by adding the required options as parameter. No restriction for the name of attributes at this step. \n",
    "\n",
    "**Nb :** However, these names will be used afterwards by `PowerGridBenchmark` class to parameterize the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.create_config(attr_y=\"('p_or', 'a_or')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print now the configuration options, you may see some options that came from the `DEFAULT` section of the configuration file. If this is not the required behavior, you can skip the rest of this section and see the the [next](#create-new) section in which we create a configuration file from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add another option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"'this benchmark is intended to output power and current'\"\n",
    "cm.edit_config_option(option='description', value=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.get_option(\"description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and remove an undesired option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.remove_config_option(option=\"description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this benchmark is added among the existing benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.config.sections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify its options.\n",
    "\n",
    "*NB*: the returned options comprise also `DEFAULT` section options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.get_options_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.get_option('attr_y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally write it to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm._write_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unused or test sections and update `config.ini`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.remove_section(section_name=\"Benchmark_new\")\n",
    "cm._write_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new configuration file <a class=\"anchor\" id=\"create-new\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIPS_PATH = pathlib.Path().resolve().parent\n",
    "CONFIG_FILE = LIPS_PATH / \"configurations\" / \"powergrid\" / \"benchmarks\" / \"new_config_file.ini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should start by creating the corresponding file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    open(CONFIG_FILE, 'a').close()\n",
    "except OSError:\n",
    "    print('Failed creating the file')\n",
    "else:\n",
    "    print('File created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfigManager(section_name=\"MyBenchmark\",\n",
    "                   path=CONFIG_FILE\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the configuration section in the corresponding file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.create_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a set of required options for this custom benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.edit_config_option(option=\"attr_x\", value='(\"prod_p\", \"prod_v\", \"load_p\", \"load_q\")')\n",
    "cm.edit_config_option(option=\"attr_y\", value=\"('p_or', 'a_or')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the structure of you configuration section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add more options if required and write it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm._write_config()"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "lips",
   "language": "python",
   "name": "lips"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
