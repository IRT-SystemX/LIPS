{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5648efb1-f90f-4261-a1cc-dba902b1b173",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T09:47:19.828698Z",
     "start_time": "2025-03-10T09:47:19.823375Z"
    }
   },
   "source": [
    "# Getting Started with the Scoring Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da8963b9847353",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use the scoring feature from the LIPS package. We will cover the following topics:\n",
    "\n",
    "- **Defining Scoring Based on Configuration File**: We will show how to define scoring configurations and explain the structure of the configuration file.\n",
    "- **Application to Competitions**: We will apply the scoring feature to two competitions: the airfoil competition and the powergrid competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35507c90-988e-4954-8351-9655d172503f",
   "metadata": {},
   "source": [
    "## 1. Configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8cad8d-560d-4741-9274-038dbc2d86e4",
   "metadata": {},
   "source": [
    "The core configuration for scoring is located in **configurations/powergrid/scoring/ScoreConfig.ini**. This file defines the scoring logic and allows for flexible customization of your score evaluation process. Three key sections are essential within this configuration: Thresholds, ValueByColor, and Coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9daea4-1b29-4d96-9bd8-7879cd561f54",
   "metadata": {},
   "source": [
    "### 1. Thresholds: Defining Performance Boundaries\n",
    "\n",
    "The `Thresholds` section specifies the performance benchmarks for individual metrics. It includes both the threshold values and the comparison type, indicating whether to minimize, maximize, or evaluate a ratio.\n",
    "\n",
    "* **Comparison Types:**\n",
    "    * `minimize`: Lower values are considered better.\n",
    "    * `maximize`: Higher values are considered better.\n",
    "    * `ratio`: used to compare two values, if the ratio is under or above the thresholds.\n",
    "* **Example:**\n",
    "    ```ini\n",
    "    \"a_or\": {\"comparison_type\": \"minimize\", \"thresholds\": [0.02, 0.05]}\n",
    "    ```\n",
    "    * **Explanation:** This entry defines thresholds for the metric \"a_or\". If the \"a_or\" value is less than 0.02, it falls within the best (green) range. If it's between 0.02 and 0.05, it falls within the middle (orange) range. If it's greater than 0.05, it falls within the worst (red) range. Because the comparison type is minimize, the lower the a_or value is the better.\n",
    "\n",
    "### 2. ValueByColor: Assigning Scores to Performance Ranges\n",
    "\n",
    "The `ValueByColor` section maps performance ranges (represented by colors) to numerical scores. This allows you to quantify the qualitative assessment of your metrics.\n",
    "\n",
    "* **Example:**\n",
    "    ```ini\n",
    "    {\"green\": 2, \"orange\": 1, \"red\": 0}\n",
    "    ```\n",
    "    * **Explanation:** In this example, a metric falling within the \"green\" range receives a score of 2, \"orange\" receives 1, and \"red\" receives 0.\n",
    "\n",
    "### 3. Coefficients: Weighting Metric Contributions\n",
    "\n",
    "The `Coefficients` section defines the relative importance of different metrics and sub-metrics in the overall score. This allows you to prioritize certain aspects of your model's performance.\n",
    "\n",
    "* **Hierarchical Structure:** Coefficients can be organized hierarchically, allowing you to assign weights to both top-level metrics (e.g., \"ML\", \"OOD\", \"Physics\") and their sub-metrics (e.g., \"Accuracy\", \"SpeedUP\").\n",
    "* **Example:**\n",
    "    ```ini\n",
    "    {\"ML\": {\"value\": 0.4, \"Accuracy\": {\"value\": 0.75}, \"SpeedUP\": {\"value\": 0.25}},\n",
    "     \"OOD\": {\"value\": 0.3, \"Accuracy\": {\"value\": 0.75}, \"SpeedUP\": {\"value\": 0.25}},\n",
    "     \"Physics\": {\"value\": 0.3}}\n",
    "    ```\n",
    "    * **Explanation:**\n",
    "        * The \"ML\" metric contributes 40% (0.4) to the overall score.\n",
    "        * Within \"ML\", \"Accuracy\" contributes 75% (0.75) and \"SpeedUP\" contributes 25% (0.25) to the \"ML\" sub-score.\n",
    "        * The \"OOD\" metric contributes 30% to the overall score, and also uses the same accuracy and speedup sub metric weights.\n",
    "        * The \"Physics\" metric contributes 30% to the overall score, and has no submetrics.\n",
    "        * This structure allows for fine-grained control over the scoring process, ensuring that the final score reflects the relative importance of different aspects of your model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9eda4-5953-4fda-b3fd-a8c73bab433d",
   "metadata": {},
   "source": [
    "## 2. Metric format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f32854-1470-4274-b5b0-dbfdb3715cef",
   "metadata": {},
   "source": [
    " The scoring feature expects metrics data in a nested dictionary format. The structure should represent a tree-like hierarchy, where each node\n",
    " contains either sub-metrics or leaf metrics. Leaf metrics are the actual values that will be colorized and scored.\n",
    "\n",
    " Here's an example of the expected format:\n",
    "\n",
    " ```json\n",
    " {\n",
    "     \"ML\": {\n",
    "         \"metric1\": 0.85,\n",
    "         \"metric2\": 0.20\n",
    "     },\n",
    "     \"OOD\": {\n",
    "         \"metric3\": 0.92,\n",
    "         \"metric4\": 0.15\n",
    "     },\n",
    "     \"Physics\": {\n",
    "         \"metric5\": 0.78,\n",
    "         \"metric6\": 0.30\n",
    "     }\n",
    " }\n",
    " ```\n",
    "\n",
    " In this example:\n",
    " - The top-level keys (`ML`, `OOD`, `Physics`) represent different categories or components.\n",
    " - Each category contains several metrics (e.g., `metric1`, `metric2`).\n",
    " - The values associated with each metric are numerical values.\n",
    "\n",
    " The metrics can also be structured in deeper hierarchies:\n",
    "\n",
    " ```json\n",
    " {\n",
    "     \"Category1\": {\n",
    "         \"SubCategory1\": {\n",
    "             \"metric1\": 0.75,\n",
    "             \"metric2\": 0.25\n",
    "         },\n",
    "         \"SubCategory2\": {\n",
    "             \"metric3\": 0.60,\n",
    "             \"metric4\": 0.40\n",
    "         }\n",
    "     },\n",
    "     \"Category2\": {\n",
    "         \"metric5\": 0.90,\n",
    "         \"metric6\": 0.10\n",
    "     }\n",
    " }\n",
    " ```\n",
    "\n",
    " It's important to maintain a consistent branching structure throughout the metrics data. This means that if one sub-category contains further nested\n",
    " categories, all other sub-categories at the same level should also have the same structure.\n",
    "\n",
    " The keys of the metrics should match the keys defined in the configuration file. For example, if the configuration file defines thresholds for `metric1`, the metrics data must also contain `metric1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd4161c-5ed3-48af-b2ee-9e1fe7e7725a",
   "metadata": {},
   "source": [
    "## End-to-End Scoring Example\n",
    "This section demonstrates an end-to-end example of how to use the scoring feature. We will load a configuration file, read a metrics file, colorize the metrics, calculate sub-scores, and then calculate the global score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f97d0-f68c-4fea-88c1-e67b0583c2a3",
   "metadata": {},
   "source": [
    "### 1. Define Example Configuration File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c202e-dc1d-4307-8a81-1df47cee3d57",
   "metadata": {},
   "source": [
    "### 1. Define Example Configuration File\n",
    "\n",
    "First, let's define an example configuration file (`config.ini`).\n",
    "This file contains the thresholds, value_by_color mappings, and coefficients needed for scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783fb53d-6f20-49cc-89fc-0fc028ae6145",
   "metadata": {},
   "source": [
    "```ini\n",
    "[thresholds]\n",
    "a_or = {\"comparison_type\": \"minimize\", \"thresholds\": [0.02, 0.05]}\n",
    "spearman_correlation_drag = {\"comparison_type\": \"maximize\", \"thresholds\": [0.7, 0.9]}\n",
    "inference_time = {\"comparison_type\": \"minimize\", \"thresholds\": [500, 700]}\n",
    "[valuebycolor]\n",
    "green = 2\n",
    "orange = 1\n",
    "red = 0\n",
    "[coefficients]\n",
    "ML = {\"value\": 0.4, \"Accuracy\": {\"value\": 0.75}, \"SpeedUP\": {\"value\": 0.25}}\n",
    "OOD = {\"value\": 0.3, \"Accuracy\": {\"value\": 0.75}, \"SpeedUP\": {\"value\": 0.25}}\n",
    "Physics = {\"value\": 0.3}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2a889-8f22-4a4c-838c-9cf56c3fde2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%% [markdown]\n",
    "# ### 2. Define Example Metrics File\n",
    "#\n",
    "# Next, let's define an example metrics file (`metrics.json`).\n",
    "# This file contains the metric values that we want to score.\n",
    "#\n",
    "# ```json\n",
    "# {\n",
    "#     \"ML\": {\n",
    "#         \"a_or\": 0.01,\n",
    "#         \"spearman_correlation_drag\": 0.95\n",
    "#     },\n",
    "#     \"OOD\": {\n",
    "#         \"inference_time\": 400\n",
    "#     },\n",
    "#     \"Physics\": {\n",
    "#         \"a_or\": 0.06,\n",
    "#         \"spearman_correlation_drag\": 0.75\n",
    "#     },\n",
    "#     \"Speed\": {\n",
    "#         \"inference_time\": 600\n",
    "#     }\n",
    "# }\n",
    "# ```\n",
    "\n",
    "#%%\n",
    "import json\n",
    "from scoring import Scoring\n",
    "from utils import read_json\n",
    "\n",
    "# Initialize the Scoring class with the path to the configuration file\n",
    "scoring = Scoring(config_path=\"config.ini\")\n",
    "\n",
    "# Load metrics data from the JSON file\n",
    "metrics_path = \"metrics.json\"\n",
    "metrics_data = read_json(json_path=metrics_path)\n",
    "\n",
    "#%% [markdown]\n",
    "# ### 3. Colorize Metrics\n",
    "#\n",
    "# Now, let's colorize the metrics using the `colorize_metrics` function.\n",
    "# This will convert the numerical metric values into color strings based on the\n",
    "# thresholds defined in the configuration file.\n",
    "\n",
    "#%%\n",
    "# Colorize the metrics data\n",
    "colorized_metrics = scoring.colorize_metrics(metrics_data)\n",
    "print(\"Colorized Metrics:\", json.dumps(colorized_metrics, indent=4))\n",
    "\n",
    "#%% [markdown]\n",
    "# ### 4. Calculate Sub-Scores\n",
    "#\n",
    "# Next, let's calculate the sub-scores using the `calculate_sub_scores` function.\n",
    "# This will calculate the score for each sub-tree in the metrics data.\n",
    "\n",
    "#%%\n",
    "# Calculate sub-scores\n",
    "sub_scores = scoring.calculate_sub_scores(colorized_metrics)\n",
    "print(\"Sub-Scores:\", json.dumps(sub_scores, indent=4))\n",
    "\n",
    "#%% [markdown]\n",
    "# ### 5. Calculate Global Score\n",
    "#\n",
    "# Finally, let's calculate the global score using the `calculate_global_score` function.\n",
    "# This will calculate the overall score for the entire metrics tree.\n",
    "\n",
    "#%%\n",
    "# Calculate global score\n",
    "global_score = scoring.calculate_global_score(sub_scores)\n",
    "print(\"Global Score:\", global_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
