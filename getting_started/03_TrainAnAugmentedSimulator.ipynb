{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an augmented simulator (Power Grid use case)\n",
    "\n",
    "This notebook shows how to select and train an augmented simulator. In this notebook, we concentrate on the already implemented architectures. \n",
    "\n",
    "We will also show how the trained augmented simulators could be stored and be restored in future for the evaluation (for the evaluation see Notebook 2). As the training of an augmented simulator could be costly, we suggest users to use a GPU ressource if available and also to store their model for its further evaluation.\n",
    "\n",
    "We use the `Benchmark1` and the `Benchmark2` of the Power Grid use case to demonstrate how to perform this task. \n",
    "\n",
    "On the first section, we explain how to use a model that is already available on this reposotiry. The second section is dedicated to the explanation of what is needed to create a different kind of `AugmentedSimulator` with a different Neural Network archiecture with a customized loss etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Benchmark1](#benchmark1)\n",
    "    * [FullyConnected](#fc_benchmark1)\n",
    "    * [LeapNet](#leapnet_benchmark1)\n",
    "* [Benchmark2](#benchmark2)\n",
    "    * [FullyConnected](#fc_benchmark2)\n",
    "    * [LeapNet](#leapnet_benchmark2)\n",
    "* [Code another `augmented simulator`](#code-another)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pprint import pprint\n",
    "from lips.benchmark.powergridBenchmark import PowerGridBenchmark\n",
    "from lips.utils import get_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate required paths\n",
    "LIPS_PATH = pathlib.Path().resolve().parent # it is supposed that the notebook had run from getting_started folder\n",
    "DATA_PATH = LIPS_PATH / \"reference_data\" / \"powergrid\" / \"l2rpn_case14_sandbox\"\n",
    "BENCH_CONFIG_PATH = LIPS_PATH / \"configurations\" / \"powergrid\" / \"benchmarks\" / \"l2rpn_case14_sandbox.ini\"\n",
    "SIM_CONFIG_PATH = LIPS_PATH / \"configurations\" / \"powergrid\" / \"simulators\"\n",
    "BASELINES_PATH = LIPS_PATH / \"trained_baselines\" / \"powergrid\"\n",
    "TRAINED_MODEL_PATH = LIPS_PATH / \"trained_models\" / \"powergrid\"\n",
    "EVALUATION_PATH = LIPS_PATH / \"evaluation_results\" / \"PowerGrid\"\n",
    "LOG_PATH = LIPS_PATH / \"lips_logs.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark1 <a class=\"anchor\" id=\"benchmark1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the benchmark dataset\n",
    "\n",
    "As always the first step is always to load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark1 = PowerGridBenchmark(benchmark_name=\"Benchmark1\",\n",
    "                                benchmark_path=DATA_PATH,\n",
    "                                load_data_set=True,\n",
    "                                log_path=LOG_PATH,\n",
    "                                config_path=BENCH_CONFIG_PATH\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the config is loaded appropriately for this benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark name: \", benchmark1.config.section_name)\n",
    "print(\"Environment name: \", benchmark1.config.get_option(\"env_name\"))\n",
    "print(\"Output attributes: \", benchmark1.config.get_option(\"attr_y\"))\n",
    "print(\"Evaluation criteria: \")\n",
    "pprint(benchmark1.config.get_option(\"eval_dict\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an available model (Fully Connected) <a class=\"anchor\" id=\"fc_benchmark1\"></a>\n",
    "\n",
    "In this section we explain how to tune an available model. We take the example of the `FullyConnectedAS` that is an available fully connected neural network.\n",
    "\n",
    "This section supposes that you already have a \"model\" (for example based on neural networks) that meets the `AugmentedSimulator` interface. If you do not have it already, the next section will cover the main principles.\n",
    "\n",
    "**NB** The creation of the 'augmented_simulator' depends on each type of 'augmented_simulator'. \n",
    "\n",
    "The first step is to create the class you want to use, with the meta parameters you want to test. For this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.augmented_simulators.tensorflow_models import TfFullyConnected\n",
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "# the three lines bellow might be familiar to the tensorflow users. They tell tensorflow to not take all\n",
    "# the GPU video RAM for the model.\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for el in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(el, True)\n",
    "    \n",
    "tf_fc = TfFullyConnected(name=\"tf_fc\",\n",
    "                         # the path to the augmented simulator config file containing its hyperparameters\n",
    "                         sim_config_path=SIM_CONFIG_PATH / \"tf_fc.ini\",\n",
    "                         # using a specific configuration available in configuration file, DEFAULT here\n",
    "                         sim_config_name=\"DEFAULT\",\n",
    "                         # the path to the benchmark config file\n",
    "                         bench_config_path=BENCH_CONFIG_PATH,\n",
    "                         # `Benchmark_name` should correspond to one of config sections created earlier\n",
    "                         bench_config_name=\"Benchmark1\",\n",
    "                         # use a scaler class used to scale the datasets for a better learning\n",
    "                         scaler=StandardScaler,\n",
    "                         # a path where the logs during the training should be stored for debugging purpose\n",
    "                         log_path=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to train it. For example here we will train it for 200 epochs.\n",
    "\n",
    "**NB** You are responsible to use the correct dataset for training your model ! You can make experiments by training on the `test` set or on the `test_ood_topo` set if you want but we don't recommend you do to so !\n",
    "\n",
    "**NB** This code is generic and should work for all `AugementedSimulator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc.train(train_dataset=benchmark1.train_dataset,\n",
    "            val_dataset=benchmark1.val_dataset,\n",
    "            epochs=100\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc.visualize_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then you can save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = get_path(TRAINED_MODEL_PATH, benchmark1)\n",
    "tf_fc.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once saved, if you want to reuse it you can do exactly as we did in the previous notebook:\n",
    "```python\n",
    "from lips.augmented_simulators.tensorflow_models import TfFullyConnected\n",
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "tf_fc = TfFullyConnected(name=\"tf_fc\", # the name should match!\n",
    "                         sim_config_path=SIM_CONFIG_PATH / \"tf_fc.ini\",\n",
    "                         sim_config_name=\"DEFAULT\",\n",
    "                         bench_config_path=BENCH_CONFIG_PATH,\n",
    "                         bench_config_name=\"Benchmark1\",\n",
    "                         scaler=StandardScaler,\n",
    "                         log_path=LOG_PATH) \n",
    "\n",
    "LOAD_PATH = get_path(BASELINES_PATH, benchmark1)\n",
    "tf_fc.restore(LOAD_PATH)\n",
    "```\n",
    "\n",
    "And you are good to go !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train another architecture (LeapNet model) <a class=\"anchor\" id=\"leapnet_benchmark1\"></a>\n",
    "The leap nets allows to take into account the topology in the latent space, and have a more robust generalization performance than a simple fully connected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.augmented_simulators.tensorflow_models import LeapNet\n",
    "from lips.dataset.scaler import PowerGridScaler\n",
    "\n",
    "# the three lines bellow might be familiar to the tensorflow users. They tell tensorflow to not take all\n",
    "# the GPU video RAM for the model.\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for el in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(el, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leap_net = LeapNet(name=\"tf_leapnet\",\n",
    "                   # The path to the augmented simulator config file. It contains all the required hyperparameters\n",
    "                   sim_config_path=SIM_CONFIG_PATH / \"tf_leapnet.ini\",\n",
    "                   sim_config_name=\"DEFAULT\",\n",
    "                   # the path to Benchmark config file\n",
    "                   bench_config_path=BENCH_CONFIG_PATH,\n",
    "                   # `Benchmark_name` should correspond to one of config sections created earlier\n",
    "                   bench_config_name=\"Benchmark1\",\n",
    "                   # We can indeed override the hyperparameters considered in the config file directly by assigning the corresponding arguments\n",
    "                   sizes_main=(150, 150), # we change the number of the layers and the neurons in the main part\n",
    "                   sizes_enc=(20, 20, 20), # changing the number of the layers and neurones in encoding part\n",
    "                   sizes_out=(100, 40), # changing the number of the layers and neurones in decoding part\n",
    "                   # A scaler should be used to normalize the data. Here, we use a specific scaler for power grid data\n",
    "                   scaler=PowerGridScaler, \n",
    "                   # A path where the logs should be stored for debugging purpose\n",
    "                   log_path=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leap_net.train(train_dataset=benchmark1.train_dataset,\n",
    "               val_dataset=benchmark1.val_dataset,\n",
    "               epochs=200\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leap_net.visualize_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then you can save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = get_path(TRAINED_MODEL_PATH, benchmark1)\n",
    "leap_net.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark2 <a class=\"anchor\" id=\"benchmark2\"></a>\n",
    "The same procedure in benchmark1 can be applied without any exception to other benchmarks. The only difference resides in the instantiating of the benchmark class which provides the required datasets for this specific benchmark (`Benchmark2`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark2 = PowerGridBenchmark(benchmark_name=\"Benchmark2\",\n",
    "                                benchmark_path=DATA_PATH,\n",
    "                                load_data_set=True,\n",
    "                                log_path=LOG_PATH,\n",
    "                                config_path=BENCH_CONFIG_PATH\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the config is loaded appropriately for this benchmark. As we have seen in Notebook 0, there are more evaluation criteria for this more complex benchmark, as the augmented simulators are supposed to predict more flow variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark name: \", benchmark2.config.section_name)\n",
    "print(\"Environment name: \", benchmark2.config.get_option(\"env_name\"))\n",
    "print(\"Output attributes: \", benchmark2.config.get_option(\"attr_y\"))\n",
    "print(\"Evaluation criteria: \")\n",
    "pprint(benchmark2.config.get_option(\"eval_dict\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected model <a class=\"anchor\" id=\"fc_benchmark2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.augmented_simulators.tensorflow_models import TfFullyConnected\n",
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "# the three lines bellow might be familiar to the tensorflow users. They tell tensorflow to not take all\n",
    "# the GPU video RAM for the model.\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for el in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(el, True)\n",
    "\n",
    "# rebuild the baseline architecture\n",
    "tf_fc = TfFullyConnected(name=\"tf_fc\",\n",
    "                         bench_config_path=BENCH_CONFIG_PATH,\n",
    "                         bench_config_name=\"Benchmark2\",\n",
    "                         sim_config_path=SIM_CONFIG_PATH / \"tf_fc.ini\",\n",
    "                         sim_config_name=\"DEFAULT\",\n",
    "                         scaler=StandardScaler,\n",
    "                         log_path=LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc.train(train_dataset=benchmark2.train_dataset, val_dataset=benchmark2.val_dataset, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visalize the convergence curves of the model. It comprises two curves which are based on the Loss metric on the basis of which the model is trained (Mean Squared Error `MSE`) and also a metric that is computed alongside the loss criteria (Mean Absolute Error `MAE`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc.visualize_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then you can save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = get_path(TRAINED_MODEL_PATH, benchmark2)\n",
    "tf_fc.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeapNet model <a class=\"anchor\" id=\"leapnet_benchmark2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.augmented_simulators.tensorflow_models import LeapNet\n",
    "from lips.dataset.scaler import PowerGridScaler\n",
    "\n",
    "# the three lines bellow might be familiar to the tensorflow users. They tell tensorflow to not take all\n",
    "# the GPU video RAM for the model.\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for el in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(el, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leap_net = LeapNet(name=\"tf_leapnet\",\n",
    "                   # The path to the augmented simulator config file. It contains all the required hyperparameters\n",
    "                   sim_config_path=SIM_CONFIG_PATH / \"tf_leapnet.ini\",\n",
    "                   sim_config_name=\"DEFAULT\",\n",
    "                   # the path to Benchmark config file\n",
    "                   bench_config_path=BENCH_CONFIG_PATH,\n",
    "                   # `Benchmark_name` should correspond to one of config sections created earlier\n",
    "                   bench_config_name=\"Benchmark2\",\n",
    "                   # We can indeed override the hyperparameters considered in the config file directly by assigning the corresponding arguments\n",
    "                   sizes_main=(150, 150), # we change the number of the layers and the neurons in the main part\n",
    "                   sizes_enc=(20, 20, 20), # changing the number of the layers and neurones in encoding part\n",
    "                   sizes_out=(100, 40), # changing the number of the layers and neurones in decoding part\n",
    "                   # A scaler should be used to normalize the data. Here, we use a specific scaler for power grid data\n",
    "                   scaler=PowerGridScaler, \n",
    "                   # A path where the logs should be stored for debugging purpose\n",
    "                   log_path=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leap_net.train(train_dataset=benchmark2.train_dataset,\n",
    "               val_dataset=benchmark2.val_dataset,\n",
    "               epochs=200\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leap_net.visualize_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then you can save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = get_path(TRAINED_MODEL_PATH, benchmark2)\n",
    "leap_net.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code another type of \"augmented_simulator\" <a id=\"code-another\"></a>\n",
    "\n",
    "In our framework, we have provided the implementations of augmented simulators based on `Tensorflow` and `Pytorch` libraries following the class diagram shown below.  \n",
    "\n",
    "![title](img/ClassDiagramAS.png)\n",
    "\n",
    "For now, we have provided two augmented simulators based on Tensorflow, namely `FullyConnected` and `LeapNet` architectures, and one augmented simulator (`FullyConnected`) using Pytorch library as a starting point for users familiar with Pytorch. \n",
    "\n",
    "For each of these libraries, we provide also a controller (base class) which provide the main functionalities to train and evaluate the augmented simulators. So the users could focus only on the interesting part which is the construction of an augmented simulator architecture. \n",
    "\n",
    "Coding another type of \"augmented simulator\" is not difficult. Finding one that work well for all the criteria is of course a different challenge.\n",
    "\n",
    "### Tensorflow\n",
    "Basically, an augmented simulator based on `TensorFlow` should:\n",
    "\n",
    "- inherit from `TensorflowSimulator`\n",
    "- implements the `build_model` where the architecture is defined\n",
    "- implements the `process_data` which prepare and scale the data for the specific augmented simulator\n",
    "- implementes the `_infer_size`, `_post_process` to infer the data dimensions and to post process the predictions (inverse scaling to find the original values)\n",
    "- implements `_save_metadata` and `_load_metadata` to save and load all the specific metadata of the implemented model\n",
    "\n",
    "More information is given on the documentation. And a fully working example is given in the `TfFullyConnected` class.\n",
    "\n",
    "\n",
    "### Pytorch\n",
    "For an augmented simulator based on `Pytorch`, the only difference resides in the first step where you can directly use pytorch logic, however, the remaining steps remains very similar:\n",
    "\n",
    "- Inherit from `torch.nn.Module`\n",
    "- Implements `build_model` to defin the architecture\n",
    "- Implements `forward` to define the interaction between modules\n",
    "- Implements `process_data`  which prepare and scale the data for the specific augmented simulator\n",
    "- implementes the `_infer_size`, `_post_process` to infer the data dimensions and to post process the predictions (inverse scaling to find the original values)\n",
    "- implements `_save_metadata` and `_load_metadata` to save and load all the specific metadata of the implemented model\n",
    "\n",
    "This is it, nothing more is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide also a **complete execution** example in Notebook 4 using both `Tensorflow` and `Pytorch` libraries:\n",
    "\n",
    "- Selecting a benchmark\n",
    "- Learn an augmented simulator on benchmark dataset\n",
    "- Evaluation of augmented simulator using selected evaluation criteria from 4 categories\n",
    "- Analysis and comparison of results\n",
    "\n",
    "Go to the Next notebook to learn more about it [$\\rightarrow$](./04_Complete_example.ipynb).\n",
    "\n",
    "Got to the previous notebook to see the examples of evaluation [$\\leftarrow$](./02_EvaluateBaseline.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
