{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Baselines (AirfRANS use case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to demonstrate how we can evaluate the results of a baseline on a given benchmark.\n",
    "\n",
    "We will show how to load a baseline (or any other `AugmentedSimulator`) and evaluate it on a `Benchmark` of our choice.\n",
    "\n",
    "**To learn more about the training procedure, visit [this notebook](../03_TrainAnAugmentedSimulator.ipynb)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lips import get_root_path\n",
    "from lips.dataset.airfransDataSet import download_data\n",
    "from lips.benchmark.airfransBenchmark import AirfRANSBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate required paths\n",
    "LIPS_PATH = get_root_path()\n",
    "DIRECTORY_NAME = 'Dataset'\n",
    "BENCHMARK_NAME = \"Case1\"\n",
    "BENCH_CONFIG_PATH = LIPS_PATH + os.path.join(\"..\",\"configurations\",\"airfrans\",\"benchmarks\",\"confAirfoil.ini\")\n",
    "SIM_CONFIG_PATH = LIPS_PATH + os.path.join(\"..\",\"configurations\",\"airfrans\",\"simulators\",\"torch_fc.ini\")\n",
    "LOG_PATH = LIPS_PATH + \"lips_logs.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial step: download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(root_path=\".\", directory_name=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Benchmark <a id=\"Case1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step: load the dataset\n",
    "\n",
    "A common dataset will be used for evaluate the two augmented simulator. This initial step aims at loading it once and for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark=AirfRANSBenchmark(benchmark_path = DIRECTORY_NAME,\n",
    "                            config_path = BENCH_CONFIG_PATH,\n",
    "                            benchmark_name = BENCHMARK_NAME,\n",
    "                            log_path = LOG_PATH)\n",
    "benchmark.load(path_train=DIRECTORY_NAME,path_test=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to verify the config is loaded appropriately for this benchmark\n",
    "print(\"Benchmark name: \", benchmark.config.section_name)\n",
    "print(\"Environment name: \", benchmark.config.get_option(\"env_name\"))\n",
    "print(\"Output attributes: \", benchmark.config.get_option(\"attr_x\"))\n",
    "print(\"Output attributes: \", benchmark.config.get_option(\"attr_y\"))\n",
    "print(\"Evaluation criteria: \")\n",
    "print(benchmark.config.get_option(\"eval_dict\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A baseline \"augmented simulator\" <a id=\"bench1-fc\"></a>\n",
    "\n",
    "Along with some dataset, we provide also some baseline (from a trained neural network). This baseline is made of a fully connected neural network that takes the available input of the airfrans case and tries to predict all the output of the simulator.\n",
    "\n",
    "The fully connected neural network is made of XXX layer each with YYY units.\n",
    "\n",
    "It is learned for KKK epochs on the training set of the `Case1`.\n",
    "\n",
    "First we need to load the baseline and initialize it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.augmented_simulators.torch_models.fully_connected import TorchFullyConnected\n",
    "from lips.augmented_simulators.torch_simulator import TorchSimulator\n",
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "augmented_simulator = TorchSimulator(name=\"torch_fc\",\n",
    "                                     model=TorchFullyConnected,\n",
    "                                     scaler=StandardScaler,\n",
    "                                     log_path=\"log_benchmark\",\n",
    "                                     device=\"cuda:0\",\n",
    "                                     bench_config_path=BENCH_CONFIG_PATH,\n",
    "                                     bench_config_name=BENCHMARK_NAME,\n",
    "                                     sim_config_path=SIM_CONFIG_PATH,\n",
    "                                     sim_config_name=\"DEFAULT\",\n",
    "                                     architecture_type=\"Classical\",\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_simulator.train(train_dataset=benchmark.train_dataset, epochs=1, train_batch_size=128000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate it on the test datasets of the benchmark. This is done by indicating the learned augmented simulator `augmented_simulator` as the argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_metrics = benchmark.evaluate_simulator(augmented_simulator=augmented_simulator,eval_batch_size=128000 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of an augmented simulator <a id=\"bench1-comp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can assess the performance of the \"augmented simulator\". For example, if we want to retrieve the MSE (mean squared error) on the test dataset we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_metrics = \"ML\"\n",
    "dataset_name = \"test\"\n",
    "print(\"Fully Connected Augmented Simulator\")\n",
    "print(f\"Dataset : {dataset_name}\")\n",
    "print(\"{:<10} : {}\".format(\"MSE\", fc_metrics[dataset_name][ML_metrics]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physic compliance\n",
    "A trained augmented simulator could make some errors when verifying physics compliances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physic_compliances = \"Physics\"\n",
    "dataset_name = \"test\"\n",
    "physical_metrics = fc_metrics[dataset_name][physic_compliances]\n",
    "print(physical_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
