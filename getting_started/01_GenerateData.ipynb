{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains how to generate the datasets for various designed scenarios in previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIPS_PATH = pathlib.Path().resolve().parent\n",
    "DATA_PATH = LIPS_PATH / \"reference_data\"\n",
    "LOG_PATH = LIPS_PATH / \"lips_logs.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_PATH.exists():\n",
    "    DATA_PATH.mkdir(mode=511, parents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIPS : Learning Industrial physical simulationbenchmark suite: the power grid case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmarking platform (as it can be observed in the Figure below) is composed of three main modules: Data, Benchmarking and Evaluation. As the names suggest, each module is specialized with its own set of functions for a specific task. The Data module is the one that helps to generate different dataset and distributions providing a high flexibility using a set of parameters. Once the datasets are generated, the benchmarking platform will take as entry the generated dataset and perform a required benchmark on it. Each benchmark could be precised via the inputs and outputs required for modeling and prediction. Finally, in order to evaluate the performance of each benchmark, the Evaluation module provides a set of functions to inspect the models from different point of views (physics compliances, machine learning metrics, adaptability and readiness.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below presents the scheme of benchmarking platform which is composed of three distinct parts : \n",
    "* DataSet: Generate some dataset for training and evaluation \n",
    "* Benchmark: coordinates between different parts and allows train and evaluate an agumented simulator \n",
    "* Evaluation: once the benchmark done, it allow to evaluate the performance with respect to various point of views\n",
    "\n",
    "![title](img/Benchmarking_scheme_v2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 1 : Risk assessment through contingency screening\n",
    "The first power grid benchmark tackles the step of risk assessment in power grids. The problem is to anticipate potential threats on the power grid (several hours ahead) and warn the operators accordingly. It simulates incidents (aka contingencies) involving various elements of the grid (such as the disconnection of a line/production unit), one by one. For each contingency, a risk (weakness of the grid) is detected when overloads are detected by the simulation engine on some lines. On a real grid, this scenario means running several dozens of thousands of simulations, thereby, computation time is critical, especially since this risk assessment is refreshed every 15 minutes. In this benchmark, the main physical variable we are interested in is the value of electric current in the lines (in amperes), because an overload is detected if this value exceeds a line-dependent threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create the associated benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.benchmark import PowerGridBenchmark\n",
    "benchmark1 = PowerGridBenchmark(benchmark_path=DATA_PATH,\n",
    "                                benchmark_name=\"Benchmark1\",\n",
    "                                load_data_set=False,\n",
    "                                config_path=None, # opt for default config path (benchmark/config.ini)\n",
    "                                log_path=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to generate the data, note that this step needs to be done only once (by the person in charge of releasing the benchmark).\n",
    "\n",
    "People that want to use this benchmark are after free to load the data (see the next notebook for an example)\n",
    "\n",
    "**NB** To generate the datasets, you need extra libraries (for example `grid2op`, `lightsim2grid` or `leap_net`). These libraries are not required to use this platform once the data are generated.\n",
    "\n",
    "**NB** This process takes a lot of time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 100000/100000 [14:44<00:00, 113.10it/s]\n",
      "val: 100%|██████████| 10000/10000 [02:23<00:00, 69.80it/s]\n",
      "test: 100%|██████████| 10000/10000 [02:37<00:00, 63.34it/s]\n",
      "test_ood_topo: 100%|██████████| 10000/10000 [09:10<00:00, 18.18it/s]\n"
     ]
    }
   ],
   "source": [
    "benchmark1.generate(nb_sample_train=int(1e5),\n",
    "                    nb_sample_val=int(1e4),\n",
    "                    nb_sample_test=int(1e4),\n",
    "                    nb_sample_test_ood_topo=int(1e4),\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 2 : Remedial action search\n",
    "Once risky contingencies are identified, we need to explore possible solutions (aka \"remedial actions\") to recommend suitable solutions to the grid operator. In this benchmark, a solution consists in predefined topological change on the grid. It is successful if the simulation run on this modified grid alleviates the previous overflow without generating any new problem. In this case, the computation space in practice is significantly smaller compared to benchmark 1 (only small subsets of remedial actions to test over in most cases, compared to every contingency simulations). We here target  10-100 simulation batches whatever the grid size. This only depends on the number of preselected action candidates by the operator for a given risk. This benchmark includes the prediction of a few more physical variables: active power flows $p^\\ell$ (in MW), current $a^\\ell$ (in A) voltages $v_{k}$ (in V) at both sides of each powerline. Although this is not sufficient to fully describe the grid from a physical point of view, it is enough to give the operator some confident insights about the underlying physical behavior. For instance, is an overload somehow rather local and related to neighboring voltage drop phenomena or possibly long-distance considering power flow transmission from large power plants?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firt we need to create the associated benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark2 = PowerGridBenchmark(benchmark_path=DATA_PATH,\n",
    "                                benchmark_name=\"Benchmark2\",\n",
    "                                load_data_set=False,\n",
    "                                config_path=None, # opt for default config path (benchmark/config.ini)\n",
    "                                log_path=LOG_PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|█████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [11:22<00:00, 146.51it/s]\n",
      "val: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [01:05<00:00, 153.48it/s]\n",
      "test: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:08<00:00, 77.62it/s]\n",
      "test_ood_topo: 100%|████████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:47<00:00, 59.74it/s]\n"
     ]
    }
   ],
   "source": [
    "benchmark2.generate(nb_sample_train=int(1e5),\n",
    "                    nb_sample_val=int(1e4),\n",
    "                    nb_sample_test=int(1e4),\n",
    "                    nb_sample_test_ood_topo=int(1e4),\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 3 : Validation of decision\n",
    "Once preferred solutions have been selected by the operator to alleviate an overflow, a last detailed simulation is run and studied more in depth by the operator, prior to apply the remedial actions on the actual power grid. The purpose of this operation is to ensure that no unforeseen event may make the grid collapse. In this application, the quality of prediction is decisive, to ensure that the adopted interventions will not violate the consistency within the network; therefore the surrogate simulator shall be nearly as good as the numerical solver. Here the main goal is to ensure the accuracy and consistency of the solution for 1-10 simulations. All physical variables of the physical problem shall be predicted: currents $a^\\ell$, active power flows $p^\\ell$, voltages $v_{k}$, reactive power flows $q^\\ell$, angles $\\theta_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">This benchmark is under progress</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark3 = PowerGridBenchmark(benchmark_path=DATA_PATH,\n",
    "                                benchmark_name=\"Benchmark3\",\n",
    "                                load_data_set=False,\n",
    "                                config_path=None, # opt for default config path (benchmark/config.ini)\n",
    "                                log_path=LOG_PATH) \n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|█████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [09:50<00:00, 169.27it/s]\n",
      "val: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:59<00:00, 168.38it/s]\n",
      "test: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [01:46<00:00, 93.64it/s]\n",
      "test_ood_topo: 100%|████████████████████████████████████████████████████████████████████████████████| 10000/10000 [05:24<00:00, 30.85it/s]\n"
     ]
    }
   ],
   "source": [
    "benchmark3.generate(nb_sample_train=int(1e5),\n",
    "                    nb_sample_val=int(1e4),\n",
    "                    nb_sample_test=int(1e4),\n",
    "                    nb_sample_test_ood_topo=int(1e4),\n",
    "                   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lips_gitlab",
   "language": "python",
   "name": "lips_gitlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
