{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OUTPUT_DIR = os.path.join(\"lips\",\"Outputs\")\n",
    "INPUT_DIR = os.path.join(\"lips\", \"Outputs\", \"Data\", \"test\")\n",
    "BENCHMARK_DIR = os.path.join(\"lips\", \"Outputs\", \"Benchmark 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from lips.dataset import PowerGridDataSet\n",
    "from lips.simulators import DCApproximation\n",
    "from lips.benchmark import Benchmark\n",
    "from lips.evaluation import Evaluation\n",
    "from lips.simulators import FullyConnectedNN\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cell below to produce a table of contents \n",
    "res = None\n",
    "try:\n",
    "    from jyquickhelper import add_notebook_menu\n",
    "    res = add_notebook_menu()\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Impossible to automatically add a menu / table of content to this notebook.\\nYou can download \\\"jyquickhelper\\\" package with: \\n\\\"pip install jyquickhelper\\\"\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "To generate the datasets, we use `PowerGridDataSet` class which is a sub-class of a more general `DataSet` class and is designed specifically for power grid data. It allows to generate one dataset at a time. We keep the DataSet class (therefore the platform) as generic as possible for its use in other research and industrial domains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PowerGridDataSet(env_name=\"l2rpn_case14_sandbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset.generate(nb_samples=int(1024) * 128,\n",
    "                             tag=\"test\",\n",
    "                             val_regex=\".*99[0-9].*\",\n",
    "                             use_lightsim_if_available=True,\n",
    "                             reference_number=0,\n",
    "                             agent_generator_name=\"random_nn1\",\n",
    "                             agent_parameters={\"p\": 0.5},\n",
    "                             skip_gameover=True,\n",
    "                             env_seed=1234,\n",
    "                             agent_seed=14,\n",
    "                             verbose=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save the generated dataset\n",
    "<span style=\"color:red\">In this version, I have stored a list of Grid2op `Observations`, but it takes a lot of space on disk !!! </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save(path=OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the generated dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset.load(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and evaluating a Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical Simulator (DC approximation)\n",
    "To assess the performance of a physical solver, one can use the DCApproximation available in `simulators` module. As opposed to augmented simulators, it does not require to be learned. Hence, only test dataset should be used to evaluate its performance. Various variables could be used as inputs and outputs of this solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_app = DCApproximation(dataset.env._init_grid_path,\n",
    "                         name=\"dc_approx\",\n",
    "                         attr_x=(\"prod_p\", \"prod_v\", \"load_p\", \"load_q\", \"topo_vect\"),  # input that will be given to the proxy\n",
    "                         attr_y=(\"a_or\", \"a_ex\", \"p_or\", \"p_ex\", \"q_or\", \"q_ex\", \"prod_q\", \"load_v\", \"v_or\", \"v_ex\"),\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DC approximator should be initialized with generated data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have reduced the size of experimentation, as it takes long time to be excuted for DC approximation\n",
    "dc_app.init(test_data[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once, intitialized, it allows to infer the output variables from the input ones using its `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, observations, predict_time = dc_app.predict(save_path=BENCHMARK_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "The `Evaluation` class allows to evaluate the performance of different simulators with respect to various criteria arosed from various domains. \n",
    "- Physic compliance\n",
    "- Machine Learning metrics\n",
    "- Generalization\n",
    "- Readiness (not yet implemented)\n",
    "\n",
    "In this new version of platform, one should activate or deactivate the verifications required for a specific benchmark, before passing an object of this class to the `Benchmark` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we start by creating an object of the Evaluation class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = Evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a dictionary of Metric Activation is provided using `get_active_dict()` function : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dict = evaluation.get_active_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(active_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activate the required verifications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we wish for benchmark 1 only test the ML metrics and to verify the positivity of current values, one possible way to active them is the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dict[\"evaluate_ML\"] = True\n",
    "active_dict[\"evaluate_physic\"][\"verify_current_pos\"] = True\n",
    "evaluation.set_active_dict(active_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(evaluation.get_active_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be verified as above, that these two tests are activated for the Benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "Once a dataset has been generated, a simulator has been trained and predicted the required results and an evaluator with required metrics is parameterized, the `Benchmark` class allows to assess the performance of the simulator with respect to the indicated metrics and criteria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen below, the Benchmark class, takes as input an object of the following classes:\n",
    "\n",
    "- PowerGridDataSet\n",
    "- DCApproximation\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dc = Benchmark(\"Risk assessment\", dataset, dc_app, evaluation, save_path=BENCHMARK_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction time of the corresponding simulator could be accessed rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dc.prediction_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can evaluate the simulator using all the indicated metrics in the previous section by activating the metrics. The choice allows to consider the real observations for physic compliance verifications. However, for Machine Learning metrics, the verification does not change by this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dc.evaluate_simulator(choice=\"real\",\n",
    "                                EL_tolerance=0.04,\n",
    "                                LCE_tolerance=1e-3,\n",
    "                                KCL_tolerance=1e-2,\n",
    "                                active_flow=True,\n",
    "                                save_path=None\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the metrics could be accessed via the member variables of Benchmark class as follows : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The list of all the available metrics\")\n",
    "pprint(benchmark_dc.metrics_ML.keys())\n",
    "print(\"********************\")\n",
    "print(\"MAE_avg\")\n",
    "pprint(benchmark_dc.metrics_ML[\"MAE_avg\"])\n",
    "print(\"MAPE\")\n",
    "pprint(benchmark_dc.metrics_ML[\"mape_avg\"])\n",
    "print(\"MAPE90\")\n",
    "pprint(benchmark_dc.metrics_ML[\"mape90\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Physic compliances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dc.metrics_physics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as there is no violation of current values, this metric does not contain any value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Physic compliance verification could also be performed for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dc.evaluate_simulator(choice=\"predictions\",\n",
    "                                EL_tolerance=0.04,\n",
    "                                LCE_tolerance=1e-3,\n",
    "                                KCL_tolerance=1e-2,\n",
    "                                active_flow=True,\n",
    "                                save_path=None\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen there is no violation for the positivity of current values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Critical evaluation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I activate the EL, LCE and KCL metrics from physic compliance category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dict = evaluation.get_active_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dict[\"evaluate_physic\"][\"verify_EL\"] = True\n",
    "active_dict[\"evaluate_physic\"][\"verify_LCE\"] = True\n",
    "active_dict[\"evaluate_physic\"][\"verify_KCL\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.set_active_dict(active_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(evaluation.get_active_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Now Evaluating these criteria using Benchmark class</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dc = Benchmark(\"Risk assessment\", dataset, dc_app, evaluation, save_path=BENCHMARK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dc.evaluate_simulator(choice=\"real\",\n",
    "                                EL_tolerance=0.04,\n",
    "                                LCE_tolerance=1e-3,\n",
    "                                KCL_tolerance=1e-2,\n",
    "                                active_flow=True,\n",
    "                                save_path=None\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no problem using Real Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dc.evaluate_simulator(choice=\"predictions\",\n",
    "                                EL_tolerance=0.04,\n",
    "                                LCE_tolerance=1e-3,\n",
    "                                KCL_tolerance=1e-2,\n",
    "                                active_flow=True,\n",
    "                                save_path=None\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen here that, therea 100% of the cases that DC violates the Law of Conservation of Energy and 7% of the cases that it violates the Kirchoff's current law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">I dont think that it could be a problem from the implementation point of view !</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
