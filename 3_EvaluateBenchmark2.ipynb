{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3871b7f",
   "metadata": {},
   "source": [
    "The goal of this notebook is to evaluate various augmneted simulators using the `NeuripsBenchmark2` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec3ed4",
   "metadata": {},
   "source": [
    "# Initial step: load the dataset (Benchmark2)\n",
    "\n",
    "A common dataset generated in the context of second benchmark will be used to evaluate the augmented simulators. This initial step aims at loading it once and for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc7cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from lips.neurips_benchmark import NeuripsBenchmark2\n",
    "path_benchmark = os.path.join(\"reference_data\")\n",
    "neurips_benchmark2 = NeuripsBenchmark2(path_benchmark=path_benchmark,\n",
    "                                       load_data_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0619d2",
   "metadata": {},
   "source": [
    "# The DC approximation\n",
    "\n",
    "We remind that the `grid2op` library is required for this part. You can install it with `pip install grid2op` if you do not have it already.\n",
    "\n",
    "First we will create the \"augmented simulator\". As opposed to the second model we will expose here, this method require access to a powergrid. This is one of the reason we need grid2op. \n",
    "\n",
    "The way to load each `AugmentedSimulator` is specific. Here for example we load the DCApproximation that will use the same powergrid as the one used to generate the data in the previous Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc3fb303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next few lines are specific for each benchmark and each `AugmentedSimulator`\n",
    "import grid2op\n",
    "import warnings\n",
    "from lips.augmented_simulators import DCApproximationAS\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    env = grid2op.make(\"l2rpn_case14_sandbox\", test=True)\n",
    "    grid_path = os.path.join(env.get_path_env(), \"grid.json\")\n",
    "\n",
    "dc_augmented_sim = DCApproximationAS(grid_path=grid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f75190",
   "metadata": {},
   "source": [
    "Now that the model is load, there is a common interface to evaluate its performance, on a dataset. This is showed in the cell bellow where we evaluate this specific `AugmentedSimulator` one this two dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db3debc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A log file including some verifications is created at root directory with the name logs.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milad\\Anaconda3\\envs\\RTE\\lib\\site-packages\\lightsim2grid\\_aux_add_trafo.py:33: UserWarning: There were some Nan in the pp_net.trafo[\"tap_neutral\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_neutral\\\"], they have been replaced by 0\")\n",
      "C:\\Users\\Milad\\Anaconda3\\envs\\RTE\\lib\\site-packages\\lightsim2grid\\_aux_add_trafo.py:41: UserWarning: There were some Nan in the pp_net.trafo[\"tap_step_percent\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_step_percent\\\"], they have been replaced by 0\")\n",
      "C:\\Users\\Milad\\Anaconda3\\envs\\RTE\\lib\\site-packages\\lightsim2grid\\_aux_add_trafo.py:46: UserWarning: There were some Nan in the pp_net.trafo[\"tap_pos\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_pos\\\"], they have been replaced by 0\")\n",
      "evaluate dc: 100%|████████████████████████████████████████████████████████████████████| 10000/10000 [02:35<00:00, 64.39it/s]\n",
      "evaluate dc: 100%|████████████████████████████████████████████████████████████████████| 10000/10000 [02:36<00:00, 63.90it/s]\n",
      "evaluate dc: 100%|████████████████████████████████████████████████████████████████████| 10000/10000 [02:36<00:00, 63.88it/s]\n"
     ]
    }
   ],
   "source": [
    "dc_metrics_per_dataset = neurips_benchmark2.evaluate_augmented_simulator(dc_augmented_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e3e3f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a_ex': 0.15019253158742082,\n",
      " 'a_or': 0.16548017823578715,\n",
      " 'p_ex': 0.056530448354166186,\n",
      " 'p_or': 0.06371198969813041,\n",
      " 'q_ex': 1.0,\n",
      " 'q_or': 1.0,\n",
      " 'v_ex': 0.029804464251625373,\n",
      " 'v_or': 0.020190230279398043}\n",
      "{'a_ex': 0.22866990772813062,\n",
      " 'a_or': 0.24527175106457233,\n",
      " 'load_v': 0.0342080770847991,\n",
      " 'p_ex': 0.0961026572346714,\n",
      " 'p_or': 0.10059158437961797,\n",
      " 'prod_q': 0.797376105973529,\n",
      " 'q_ex': 1.0,\n",
      " 'q_or': 1.0,\n",
      " 'v_ex': 0.035055280819845804,\n",
      " 'v_or': 0.022337829778029655}\n",
      "{'a_ex': 111.42586134788584,\n",
      " 'a_or': 88.95999453197763,\n",
      " 'load_v': 0.9770700236060402,\n",
      " 'p_ex': 1.0510166989904774,\n",
      " 'p_or': 1.2138148931696529,\n",
      " 'prod_q': 20.190512474052607,\n",
      " 'q_ex': 7.86762334338003,\n",
      " 'q_or': 7.886646210580362,\n",
      " 'v_ex': 1.0652534383106231,\n",
      " 'v_or': 0.7267051539993286}\n",
      "{'a_ex': 0.08111458764553084,\n",
      " 'a_or': 0.08362979429752207,\n",
      " 'load_v': 0.36094990402845295,\n",
      " 'p_ex': 0.012799259392983894,\n",
      " 'p_or': 0.014338744554198982,\n",
      " 'prod_q': 0.3454620270574093,\n",
      " 'q_ex': 0.20671481308690048,\n",
      " 'q_or': 0.23918371709528524,\n",
      " 'v_ex': 0.034581965828315644,\n",
      " 'v_or': 0.057553830674689554}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "ML_metrics = 0\n",
    "pprint(dc_metrics_per_dataset[\"test\"][ML_metrics][\"mape90\"])\n",
    "pprint(dc_metrics_per_dataset[\"test\"][ML_metrics][\"mape_avg\"])\n",
    "pprint(dc_metrics_per_dataset[\"test\"][ML_metrics][\"MAE_avg\"])\n",
    "pprint(dc_metrics_per_dataset[\"test\"][ML_metrics][\"NRMSE_avg\"])\n",
    "\n",
    "#pprint(dc_metrics_per_dataset[\"test\"][ML_metrics][\"mape_avg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99d4512a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a_ex': 0.1797100506098785,\n",
      " 'a_or': 0.19509703890945235,\n",
      " 'p_ex': 0.06612898586333123,\n",
      " 'p_or': 0.07204024266980322,\n",
      " 'q_ex': 1.0,\n",
      " 'q_or': 1.0,\n",
      " 'v_ex': 0.03229111725430374,\n",
      " 'v_or': 0.021471273653875567}\n",
      "{'a_ex': 0.2440762191605486,\n",
      " 'a_or': 0.2627461946447337,\n",
      " 'load_v': 0.03472837203215465,\n",
      " 'p_ex': 0.10271137816391421,\n",
      " 'p_or': 0.1068070144424016,\n",
      " 'prod_q': 0.7653422728515755,\n",
      " 'q_ex': 1.0,\n",
      " 'q_or': 1.0,\n",
      " 'v_ex': 0.036822367231527396,\n",
      " 'v_or': 0.022693847255619494}\n",
      "{'a_ex': 127.59597935117668,\n",
      " 'a_or': 101.75602431680647,\n",
      " 'load_v': 1.011228824667497,\n",
      " 'p_ex': 1.2329470103949232,\n",
      " 'p_or': 1.480156619568586,\n",
      " 'prod_q': 21.601368773038093,\n",
      " 'q_ex': 9.104607537559989,\n",
      " 'q_or': 8.936631214781038,\n",
      " 'v_ex': 1.208833802871704,\n",
      " 'v_or': 0.7985767858791352}\n",
      "{'a_ex': 0.09347048235003834,\n",
      " 'a_or': 0.09660141290755525,\n",
      " 'load_v': 0.38698228015553293,\n",
      " 'p_ex': 0.015362700529026089,\n",
      " 'p_or': 0.01742026803691033,\n",
      " 'prod_q': 0.3092724178162995,\n",
      " 'q_ex': 0.23363312298437117,\n",
      " 'q_or': 0.2691938564033526,\n",
      " 'v_ex': 0.03666851467682115,\n",
      " 'v_or': 0.05695227582729508}\n"
     ]
    }
   ],
   "source": [
    "pprint(dc_metrics_per_dataset[\"test_ood_topo\"][ML_metrics][\"mape90\"])\n",
    "pprint(dc_metrics_per_dataset[\"test_ood_topo\"][ML_metrics][\"mape_avg\"])\n",
    "pprint(dc_metrics_per_dataset[\"test_ood_topo\"][ML_metrics][\"MAE_avg\"])\n",
    "pprint(dc_metrics_per_dataset[\"test_ood_topo\"][ML_metrics][\"NRMSE_avg\"])\n",
    "\n",
    "#pprint(dc_metrics_per_dataset[\"test_ood_topo\"][ML_metrics][\"mape_avg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b55e29dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pos\n",
      "{}\n",
      "voltage pos\n",
      "{}\n",
      "loss\n",
      "{}\n",
      "line_status\n",
      "{'a_ex_not_null': 0.0,\n",
      " 'a_or_not_null': 0.0,\n",
      " 'a_violations': 0.0,\n",
      " 'p_ex_not_null': 0.0,\n",
      " 'p_or_not_null': 0,\n",
      " 'p_violations': 0.0,\n",
      " 'q_ex_not_null': 0.0,\n",
      " 'q_or_not_null': 0.0,\n",
      " 'q_violations': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"current pos\")\n",
    "pprint(dc_metrics_per_dataset[\"test_ood_topo\"][1][\"BasicVerifications\"][\"currents\"])\n",
    "print(\"voltage pos\")\n",
    "pprint(dc_metrics_per_dataset[\"test_ood_topo\"][1][\"BasicVerifications\"][\"voltages\"])\n",
    "print(\"loss\")\n",
    "pprint(dc_metrics_per_dataset[\"test_ood_topo\"][1][\"BasicVerifications\"][\"loss\"])\n",
    "print(\"line_status\")\n",
    "pprint(dc_metrics_per_dataset[\"test_ood_topo\"][1][\"BasicVerifications\"][\"line_status\"])\n",
    "#pprint(dc_metrics_per_dataset[\"test_ood_topo\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7baeba",
   "metadata": {},
   "source": [
    "# Training an augmented simulator \n",
    "In this benchmark the augmented simulators will predict all the power flow related variables (active and reactive powers, voltages and currents) contrary to the first benchmark which was concentrated only on currents.\n",
    "\n",
    "## Fully connected architecture\n",
    "In this section we explain how to tune an available model. We take the example of the `FullyConnectedAS` that is an available fully connected neural network.\n",
    "\n",
    "The first step is to create the class you want to use, with the meta parameters you want to test. For this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec9845e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = os.path.join(\"trained_models\")\n",
    "if not os.path.exists(path_save):\n",
    "    os.mkdir(path_save)\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from lips.augmented_simulators import FullyConnectedAS\n",
    "\n",
    "# the three lines bellow might be familiar to the tensorflow users. They tell tensorflow to not take all\n",
    "# the GPU video RAM for the model.\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for el in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(el, True)\n",
    "\n",
    "my_simulator = FullyConnectedAS(name=\"test_FullyConnectedAS_benchmark2\",\n",
    "                                # `attr_x` represents the variables of the dataset you want to use to predict \n",
    "                                # the output.\n",
    "                                attr_x=(\"prod_p\", \"prod_v\", \"load_p\", \"load_q\", \"line_status\", \"topo_vect\"),\n",
    "                                # `attr_y` represents the variables of the dataset you want to predict\n",
    "                                # we predict everything needed, you can try to change them if you want, add some \n",
    "                                # others etc.\n",
    "                                attr_y=(\"a_or\", \"a_ex\", \"p_or\", \"p_ex\", \"q_ex\", \"q_or\", \"v_or\", \"v_ex\"),\n",
    "                                # `sizes_layer` represents the size of each hidden layer in the neural network. The number\n",
    "                                # of layers is determined by the length of this list, for example\n",
    "                                sizes_layer=(300, 300, 300, 300),\n",
    "                                # `lr` is the learning rate\n",
    "                                lr=3e-4, \n",
    "                                # `layer` is the type of keras layer you want to use. We don't recommend to \n",
    "                                # change it\n",
    "                                layer=Dense,\n",
    "                                # `layer_act` is the activation function you want to use after each layer\n",
    "                                layer_act=\"relu\",\n",
    "                                # `loss` is the training loss\n",
    "                                loss=\"mse\",  # loss used to train the model\n",
    "                                # `batch_size` is the size of the batch for training\n",
    "                                batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ae810",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90fabd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2779 - val_loss: 0.0184\n",
      "Epoch 2/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0119 - val_loss: 0.0086\n",
      "Epoch 3/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0062 - val_loss: 0.0056\n",
      "Epoch 4/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 5/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 6/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0027 - val_loss: 0.0035\n",
      "Epoch 7/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 8/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 9/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 10/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 11/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 12/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 13/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 14/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 15/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 16/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 17/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 9.6725e-04 - val_loss: 0.0011\n",
      "Epoch 18/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 9.1613e-04 - val_loss: 0.0011\n",
      "Epoch 19/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 8.9301e-04 - val_loss: 9.9078e-04\n",
      "Epoch 20/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 8.4538e-04 - val_loss: 0.0010\n",
      "Epoch 21/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 8.2825e-04 - val_loss: 8.3440e-04\n",
      "Epoch 22/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 8.3678e-04 - val_loss: 0.0011\n",
      "Epoch 23/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 7.8074e-04 - val_loss: 9.5982e-04\n",
      "Epoch 24/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 7.2912e-04 - val_loss: 7.9662e-04\n",
      "Epoch 25/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 7.2482e-04 - val_loss: 8.9176e-04\n",
      "Epoch 26/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 7.2691e-04 - val_loss: 7.4509e-04\n",
      "Epoch 27/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 6.7177e-04 - val_loss: 0.0011\n",
      "Epoch 28/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 6.9088e-04 - val_loss: 8.4411e-04\n",
      "Epoch 29/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 6.6093e-04 - val_loss: 9.3348e-04\n",
      "Epoch 30/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 6.5163e-04 - val_loss: 6.7833e-04\n",
      "Epoch 31/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 6.5270e-04 - val_loss: 7.2842e-04\n",
      "Epoch 32/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 6.4700e-04 - val_loss: 7.2014e-04\n",
      "Epoch 33/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 6.5153e-04 - val_loss: 6.7194e-04\n",
      "Epoch 34/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.8426e-04 - val_loss: 7.1024e-04\n",
      "Epoch 35/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 6.1802e-04 - val_loss: 7.0817e-04\n",
      "Epoch 36/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.8245e-04 - val_loss: 6.6888e-04\n",
      "Epoch 37/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.8296e-04 - val_loss: 7.0596e-04\n",
      "Epoch 38/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.9894e-04 - val_loss: 6.8123e-04\n",
      "Epoch 39/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.4530e-04 - val_loss: 6.8441e-04\n",
      "Epoch 40/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.4260e-04 - val_loss: 6.2865e-04\n",
      "Epoch 41/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.5013e-04 - val_loss: 6.8556e-04\n",
      "Epoch 42/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.6200e-04 - val_loss: 5.1615e-04\n",
      "Epoch 43/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.3751e-04 - val_loss: 7.6971e-04\n",
      "Epoch 44/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.9786e-04 - val_loss: 6.5847e-04\n",
      "Epoch 45/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.9987e-04 - val_loss: 6.4705e-04\n",
      "Epoch 46/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.9692e-04 - val_loss: 6.8989e-04\n",
      "Epoch 47/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.1786e-04 - val_loss: 6.8527e-04\n",
      "Epoch 48/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.1683e-04 - val_loss: 5.4839e-04\n",
      "Epoch 49/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.5564e-04 - val_loss: 5.9760e-04\n",
      "Epoch 50/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.8066e-04 - val_loss: 5.0167e-04\n",
      "Epoch 51/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.6531e-04 - val_loss: 6.5652e-04\n",
      "Epoch 52/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.8918e-04 - val_loss: 5.2162e-04\n",
      "Epoch 53/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.6800e-04 - val_loss: 5.8533e-04\n",
      "Epoch 54/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.6789e-04 - val_loss: 7.4190e-04\n",
      "Epoch 55/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.6414e-04 - val_loss: 5.1102e-04\n",
      "Epoch 56/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.6710e-04 - val_loss: 6.0326e-04\n",
      "Epoch 57/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.4217e-04 - val_loss: 6.7501e-04\n",
      "Epoch 58/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.1812e-04 - val_loss: 5.0510e-04\n",
      "Epoch 59/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.4744e-04 - val_loss: 5.3828e-04\n",
      "Epoch 60/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.5800e-04 - val_loss: 6.6403e-04\n",
      "Epoch 61/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.1829e-04 - val_loss: 5.8757e-04\n",
      "Epoch 62/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.2576e-04 - val_loss: 5.8642e-04\n",
      "Epoch 63/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.4674e-04 - val_loss: 4.9290e-04\n",
      "Epoch 64/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.2069e-04 - val_loss: 6.2389e-04\n",
      "Epoch 65/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.3668e-04 - val_loss: 4.8755e-04\n",
      "Epoch 66/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.0066e-04 - val_loss: 4.0430e-04\n",
      "Epoch 67/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.0036e-04 - val_loss: 4.5889e-04\n",
      "Epoch 68/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.9916e-04 - val_loss: 5.5153e-04\n",
      "Epoch 69/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.0377e-04 - val_loss: 6.9187e-04\n",
      "Epoch 70/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.0048e-04 - val_loss: 5.2103e-04\n",
      "Epoch 71/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.9460e-04 - val_loss: 4.3780e-04\n",
      "Epoch 72/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.9230e-04 - val_loss: 3.9212e-04\n",
      "Epoch 73/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.6918e-04 - val_loss: 4.1738e-04\n",
      "Epoch 74/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.9524e-04 - val_loss: 4.5875e-04\n",
      "Epoch 75/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.8092e-04 - val_loss: 5.0980e-04\n",
      "Epoch 76/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.9567e-04 - val_loss: 4.0927e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.7698e-04 - val_loss: 4.9359e-04\n",
      "Epoch 78/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.8879e-04 - val_loss: 5.6709e-04\n",
      "Epoch 79/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.7111e-04 - val_loss: 3.9232e-04\n",
      "Epoch 80/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.5583e-04 - val_loss: 4.0238e-04\n",
      "Epoch 81/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.5614e-04 - val_loss: 4.4457e-04\n",
      "Epoch 82/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.6376e-04 - val_loss: 5.1984e-04\n",
      "Epoch 83/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.8294e-04 - val_loss: 4.1710e-04\n",
      "Epoch 84/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.6174e-04 - val_loss: 4.1503e-04\n",
      "Epoch 85/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.6413e-04 - val_loss: 3.7529e-04\n",
      "Epoch 86/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.5631e-04 - val_loss: 4.3918e-04\n",
      "Epoch 87/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.6327e-04 - val_loss: 4.1024e-04\n",
      "Epoch 88/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.5440e-04 - val_loss: 3.8731e-04\n",
      "Epoch 89/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.4346e-04 - val_loss: 4.1546e-04\n",
      "Epoch 90/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.2737e-04 - val_loss: 3.8453e-04\n",
      "Epoch 91/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.5567e-04 - val_loss: 3.8583e-04\n",
      "Epoch 92/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.3874e-04 - val_loss: 4.2371e-04\n",
      "Epoch 93/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.3594e-04 - val_loss: 4.6632e-04\n",
      "Epoch 94/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.7111e-04 - val_loss: 4.4917e-04\n",
      "Epoch 95/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.3345e-04 - val_loss: 3.8963e-04\n",
      "Epoch 96/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.2948e-04 - val_loss: 5.0328e-04\n",
      "Epoch 97/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.2480e-04 - val_loss: 4.3044e-04\n",
      "Epoch 98/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.5644e-04 - val_loss: 4.8489e-04\n",
      "Epoch 99/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.0885e-04 - val_loss: 5.3512e-04\n",
      "Epoch 100/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.3602e-04 - val_loss: 4.9608e-04\n",
      "Epoch 101/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.6354e-04 - val_loss: 3.8211e-04\n",
      "Epoch 102/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.2633e-04 - val_loss: 3.7216e-04\n",
      "Epoch 103/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9885e-04 - val_loss: 4.4174e-04\n",
      "Epoch 104/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.0701e-04 - val_loss: 4.8004e-04\n",
      "Epoch 105/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.4515e-04 - val_loss: 4.0196e-04\n",
      "Epoch 106/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.0560e-04 - val_loss: 4.3454e-04\n",
      "Epoch 107/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9947e-04 - val_loss: 3.5499e-04\n",
      "Epoch 108/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.2052e-04 - val_loss: 3.4193e-04\n",
      "Epoch 109/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.1003e-04 - val_loss: 3.3017e-04\n",
      "Epoch 110/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9836e-04 - val_loss: 4.5200e-04\n",
      "Epoch 111/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.5116e-04 - val_loss: 3.0893e-04\n",
      "Epoch 112/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9364e-04 - val_loss: 3.5039e-04\n",
      "Epoch 113/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9342e-04 - val_loss: 4.5227e-04\n",
      "Epoch 114/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.3362e-04 - val_loss: 4.1494e-04\n",
      "Epoch 115/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9595e-04 - val_loss: 4.4229e-04\n",
      "Epoch 116/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.1926e-04 - val_loss: 3.8331e-04\n",
      "Epoch 117/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.1762e-04 - val_loss: 3.3545e-04\n",
      "Epoch 118/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9660e-04 - val_loss: 3.9720e-04\n",
      "Epoch 119/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7724e-04 - val_loss: 4.0703e-04\n",
      "Epoch 120/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.8842e-04 - val_loss: 4.1578e-04\n",
      "Epoch 121/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.0135e-04 - val_loss: 5.8436e-04\n",
      "Epoch 122/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.3037e-04 - val_loss: 4.2027e-04\n",
      "Epoch 123/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.8693e-04 - val_loss: 3.7674e-04\n",
      "Epoch 124/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.8512e-04 - val_loss: 3.1133e-04\n",
      "Epoch 125/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6444e-04 - val_loss: 3.8573e-04\n",
      "Epoch 126/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9730e-04 - val_loss: 3.0319e-04\n",
      "Epoch 127/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.8661e-04 - val_loss: 4.3273e-04\n",
      "Epoch 128/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9051e-04 - val_loss: 3.7193e-04\n",
      "Epoch 129/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7416e-04 - val_loss: 3.3983e-04\n",
      "Epoch 130/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7226e-04 - val_loss: 3.9270e-04\n",
      "Epoch 131/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9171e-04 - val_loss: 5.0136e-04\n",
      "Epoch 132/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.0276e-04 - val_loss: 3.5940e-04\n",
      "Epoch 133/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.8843e-04 - val_loss: 3.0547e-04\n",
      "Epoch 134/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6936e-04 - val_loss: 3.7071e-04\n",
      "Epoch 135/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7276e-04 - val_loss: 3.9417e-04\n",
      "Epoch 136/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.0843e-04 - val_loss: 2.8346e-04\n",
      "Epoch 137/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5882e-04 - val_loss: 3.5988e-04\n",
      "Epoch 138/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7535e-04 - val_loss: 3.7916e-04\n",
      "Epoch 139/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9605e-04 - val_loss: 5.1752e-04\n",
      "Epoch 140/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.5310e-04 - val_loss: 3.5297e-04\n",
      "Epoch 141/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6657e-04 - val_loss: 3.2658e-04\n",
      "Epoch 142/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7300e-04 - val_loss: 3.2125e-04\n",
      "Epoch 143/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6921e-04 - val_loss: 2.8705e-04\n",
      "Epoch 144/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6741e-04 - val_loss: 3.0896e-04\n",
      "Epoch 145/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5392e-04 - val_loss: 3.8857e-04\n",
      "Epoch 146/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6203e-04 - val_loss: 2.6300e-04\n",
      "Epoch 147/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4684e-04 - val_loss: 3.0645e-04\n",
      "Epoch 148/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5244e-04 - val_loss: 4.1672e-04\n",
      "Epoch 149/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6694e-04 - val_loss: 4.0463e-04\n",
      "Epoch 150/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6343e-04 - val_loss: 3.8356e-04\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6175e-04 - val_loss: 3.4403e-04\n",
      "Epoch 152/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6499e-04 - val_loss: 2.6849e-04\n",
      "Epoch 153/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5129e-04 - val_loss: 3.9708e-04\n",
      "Epoch 154/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7220e-04 - val_loss: 4.0226e-04\n",
      "Epoch 155/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9172e-04 - val_loss: 2.7082e-04\n",
      "Epoch 156/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4793e-04 - val_loss: 5.2921e-04\n",
      "Epoch 157/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7936e-04 - val_loss: 3.6211e-04\n",
      "Epoch 158/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6152e-04 - val_loss: 3.2016e-04\n",
      "Epoch 159/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7471e-04 - val_loss: 3.6700e-04\n",
      "Epoch 160/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6009e-04 - val_loss: 3.7933e-04\n",
      "Epoch 161/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6078e-04 - val_loss: 2.8930e-04\n",
      "Epoch 162/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4833e-04 - val_loss: 5.0452e-04\n",
      "Epoch 163/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7062e-04 - val_loss: 2.6530e-04\n",
      "Epoch 164/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2647e-04 - val_loss: 2.8681e-04\n",
      "Epoch 165/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4875e-04 - val_loss: 2.8031e-04\n",
      "Epoch 166/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7667e-04 - val_loss: 2.6307e-04\n",
      "Epoch 167/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3411e-04 - val_loss: 4.1129e-04\n",
      "Epoch 168/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4100e-04 - val_loss: 2.9262e-04\n",
      "Epoch 169/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7022e-04 - val_loss: 3.0539e-04\n",
      "Epoch 170/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5827e-04 - val_loss: 2.7819e-04\n",
      "Epoch 171/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4097e-04 - val_loss: 2.4080e-04\n",
      "Epoch 172/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3319e-04 - val_loss: 3.6743e-04\n",
      "Epoch 173/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5629e-04 - val_loss: 2.9087e-04\n",
      "Epoch 174/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2876e-04 - val_loss: 3.0710e-04\n",
      "Epoch 175/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4177e-04 - val_loss: 3.1228e-04\n",
      "Epoch 176/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4772e-04 - val_loss: 3.6943e-04\n",
      "Epoch 177/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6961e-04 - val_loss: 2.4622e-04\n",
      "Epoch 178/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3274e-04 - val_loss: 2.8241e-04\n",
      "Epoch 179/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3076e-04 - val_loss: 3.9184e-04\n",
      "Epoch 180/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5923e-04 - val_loss: 2.5829e-04\n",
      "Epoch 181/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3981e-04 - val_loss: 3.2035e-04\n",
      "Epoch 182/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4134e-04 - val_loss: 2.3453e-04\n",
      "Epoch 183/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2088e-04 - val_loss: 3.1478e-04\n",
      "Epoch 184/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3099e-04 - val_loss: 2.6242e-04\n",
      "Epoch 185/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3105e-04 - val_loss: 2.5202e-04\n",
      "Epoch 186/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.0434e-04 - val_loss: 3.1383e-04\n",
      "Epoch 187/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3061e-04 - val_loss: 2.6590e-04\n",
      "Epoch 188/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3789e-04 - val_loss: 2.9927e-04\n",
      "Epoch 189/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6449e-04 - val_loss: 2.7348e-04\n",
      "Epoch 190/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3413e-04 - val_loss: 3.0952e-04\n",
      "Epoch 191/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3052e-04 - val_loss: 3.1129e-04\n",
      "Epoch 192/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2003e-04 - val_loss: 2.9742e-04\n",
      "Epoch 193/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2648e-04 - val_loss: 2.4973e-04\n",
      "Epoch 194/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.1554e-04 - val_loss: 2.5220e-04\n",
      "Epoch 195/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.1353e-04 - val_loss: 2.7776e-04\n",
      "Epoch 196/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3871e-04 - val_loss: 2.2641e-04\n",
      "Epoch 197/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.0582e-04 - val_loss: 2.9276e-04\n",
      "Epoch 198/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4643e-04 - val_loss: 2.3129e-04\n",
      "Epoch 199/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2566e-04 - val_loss: 2.7952e-04\n",
      "Epoch 200/200\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3528e-04 - val_loss: 2.3060e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f7d19da880>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_simulator.train(nb_iter=200,\n",
    "                   train_dataset=neurips_benchmark2.train_dataset,\n",
    "                   val_dataset=neurips_benchmark2.val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b2852c",
   "metadata": {},
   "source": [
    "Save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bb94d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_simulator.save(path_save)\n",
    "my_simulator.save_metadata(path_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce44d20",
   "metadata": {},
   "source": [
    "load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4070200",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_trained_models = os.path.join(\"trained_models\")\n",
    "from lips.augmented_simulators import FullyConnectedAS\n",
    "\n",
    "# recreate the baseline\n",
    "fc_augmented_sim = FullyConnectedAS(name=\"test_FullyConnectedAS_benchmark2\")\n",
    "\n",
    "# TODO create a wrapper for these 3 calls\n",
    "fc_augmented_sim.load_metadata(path_trained_models)\n",
    "fc_augmented_sim.init()\n",
    "fc_augmented_sim.restore(path_trained_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370cd3ba",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a17a0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A log file including some verifications is created at root directory with the name logs.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milad\\Anaconda3\\envs\\RTE\\lib\\site-packages\\lightsim2grid\\_aux_add_trafo.py:33: UserWarning: There were some Nan in the pp_net.trafo[\"tap_neutral\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_neutral\\\"], they have been replaced by 0\")\n",
      "C:\\Users\\Milad\\Anaconda3\\envs\\RTE\\lib\\site-packages\\lightsim2grid\\_aux_add_trafo.py:41: UserWarning: There were some Nan in the pp_net.trafo[\"tap_step_percent\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_step_percent\\\"], they have been replaced by 0\")\n",
      "C:\\Users\\Milad\\Anaconda3\\envs\\RTE\\lib\\site-packages\\lightsim2grid\\_aux_add_trafo.py:46: UserWarning: There were some Nan in the pp_net.trafo[\"tap_pos\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_pos\\\"], they have been replaced by 0\")\n"
     ]
    }
   ],
   "source": [
    "fc_metrics_per_dataset = neurips_benchmark2.evaluate_augmented_simulator(fc_augmented_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57623c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE90\n",
      "{'a_ex': 0.004216099339195946,\n",
      " 'a_or': 0.004172849978792349,\n",
      " 'p_ex': 0.005538751532104843,\n",
      " 'p_or': 0.005705857400108831,\n",
      " 'q_ex': 0.011805967906086496,\n",
      " 'q_or': 0.014627153405859978,\n",
      " 'v_ex': 0.001959765167686472,\n",
      " 'v_or': 0.002034832031918428}\n",
      "MAPE\n",
      "{'a_ex': 0.009964194116832135,\n",
      " 'a_or': 0.009919094485366917,\n",
      " 'p_ex': 0.012828909598457196,\n",
      " 'p_or': 0.01275745689491746,\n",
      " 'q_ex': 0.013614666598429925,\n",
      " 'q_or': 0.013146275491064318,\n",
      " 'v_ex': 0.00171007131704655,\n",
      " 'v_or': 0.0017662454586961345}\n",
      "MAE\n",
      "{'a_ex': 2.262838363647461,\n",
      " 'a_or': 1.578246831893921,\n",
      " 'p_ex': 0.13938064873218536,\n",
      " 'p_or': 0.13529491424560547,\n",
      " 'q_ex': 0.06099017709493637,\n",
      " 'q_or': 0.06014641001820564,\n",
      " 'v_ex': 0.10970894247293472,\n",
      " 'v_or': 0.141921266913414}\n",
      "NRMSE\n",
      "{'a_ex': 0.002238247077912092,\n",
      " 'a_or': 0.002220422262325883,\n",
      " 'p_ex': 0.0018839167896658182,\n",
      " 'p_or': 0.0018375462386757135,\n",
      " 'q_ex': 0.0017730891704559326,\n",
      " 'q_or': 0.0020277563016861677,\n",
      " 'v_ex': 0.002387902233749628,\n",
      " 'v_or': 0.00238691340200603}\n"
     ]
    }
   ],
   "source": [
    "ML_metrics = 0\n",
    "print(\"MAPE90\")\n",
    "pprint(fc_metrics_per_dataset[\"test\"][ML_metrics][\"mape90\"])\n",
    "print(\"MAPE\")\n",
    "pprint(fc_metrics_per_dataset[\"test\"][ML_metrics][\"mape_avg\"])\n",
    "print(\"MAE\")\n",
    "pprint(fc_metrics_per_dataset[\"test\"][ML_metrics][\"MAE_avg\"])\n",
    "print(\"NRMSE\")\n",
    "pprint(fc_metrics_per_dataset[\"test\"][ML_metrics][\"NRMSE_avg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46290d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pos\n",
      "0.025555\n",
      "0.02489\n",
      "voltage pos\n",
      "0.021735\n",
      "0.02348\n",
      "loss\n",
      "0.212615\n",
      "line_status\n",
      "{'a_ex_not_null': 9507.0,\n",
      " 'a_or_not_null': 9507.0,\n",
      " 'a_violations': 1.0,\n",
      " 'p_ex_not_null': 9507.0,\n",
      " 'p_or_not_null': 9507,\n",
      " 'p_violations': 1.0,\n",
      " 'q_ex_not_null': 9507.0,\n",
      " 'q_or_not_null': 9507.0,\n",
      " 'q_violations': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"current pos\")\n",
    "pprint(fc_metrics_per_dataset[\"test\"][1][\"BasicVerifications\"][\"currents\"][\"a_or\"][\"Violation_proportion\"])\n",
    "pprint(fc_metrics_per_dataset[\"test\"][1][\"BasicVerifications\"][\"currents\"][\"a_ex\"][\"Violation_proportion\"])\n",
    "print(\"voltage pos\")\n",
    "pprint(fc_metrics_per_dataset[\"test\"][1][\"BasicVerifications\"][\"voltages\"][\"v_or\"][\"Violation_proportion\"])\n",
    "pprint(fc_metrics_per_dataset[\"test\"][1][\"BasicVerifications\"][\"voltages\"][\"v_ex\"][\"Violation_proportion\"])\n",
    "\n",
    "print(\"loss\")\n",
    "pprint(fc_metrics_per_dataset[\"test\"][1][\"BasicVerifications\"][\"loss\"][\"violation_proportion\"])\n",
    "print(\"line_status\")\n",
    "pprint(fc_metrics_per_dataset[\"test\"][1][\"BasicVerifications\"][\"line_status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c1c932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE90\n",
      "{'a_ex': 0.169776333652657,\n",
      " 'a_or': 0.1695665220586458,\n",
      " 'p_ex': 0.2341240827940494,\n",
      " 'p_or': 0.2374555024265573,\n",
      " 'q_ex': 0.35203193153846024,\n",
      " 'q_or': 0.3160063239629819,\n",
      " 'v_ex': 0.021375940754753554,\n",
      " 'v_or': 0.021196424271022203}\n",
      "MAPE\n",
      "{'a_ex': 0.25960725274404656,\n",
      " 'a_or': 0.2703047559866053,\n",
      " 'p_ex': 0.39877692303950524,\n",
      " 'p_or': 0.40412361135118485,\n",
      " 'q_ex': 0.3591838664234929,\n",
      " 'q_or': 0.30226013241270966,\n",
      " 'v_ex': 0.01769691158944977,\n",
      " 'v_or': 0.017760557348702487}\n",
      "MAE\n",
      "{'a_ex': 68.91883850097656,\n",
      " 'a_or': 46.69008255004883,\n",
      " 'p_ex': 5.042790412902832,\n",
      " 'p_or': 5.156951904296875,\n",
      " 'q_ex': 2.133361339569092,\n",
      " 'q_or': 1.8387527465820312,\n",
      " 'v_ex': 1.1591956615447998,\n",
      " 'v_or': 1.5147844552993774}\n",
      "NRMSE\n",
      "{'a_ex': 0.06891918182373047,\n",
      " 'a_or': 0.06904122978448868,\n",
      " 'p_ex': 0.06481568515300751,\n",
      " 'p_or': 0.06483255326747894,\n",
      " 'q_ex': 0.06396433711051941,\n",
      " 'q_or': 0.06397207081317902,\n",
      " 'v_ex': 0.025661733001470566,\n",
      " 'v_or': 0.027623925358057022}\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPE90\")\n",
    "pprint(fc_metrics_per_dataset[\"test_ood_topo\"][ML_metrics][\"mape90\"])\n",
    "print(\"MAPE\")\n",
    "pprint(fc_metrics_per_dataset[\"test_ood_topo\"][ML_metrics][\"mape_avg\"])\n",
    "print(\"MAE\")\n",
    "pprint(fc_metrics_per_dataset[\"test_ood_topo\"][ML_metrics][\"MAE_avg\"])\n",
    "print(\"NRMSE\")\n",
    "pprint(fc_metrics_per_dataset[\"test_ood_topo\"][ML_metrics][\"NRMSE_avg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a16135c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pos\n",
      "0.014195\n",
      "0.01443\n",
      "voltage pos\n",
      "0.00647\n",
      "0.006635\n",
      "loss\n",
      "0.32857\n",
      "line_status\n",
      "{'a_ex_not_null': 9445.0,\n",
      " 'a_or_not_null': 9445.0,\n",
      " 'a_violations': 1.0,\n",
      " 'p_ex_not_null': 9445.0,\n",
      " 'p_or_not_null': 9445,\n",
      " 'p_violations': 1.0,\n",
      " 'q_ex_not_null': 9445.0,\n",
      " 'q_or_not_null': 9445.0,\n",
      " 'q_violations': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"current pos\")\n",
    "pprint(fc_metrics_per_dataset[\"test_ood_topo\"][1][\"BasicVerifications\"][\"currents\"][\"a_or\"][\"Violation_proportion\"])\n",
    "pprint(fc_metrics_per_dataset[\"test_ood_topo\"][1][\"BasicVerifications\"][\"currents\"][\"a_ex\"][\"Violation_proportion\"])\n",
    "print(\"voltage pos\")\n",
    "pprint(fc_metrics_per_dataset[\"test_ood_topo\"][1][\"BasicVerifications\"][\"voltages\"][\"v_or\"][\"Violation_proportion\"])\n",
    "pprint(fc_metrics_per_dataset[\"test_ood_topo\"][1][\"BasicVerifications\"][\"voltages\"][\"v_ex\"][\"Violation_proportion\"])\n",
    "\n",
    "print(\"loss\")\n",
    "pprint(fc_metrics_per_dataset[\"test_ood_topo\"][1][\"BasicVerifications\"][\"loss\"][\"violation_proportion\"])\n",
    "print(\"line_status\")\n",
    "pprint(fc_metrics_per_dataset[\"test_ood_topo\"][1][\"BasicVerifications\"][\"line_status\"])\n",
    "#pprint(dc_metrics_per_dataset[\"test_ood_topo\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8943e5f",
   "metadata": {},
   "source": [
    "## LeapNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d9ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = os.path.join(\"trained_models\")\n",
    "if not os.path.exists(path_save):\n",
    "    os.mkdir(path_save)\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from lips.augmented_simulators import LeapNetAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ff2c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "leapNet = LeapNetAS(name=\"test_leapNetAS\",\n",
    "                    # `attr_x` represents the variables of the dataset you want to use to predict \n",
    "                    # the output.\n",
    "                    attr_x=(\"prod_p\", \"prod_v\", \"load_p\", \"load_q\"),\n",
    "                    # `attr_y` represents the variables of the dataset you want to predict\n",
    "                    # we predict everything needed, you can try to change them if you want, add some \n",
    "                    # others etc.\n",
    "                    attr_y=(\"a_or\", \"a_ex\", \"p_or\", \"p_ex\", \"q_ex\", \"q_or\", \"v_or\", \"v_ex\"),\n",
    "                    # `lr` is the learning rate\n",
    "                    lr=3e-4, \n",
    "                    # `layer` is the type of keras layer you want to use. We don't recommend to \n",
    "                    # change it\n",
    "                    layer=Dense,\n",
    "                    # `layer_act` is the activation function you want to use after each layer\n",
    "                    layer_act=\"relu\",\n",
    "                    # `loss` is the training loss\n",
    "                    loss=\"mse\",  # loss used to train the model\n",
    "                    # `batch_size` is the size of the batch for training\n",
    "                    batch_size=128,\n",
    "                    # the method used to encode the topology vector\n",
    "                    topo_vect_to_tau=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5312262c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7e0718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "782/782 [==============================] - 18s 20ms/step - loss: 14.8312 - a_or_hat_loss: 1.7880 - a_ex_hat_loss: 1.6949 - p_or_hat_loss: 1.5221 - p_ex_hat_loss: 1.5417 - q_ex_hat_loss: 4.7545 - q_or_hat_loss: 3.0994 - v_or_hat_loss: 0.2213 - v_ex_hat_loss: 0.2093 - val_loss: 0.7441 - val_a_or_hat_loss: 0.1030 - val_a_ex_hat_loss: 0.0825 - val_p_or_hat_loss: 0.0845 - val_p_ex_hat_loss: 0.0838 - val_q_ex_hat_loss: 0.2273 - val_q_or_hat_loss: 0.1596 - val_v_or_hat_loss: 0.0018 - val_v_ex_hat_loss: 0.0015\n",
      "Epoch 2/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.5296 - a_or_hat_loss: 0.0776 - a_ex_hat_loss: 0.0584 - p_or_hat_loss: 0.0568 - p_ex_hat_loss: 0.0569 - q_ex_hat_loss: 0.1638 - q_or_hat_loss: 0.1138 - v_or_hat_loss: 0.0012 - v_ex_hat_loss: 0.0010 - val_loss: 0.2988 - val_a_or_hat_loss: 0.0510 - val_a_ex_hat_loss: 0.0403 - val_p_or_hat_loss: 0.0289 - val_p_ex_hat_loss: 0.0299 - val_q_ex_hat_loss: 0.0879 - val_q_or_hat_loss: 0.0595 - val_v_or_hat_loss: 7.2443e-04 - val_v_ex_hat_loss: 5.0046e-04\n",
      "Epoch 3/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.2309 - a_or_hat_loss: 0.0390 - a_ex_hat_loss: 0.0308 - p_or_hat_loss: 0.0216 - p_ex_hat_loss: 0.0228 - q_ex_hat_loss: 0.0686 - q_or_hat_loss: 0.0472 - v_or_hat_loss: 4.8829e-04 - v_ex_hat_loss: 3.8451e-04 - val_loss: 0.1882 - val_a_or_hat_loss: 0.0316 - val_a_ex_hat_loss: 0.0272 - val_p_or_hat_loss: 0.0193 - val_p_ex_hat_loss: 0.0187 - val_q_ex_hat_loss: 0.0517 - val_q_or_hat_loss: 0.0390 - val_v_or_hat_loss: 3.5438e-04 - val_v_ex_hat_loss: 2.9498e-04\n",
      "Epoch 4/200\n",
      "782/782 [==============================] - 14s 19ms/step - loss: 0.1512 - a_or_hat_loss: 0.0250 - a_ex_hat_loss: 0.0209 - p_or_hat_loss: 0.0145 - p_ex_hat_loss: 0.0148 - q_ex_hat_loss: 0.0431 - q_or_hat_loss: 0.0324 - v_or_hat_loss: 2.9863e-04 - v_ex_hat_loss: 2.6856e-04 - val_loss: 0.1374 - val_a_or_hat_loss: 0.0229 - val_a_ex_hat_loss: 0.0194 - val_p_or_hat_loss: 0.0129 - val_p_ex_hat_loss: 0.0127 - val_q_ex_hat_loss: 0.0398 - val_q_or_hat_loss: 0.0293 - val_v_or_hat_loss: 2.4871e-04 - val_v_ex_hat_loss: 2.6684e-04\n",
      "Epoch 5/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.1158 - a_or_hat_loss: 0.0192 - a_ex_hat_loss: 0.0161 - p_or_hat_loss: 0.0113 - p_ex_hat_loss: 0.0109 - q_ex_hat_loss: 0.0322 - q_or_hat_loss: 0.0256 - v_or_hat_loss: 2.1919e-04 - v_ex_hat_loss: 2.1947e-04 - val_loss: 0.1142 - val_a_or_hat_loss: 0.0189 - val_a_ex_hat_loss: 0.0159 - val_p_or_hat_loss: 0.0118 - val_p_ex_hat_loss: 0.0103 - val_q_ex_hat_loss: 0.0304 - val_q_or_hat_loss: 0.0263 - val_v_or_hat_loss: 2.3317e-04 - val_v_ex_hat_loss: 2.3709e-04\n",
      "Epoch 6/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0964 - a_or_hat_loss: 0.0157 - a_ex_hat_loss: 0.0134 - p_or_hat_loss: 0.0095 - p_ex_hat_loss: 0.0088 - q_ex_hat_loss: 0.0267 - q_or_hat_loss: 0.0220 - v_or_hat_loss: 1.7965e-04 - v_ex_hat_loss: 1.8651e-04 - val_loss: 0.0967 - val_a_or_hat_loss: 0.0153 - val_a_ex_hat_loss: 0.0136 - val_p_or_hat_loss: 0.0094 - val_p_ex_hat_loss: 0.0079 - val_q_ex_hat_loss: 0.0272 - val_q_or_hat_loss: 0.0229 - val_v_or_hat_loss: 1.5755e-04 - val_v_ex_hat_loss: 1.7585e-04\n",
      "Epoch 7/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0826 - a_or_hat_loss: 0.0132 - a_ex_hat_loss: 0.0115 - p_or_hat_loss: 0.0081 - p_ex_hat_loss: 0.0072 - q_ex_hat_loss: 0.0233 - q_or_hat_loss: 0.0189 - v_or_hat_loss: 1.4840e-04 - v_ex_hat_loss: 1.6745e-04 - val_loss: 0.0825 - val_a_or_hat_loss: 0.0129 - val_a_ex_hat_loss: 0.0115 - val_p_or_hat_loss: 0.0080 - val_p_ex_hat_loss: 0.0070 - val_q_ex_hat_loss: 0.0231 - val_q_or_hat_loss: 0.0198 - val_v_or_hat_loss: 1.3826e-04 - val_v_ex_hat_loss: 1.3397e-04\n",
      "Epoch 8/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0724 - a_or_hat_loss: 0.0115 - a_ex_hat_loss: 0.0101 - p_or_hat_loss: 0.0070 - p_ex_hat_loss: 0.0062 - q_ex_hat_loss: 0.0206 - q_or_hat_loss: 0.0168 - v_or_hat_loss: 1.2523e-04 - v_ex_hat_loss: 1.3903e-04 - val_loss: 0.0783 - val_a_or_hat_loss: 0.0118 - val_a_ex_hat_loss: 0.0107 - val_p_or_hat_loss: 0.0072 - val_p_ex_hat_loss: 0.0072 - val_q_ex_hat_loss: 0.0235 - val_q_or_hat_loss: 0.0175 - val_v_or_hat_loss: 1.6001e-04 - val_v_ex_hat_loss: 1.6626e-04\n",
      "Epoch 9/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0654 - a_or_hat_loss: 0.0102 - a_ex_hat_loss: 0.0090 - p_or_hat_loss: 0.0062 - p_ex_hat_loss: 0.0055 - q_ex_hat_loss: 0.0189 - q_or_hat_loss: 0.0153 - v_or_hat_loss: 1.1441e-04 - v_ex_hat_loss: 1.3449e-04 - val_loss: 0.0685 - val_a_or_hat_loss: 0.0106 - val_a_ex_hat_loss: 0.0093 - val_p_or_hat_loss: 0.0064 - val_p_ex_hat_loss: 0.0054 - val_q_ex_hat_loss: 0.0192 - val_q_or_hat_loss: 0.0175 - val_v_or_hat_loss: 9.4904e-05 - val_v_ex_hat_loss: 1.1007e-04\n",
      "Epoch 10/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0584 - a_or_hat_loss: 0.0091 - a_ex_hat_loss: 0.0081 - p_or_hat_loss: 0.0055 - p_ex_hat_loss: 0.0050 - q_ex_hat_loss: 0.0169 - q_or_hat_loss: 0.0137 - v_or_hat_loss: 9.9447e-05 - v_ex_hat_loss: 1.1680e-04 - val_loss: 0.0647 - val_a_or_hat_loss: 0.0099 - val_a_ex_hat_loss: 0.0087 - val_p_or_hat_loss: 0.0057 - val_p_ex_hat_loss: 0.0053 - val_q_ex_hat_loss: 0.0181 - val_q_or_hat_loss: 0.0166 - val_v_or_hat_loss: 2.2048e-04 - val_v_ex_hat_loss: 2.0354e-04\n",
      "Epoch 11/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0533 - a_or_hat_loss: 0.0083 - a_ex_hat_loss: 0.0074 - p_or_hat_loss: 0.0049 - p_ex_hat_loss: 0.0044 - q_ex_hat_loss: 0.0155 - q_or_hat_loss: 0.0126 - v_or_hat_loss: 9.0198e-05 - v_ex_hat_loss: 1.0257e-04 - val_loss: 0.0575 - val_a_or_hat_loss: 0.0088 - val_a_ex_hat_loss: 0.0081 - val_p_or_hat_loss: 0.0052 - val_p_ex_hat_loss: 0.0047 - val_q_ex_hat_loss: 0.0170 - val_q_or_hat_loss: 0.0135 - val_v_or_hat_loss: 8.7287e-05 - val_v_ex_hat_loss: 1.3259e-04\n",
      "Epoch 12/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0496 - a_or_hat_loss: 0.0077 - a_ex_hat_loss: 0.0069 - p_or_hat_loss: 0.0046 - p_ex_hat_loss: 0.0042 - q_ex_hat_loss: 0.0144 - q_or_hat_loss: 0.0116 - v_or_hat_loss: 9.1735e-05 - v_ex_hat_loss: 1.0507e-04 - val_loss: 0.0593 - val_a_or_hat_loss: 0.0087 - val_a_ex_hat_loss: 0.0079 - val_p_or_hat_loss: 0.0061 - val_p_ex_hat_loss: 0.0059 - val_q_ex_hat_loss: 0.0166 - val_q_or_hat_loss: 0.0139 - val_v_or_hat_loss: 1.2294e-04 - val_v_ex_hat_loss: 7.8262e-05\n",
      "Epoch 13/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0465 - a_or_hat_loss: 0.0071 - a_ex_hat_loss: 0.0065 - p_or_hat_loss: 0.0042 - p_ex_hat_loss: 0.0038 - q_ex_hat_loss: 0.0136 - q_or_hat_loss: 0.0111 - v_or_hat_loss: 8.0770e-05 - v_ex_hat_loss: 9.2124e-05 - val_loss: 0.0512 - val_a_or_hat_loss: 0.0074 - val_a_ex_hat_loss: 0.0070 - val_p_or_hat_loss: 0.0050 - val_p_ex_hat_loss: 0.0041 - val_q_ex_hat_loss: 0.0153 - val_q_or_hat_loss: 0.0123 - val_v_or_hat_loss: 6.2194e-05 - val_v_ex_hat_loss: 7.6755e-05\n",
      "Epoch 14/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0435 - a_or_hat_loss: 0.0066 - a_ex_hat_loss: 0.0061 - p_or_hat_loss: 0.0040 - p_ex_hat_loss: 0.0035 - q_ex_hat_loss: 0.0126 - q_or_hat_loss: 0.0105 - v_or_hat_loss: 7.8701e-05 - v_ex_hat_loss: 9.1408e-05 - val_loss: 0.0508 - val_a_or_hat_loss: 0.0072 - val_a_ex_hat_loss: 0.0067 - val_p_or_hat_loss: 0.0045 - val_p_ex_hat_loss: 0.0043 - val_q_ex_hat_loss: 0.0151 - val_q_or_hat_loss: 0.0129 - val_v_or_hat_loss: 9.4296e-05 - val_v_ex_hat_loss: 7.9997e-05\n",
      "Epoch 15/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0409 - a_or_hat_loss: 0.0063 - a_ex_hat_loss: 0.0058 - p_or_hat_loss: 0.0038 - p_ex_hat_loss: 0.0033 - q_ex_hat_loss: 0.0118 - q_or_hat_loss: 0.0098 - v_or_hat_loss: 7.4832e-05 - v_ex_hat_loss: 9.0290e-05 - val_loss: 0.0451 - val_a_or_hat_loss: 0.0066 - val_a_ex_hat_loss: 0.0061 - val_p_or_hat_loss: 0.0041 - val_p_ex_hat_loss: 0.0035 - val_q_ex_hat_loss: 0.0133 - val_q_or_hat_loss: 0.0112 - val_v_or_hat_loss: 6.9542e-05 - val_v_ex_hat_loss: 6.2129e-05\n",
      "Epoch 16/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0388 - a_or_hat_loss: 0.0058 - a_ex_hat_loss: 0.0055 - p_or_hat_loss: 0.0036 - p_ex_hat_loss: 0.0033 - q_ex_hat_loss: 0.0111 - q_or_hat_loss: 0.0093 - v_or_hat_loss: 7.4608e-05 - v_ex_hat_loss: 8.3808e-05 - val_loss: 0.0405 - val_a_or_hat_loss: 0.0062 - val_a_ex_hat_loss: 0.0059 - val_p_or_hat_loss: 0.0037 - val_p_ex_hat_loss: 0.0033 - val_q_ex_hat_loss: 0.0116 - val_q_or_hat_loss: 0.0097 - val_v_or_hat_loss: 5.9388e-05 - val_v_ex_hat_loss: 8.3243e-05\n",
      "Epoch 17/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0367 - a_or_hat_loss: 0.0056 - a_ex_hat_loss: 0.0052 - p_or_hat_loss: 0.0034 - p_ex_hat_loss: 0.0031 - q_ex_hat_loss: 0.0105 - q_or_hat_loss: 0.0088 - v_or_hat_loss: 7.0859e-05 - v_ex_hat_loss: 8.3688e-05 - val_loss: 0.0397 - val_a_or_hat_loss: 0.0061 - val_a_ex_hat_loss: 0.0061 - val_p_or_hat_loss: 0.0036 - val_p_ex_hat_loss: 0.0031 - val_q_ex_hat_loss: 0.0113 - val_q_or_hat_loss: 0.0094 - val_v_or_hat_loss: 5.2970e-05 - val_v_ex_hat_loss: 6.4319e-05\n",
      "Epoch 18/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0347 - a_or_hat_loss: 0.0053 - a_ex_hat_loss: 0.0051 - p_or_hat_loss: 0.0031 - p_ex_hat_loss: 0.0028 - q_ex_hat_loss: 0.0100 - q_or_hat_loss: 0.0083 - v_or_hat_loss: 6.4651e-05 - v_ex_hat_loss: 7.9468e-05 - val_loss: 0.0399 - val_a_or_hat_loss: 0.0055 - val_a_ex_hat_loss: 0.0058 - val_p_or_hat_loss: 0.0042 - val_p_ex_hat_loss: 0.0034 - val_q_ex_hat_loss: 0.0117 - val_q_or_hat_loss: 0.0092 - val_v_or_hat_loss: 5.9991e-05 - val_v_ex_hat_loss: 5.4581e-05\n",
      "Epoch 19/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0333 - a_or_hat_loss: 0.0050 - a_ex_hat_loss: 0.0049 - p_or_hat_loss: 0.0030 - p_ex_hat_loss: 0.0027 - q_ex_hat_loss: 0.0095 - q_or_hat_loss: 0.0080 - v_or_hat_loss: 6.3241e-05 - v_ex_hat_loss: 7.4099e-05 - val_loss: 0.0383 - val_a_or_hat_loss: 0.0056 - val_a_ex_hat_loss: 0.0055 - val_p_or_hat_loss: 0.0041 - val_p_ex_hat_loss: 0.0033 - val_q_ex_hat_loss: 0.0104 - val_q_or_hat_loss: 0.0091 - val_v_or_hat_loss: 6.3182e-05 - val_v_ex_hat_loss: 7.0890e-05\n",
      "Epoch 20/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0320 - a_or_hat_loss: 0.0049 - a_ex_hat_loss: 0.0048 - p_or_hat_loss: 0.0029 - p_ex_hat_loss: 0.0026 - q_ex_hat_loss: 0.0091 - q_or_hat_loss: 0.0076 - v_or_hat_loss: 6.1550e-05 - v_ex_hat_loss: 7.0994e-05 - val_loss: 0.0364 - val_a_or_hat_loss: 0.0053 - val_a_ex_hat_loss: 0.0055 - val_p_or_hat_loss: 0.0032 - val_p_ex_hat_loss: 0.0028 - val_q_ex_hat_loss: 0.0100 - val_q_or_hat_loss: 0.0095 - val_v_or_hat_loss: 4.9294e-05 - val_v_ex_hat_loss: 5.1429e-05\n",
      "Epoch 21/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0311 - a_or_hat_loss: 0.0047 - a_ex_hat_loss: 0.0046 - p_or_hat_loss: 0.0028 - p_ex_hat_loss: 0.0026 - q_ex_hat_loss: 0.0087 - q_or_hat_loss: 0.0075 - v_or_hat_loss: 5.9639e-05 - v_ex_hat_loss: 7.0204e-05 - val_loss: 0.0364 - val_a_or_hat_loss: 0.0048 - val_a_ex_hat_loss: 0.0055 - val_p_or_hat_loss: 0.0042 - val_p_ex_hat_loss: 0.0035 - val_q_ex_hat_loss: 0.0095 - val_q_or_hat_loss: 0.0088 - val_v_or_hat_loss: 4.9387e-05 - val_v_ex_hat_loss: 5.3983e-05\n",
      "Epoch 22/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0300 - a_or_hat_loss: 0.0045 - a_ex_hat_loss: 0.0045 - p_or_hat_loss: 0.0027 - p_ex_hat_loss: 0.0025 - q_ex_hat_loss: 0.0085 - q_or_hat_loss: 0.0073 - v_or_hat_loss: 5.2356e-05 - v_ex_hat_loss: 6.2583e-05 - val_loss: 0.0341 - val_a_or_hat_loss: 0.0050 - val_a_ex_hat_loss: 0.0051 - val_p_or_hat_loss: 0.0031 - val_p_ex_hat_loss: 0.0037 - val_q_ex_hat_loss: 0.0091 - val_q_or_hat_loss: 0.0079 - val_v_or_hat_loss: 5.7848e-05 - val_v_ex_hat_loss: 7.5158e-05\n",
      "Epoch 23/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0292 - a_or_hat_loss: 0.0044 - a_ex_hat_loss: 0.0043 - p_or_hat_loss: 0.0026 - p_ex_hat_loss: 0.0025 - q_ex_hat_loss: 0.0082 - q_or_hat_loss: 0.0070 - v_or_hat_loss: 5.7751e-05 - v_ex_hat_loss: 7.1899e-05 - val_loss: 0.0331 - val_a_or_hat_loss: 0.0046 - val_a_ex_hat_loss: 0.0048 - val_p_or_hat_loss: 0.0028 - val_p_ex_hat_loss: 0.0028 - val_q_ex_hat_loss: 0.0095 - val_q_or_hat_loss: 0.0085 - val_v_or_hat_loss: 3.6159e-05 - val_v_ex_hat_loss: 4.7696e-05\n",
      "Epoch 24/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0279 - a_or_hat_loss: 0.0041 - a_ex_hat_loss: 0.0042 - p_or_hat_loss: 0.0025 - p_ex_hat_loss: 0.0024 - q_ex_hat_loss: 0.0078 - q_or_hat_loss: 0.0068 - v_or_hat_loss: 4.7823e-05 - v_ex_hat_loss: 5.7429e-05 - val_loss: 0.0320 - val_a_or_hat_loss: 0.0045 - val_a_ex_hat_loss: 0.0049 - val_p_or_hat_loss: 0.0029 - val_p_ex_hat_loss: 0.0030 - val_q_ex_hat_loss: 0.0092 - val_q_or_hat_loss: 0.0074 - val_v_or_hat_loss: 4.0338e-05 - val_v_ex_hat_loss: 6.4056e-05\n",
      "Epoch 25/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0272 - a_or_hat_loss: 0.0041 - a_ex_hat_loss: 0.0041 - p_or_hat_loss: 0.0024 - p_ex_hat_loss: 0.0022 - q_ex_hat_loss: 0.0077 - q_or_hat_loss: 0.0065 - v_or_hat_loss: 5.1993e-05 - v_ex_hat_loss: 6.3827e-05 - val_loss: 0.0308 - val_a_or_hat_loss: 0.0045 - val_a_ex_hat_loss: 0.0046 - val_p_or_hat_loss: 0.0033 - val_p_ex_hat_loss: 0.0026 - val_q_ex_hat_loss: 0.0086 - val_q_or_hat_loss: 0.0071 - val_v_or_hat_loss: 5.1363e-05 - val_v_ex_hat_loss: 8.9428e-05\n",
      "Epoch 26/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0261 - a_or_hat_loss: 0.0039 - a_ex_hat_loss: 0.0039 - p_or_hat_loss: 0.0023 - p_ex_hat_loss: 0.0022 - q_ex_hat_loss: 0.0073 - q_or_hat_loss: 0.0063 - v_or_hat_loss: 5.1839e-05 - v_ex_hat_loss: 6.6625e-05 - val_loss: 0.0310 - val_a_or_hat_loss: 0.0043 - val_a_ex_hat_loss: 0.0045 - val_p_or_hat_loss: 0.0035 - val_p_ex_hat_loss: 0.0028 - val_q_ex_hat_loss: 0.0084 - val_q_or_hat_loss: 0.0073 - val_v_or_hat_loss: 3.3842e-05 - val_v_ex_hat_loss: 5.1959e-05\n",
      "Epoch 27/200\n",
      "782/782 [==============================] - 14s 19ms/step - loss: 0.0255 - a_or_hat_loss: 0.0039 - a_ex_hat_loss: 0.0038 - p_or_hat_loss: 0.0023 - p_ex_hat_loss: 0.0022 - q_ex_hat_loss: 0.0072 - q_or_hat_loss: 0.0062 - v_or_hat_loss: 4.9667e-05 - v_ex_hat_loss: 6.2888e-05 - val_loss: 0.0281 - val_a_or_hat_loss: 0.0043 - val_a_ex_hat_loss: 0.0042 - val_p_or_hat_loss: 0.0027 - val_p_ex_hat_loss: 0.0022 - val_q_ex_hat_loss: 0.0080 - val_q_or_hat_loss: 0.0066 - val_v_or_hat_loss: 3.2050e-05 - val_v_ex_hat_loss: 4.3206e-05\n",
      "Epoch 28/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0247 - a_or_hat_loss: 0.0037 - a_ex_hat_loss: 0.0037 - p_or_hat_loss: 0.0021 - p_ex_hat_loss: 0.0020 - q_ex_hat_loss: 0.0070 - q_or_hat_loss: 0.0059 - v_or_hat_loss: 4.3148e-05 - v_ex_hat_loss: 5.3143e-05 - val_loss: 0.0283 - val_a_or_hat_loss: 0.0042 - val_a_ex_hat_loss: 0.0043 - val_p_or_hat_loss: 0.0026 - val_p_ex_hat_loss: 0.0024 - val_q_ex_hat_loss: 0.0077 - val_q_or_hat_loss: 0.0069 - val_v_or_hat_loss: 4.2918e-05 - val_v_ex_hat_loss: 4.8988e-05\n",
      "Epoch 29/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0245 - a_or_hat_loss: 0.0037 - a_ex_hat_loss: 0.0036 - p_or_hat_loss: 0.0021 - p_ex_hat_loss: 0.0022 - q_ex_hat_loss: 0.0069 - q_or_hat_loss: 0.0060 - v_or_hat_loss: 4.7315e-05 - v_ex_hat_loss: 5.9565e-05 - val_loss: 0.0280 - val_a_or_hat_loss: 0.0041 - val_a_ex_hat_loss: 0.0042 - val_p_or_hat_loss: 0.0025 - val_p_ex_hat_loss: 0.0022 - val_q_ex_hat_loss: 0.0082 - val_q_or_hat_loss: 0.0068 - val_v_or_hat_loss: 4.3721e-05 - val_v_ex_hat_loss: 5.4737e-05\n",
      "Epoch 30/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0234 - a_or_hat_loss: 0.0035 - a_ex_hat_loss: 0.0035 - p_or_hat_loss: 0.0020 - p_ex_hat_loss: 0.0020 - q_ex_hat_loss: 0.0067 - q_or_hat_loss: 0.0056 - v_or_hat_loss: 4.3852e-05 - v_ex_hat_loss: 5.5669e-05 - val_loss: 0.0288 - val_a_or_hat_loss: 0.0040 - val_a_ex_hat_loss: 0.0042 - val_p_or_hat_loss: 0.0028 - val_p_ex_hat_loss: 0.0023 - val_q_ex_hat_loss: 0.0085 - val_q_or_hat_loss: 0.0069 - val_v_or_hat_loss: 4.2998e-05 - val_v_ex_hat_loss: 5.8096e-05\n",
      "Epoch 31/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0231 - a_or_hat_loss: 0.0035 - a_ex_hat_loss: 0.0035 - p_or_hat_loss: 0.0020 - p_ex_hat_loss: 0.0020 - q_ex_hat_loss: 0.0065 - q_or_hat_loss: 0.0056 - v_or_hat_loss: 4.4312e-05 - v_ex_hat_loss: 5.5230e-05 - val_loss: 0.0273 - val_a_or_hat_loss: 0.0038 - val_a_ex_hat_loss: 0.0041 - val_p_or_hat_loss: 0.0025 - val_p_ex_hat_loss: 0.0029 - val_q_ex_hat_loss: 0.0077 - val_q_or_hat_loss: 0.0063 - val_v_or_hat_loss: 6.1365e-05 - val_v_ex_hat_loss: 7.6209e-05\n",
      "Epoch 32/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0225 - a_or_hat_loss: 0.0034 - a_ex_hat_loss: 0.0034 - p_or_hat_loss: 0.0019 - p_ex_hat_loss: 0.0019 - q_ex_hat_loss: 0.0063 - q_or_hat_loss: 0.0054 - v_or_hat_loss: 3.9788e-05 - v_ex_hat_loss: 5.1936e-05 - val_loss: 0.0283 - val_a_or_hat_loss: 0.0039 - val_a_ex_hat_loss: 0.0038 - val_p_or_hat_loss: 0.0029 - val_p_ex_hat_loss: 0.0028 - val_q_ex_hat_loss: 0.0088 - val_q_or_hat_loss: 0.0062 - val_v_or_hat_loss: 4.2203e-05 - val_v_ex_hat_loss: 6.1306e-05\n",
      "Epoch 33/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0220 - a_or_hat_loss: 0.0033 - a_ex_hat_loss: 0.0033 - p_or_hat_loss: 0.0019 - p_ex_hat_loss: 0.0019 - q_ex_hat_loss: 0.0061 - q_or_hat_loss: 0.0053 - v_or_hat_loss: 4.0424e-05 - v_ex_hat_loss: 5.0849e-05 - val_loss: 0.0243 - val_a_or_hat_loss: 0.0036 - val_a_ex_hat_loss: 0.0039 - val_p_or_hat_loss: 0.0023 - val_p_ex_hat_loss: 0.0023 - val_q_ex_hat_loss: 0.0066 - val_q_or_hat_loss: 0.0057 - val_v_or_hat_loss: 3.4466e-05 - val_v_ex_hat_loss: 4.3996e-05\n",
      "Epoch 34/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0215 - a_or_hat_loss: 0.0033 - a_ex_hat_loss: 0.0033 - p_or_hat_loss: 0.0019 - p_ex_hat_loss: 0.0019 - q_ex_hat_loss: 0.0061 - q_or_hat_loss: 0.0050 - v_or_hat_loss: 3.9125e-05 - v_ex_hat_loss: 5.0595e-05 - val_loss: 0.0238 - val_a_or_hat_loss: 0.0036 - val_a_ex_hat_loss: 0.0037 - val_p_or_hat_loss: 0.0022 - val_p_ex_hat_loss: 0.0019 - val_q_ex_hat_loss: 0.0069 - val_q_or_hat_loss: 0.0054 - val_v_or_hat_loss: 4.7739e-05 - val_v_ex_hat_loss: 8.1740e-05\n",
      "Epoch 35/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0210 - a_or_hat_loss: 0.0032 - a_ex_hat_loss: 0.0032 - p_or_hat_loss: 0.0019 - p_ex_hat_loss: 0.0019 - q_ex_hat_loss: 0.0058 - q_or_hat_loss: 0.0049 - v_or_hat_loss: 3.7055e-05 - v_ex_hat_loss: 4.9582e-05 - val_loss: 0.0248 - val_a_or_hat_loss: 0.0036 - val_a_ex_hat_loss: 0.0036 - val_p_or_hat_loss: 0.0022 - val_p_ex_hat_loss: 0.0022 - val_q_ex_hat_loss: 0.0071 - val_q_or_hat_loss: 0.0060 - val_v_or_hat_loss: 6.0652e-05 - val_v_ex_hat_loss: 7.2065e-05\n",
      "Epoch 36/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0206 - a_or_hat_loss: 0.0032 - a_ex_hat_loss: 0.0032 - p_or_hat_loss: 0.0018 - p_ex_hat_loss: 0.0018 - q_ex_hat_loss: 0.0058 - q_or_hat_loss: 0.0048 - v_or_hat_loss: 3.8991e-05 - v_ex_hat_loss: 4.9927e-05 - val_loss: 0.0264 - val_a_or_hat_loss: 0.0036 - val_a_ex_hat_loss: 0.0037 - val_p_or_hat_loss: 0.0029 - val_p_ex_hat_loss: 0.0030 - val_q_ex_hat_loss: 0.0074 - val_q_or_hat_loss: 0.0058 - val_v_or_hat_loss: 3.1575e-05 - val_v_ex_hat_loss: 3.5544e-05\n",
      "Epoch 37/200\n",
      "782/782 [==============================] - 14s 19ms/step - loss: 0.0200 - a_or_hat_loss: 0.0031 - a_ex_hat_loss: 0.0031 - p_or_hat_loss: 0.0017 - p_ex_hat_loss: 0.0017 - q_ex_hat_loss: 0.0057 - q_or_hat_loss: 0.0047 - v_or_hat_loss: 3.4688e-05 - v_ex_hat_loss: 4.3650e-05 - val_loss: 0.0256 - val_a_or_hat_loss: 0.0036 - val_a_ex_hat_loss: 0.0039 - val_p_or_hat_loss: 0.0028 - val_p_ex_hat_loss: 0.0023 - val_q_ex_hat_loss: 0.0065 - val_q_or_hat_loss: 0.0064 - val_v_or_hat_loss: 3.0302e-05 - val_v_ex_hat_loss: 4.4294e-05\n",
      "Epoch 38/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0202 - a_or_hat_loss: 0.0031 - a_ex_hat_loss: 0.0031 - p_or_hat_loss: 0.0018 - p_ex_hat_loss: 0.0018 - q_ex_hat_loss: 0.0056 - q_or_hat_loss: 0.0047 - v_or_hat_loss: 4.2661e-05 - v_ex_hat_loss: 5.3204e-05 - val_loss: 0.0220 - val_a_or_hat_loss: 0.0033 - val_a_ex_hat_loss: 0.0034 - val_p_or_hat_loss: 0.0019 - val_p_ex_hat_loss: 0.0017 - val_q_ex_hat_loss: 0.0062 - val_q_or_hat_loss: 0.0055 - val_v_or_hat_loss: 2.9257e-05 - val_v_ex_hat_loss: 4.1552e-05\n",
      "Epoch 39/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0193 - a_or_hat_loss: 0.0030 - a_ex_hat_loss: 0.0030 - p_or_hat_loss: 0.0016 - p_ex_hat_loss: 0.0016 - q_ex_hat_loss: 0.0054 - q_or_hat_loss: 0.0045 - v_or_hat_loss: 3.9026e-05 - v_ex_hat_loss: 5.0297e-05 - val_loss: 0.0210 - val_a_or_hat_loss: 0.0034 - val_a_ex_hat_loss: 0.0034 - val_p_or_hat_loss: 0.0018 - val_p_ex_hat_loss: 0.0018 - val_q_ex_hat_loss: 0.0057 - val_q_or_hat_loss: 0.0049 - val_v_or_hat_loss: 2.8591e-05 - val_v_ex_hat_loss: 3.6744e-05\n",
      "Epoch 40/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0187 - a_or_hat_loss: 0.0029 - a_ex_hat_loss: 0.0029 - p_or_hat_loss: 0.0016 - p_ex_hat_loss: 0.0016 - q_ex_hat_loss: 0.0053 - q_or_hat_loss: 0.0044 - v_or_hat_loss: 3.1944e-05 - v_ex_hat_loss: 4.2218e-05 - val_loss: 0.0211 - val_a_or_hat_loss: 0.0032 - val_a_ex_hat_loss: 0.0035 - val_p_or_hat_loss: 0.0019 - val_p_ex_hat_loss: 0.0018 - val_q_ex_hat_loss: 0.0058 - val_q_or_hat_loss: 0.0048 - val_v_or_hat_loss: 2.4735e-05 - val_v_ex_hat_loss: 3.6667e-05\n",
      "Epoch 41/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0187 - a_or_hat_loss: 0.0029 - a_ex_hat_loss: 0.0029 - p_or_hat_loss: 0.0016 - p_ex_hat_loss: 0.0016 - q_ex_hat_loss: 0.0052 - q_or_hat_loss: 0.0044 - v_or_hat_loss: 3.3464e-05 - v_ex_hat_loss: 4.4180e-05 - val_loss: 0.0208 - val_a_or_hat_loss: 0.0032 - val_a_ex_hat_loss: 0.0032 - val_p_or_hat_loss: 0.0017 - val_p_ex_hat_loss: 0.0017 - val_q_ex_hat_loss: 0.0060 - val_q_or_hat_loss: 0.0049 - val_v_or_hat_loss: 2.5358e-05 - val_v_ex_hat_loss: 3.5046e-05\n",
      "Epoch 42/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0180 - a_or_hat_loss: 0.0028 - a_ex_hat_loss: 0.0028 - p_or_hat_loss: 0.0015 - p_ex_hat_loss: 0.0015 - q_ex_hat_loss: 0.0051 - q_or_hat_loss: 0.0042 - v_or_hat_loss: 2.9842e-05 - v_ex_hat_loss: 3.9436e-05 - val_loss: 0.0253 - val_a_or_hat_loss: 0.0033 - val_a_ex_hat_loss: 0.0033 - val_p_or_hat_loss: 0.0036 - val_p_ex_hat_loss: 0.0029 - val_q_ex_hat_loss: 0.0067 - val_q_or_hat_loss: 0.0055 - val_v_or_hat_loss: 2.7227e-05 - val_v_ex_hat_loss: 3.3954e-05\n",
      "Epoch 43/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0181 - a_or_hat_loss: 0.0028 - a_ex_hat_loss: 0.0028 - p_or_hat_loss: 0.0016 - p_ex_hat_loss: 0.0016 - q_ex_hat_loss: 0.0051 - q_or_hat_loss: 0.0042 - v_or_hat_loss: 2.9928e-05 - v_ex_hat_loss: 3.9967e-05 - val_loss: 0.0204 - val_a_or_hat_loss: 0.0035 - val_a_ex_hat_loss: 0.0032 - val_p_or_hat_loss: 0.0018 - val_p_ex_hat_loss: 0.0016 - val_q_ex_hat_loss: 0.0055 - val_q_or_hat_loss: 0.0047 - val_v_or_hat_loss: 2.8830e-05 - val_v_ex_hat_loss: 4.6043e-05\n",
      "Epoch 44/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0177 - a_or_hat_loss: 0.0028 - a_ex_hat_loss: 0.0028 - p_or_hat_loss: 0.0015 - p_ex_hat_loss: 0.0015 - q_ex_hat_loss: 0.0049 - q_or_hat_loss: 0.0041 - v_or_hat_loss: 2.9747e-05 - v_ex_hat_loss: 3.9922e-05 - val_loss: 0.0218 - val_a_or_hat_loss: 0.0032 - val_a_ex_hat_loss: 0.0032 - val_p_or_hat_loss: 0.0020 - val_p_ex_hat_loss: 0.0019 - val_q_ex_hat_loss: 0.0059 - val_q_or_hat_loss: 0.0055 - val_v_or_hat_loss: 2.3917e-05 - val_v_ex_hat_loss: 2.8802e-05\n",
      "Epoch 45/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0175 - a_or_hat_loss: 0.0027 - a_ex_hat_loss: 0.0027 - p_or_hat_loss: 0.0015 - p_ex_hat_loss: 0.0015 - q_ex_hat_loss: 0.0049 - q_or_hat_loss: 0.0041 - v_or_hat_loss: 2.9033e-05 - v_ex_hat_loss: 3.7691e-05 - val_loss: 0.0203 - val_a_or_hat_loss: 0.0030 - val_a_ex_hat_loss: 0.0031 - val_p_or_hat_loss: 0.0016 - val_p_ex_hat_loss: 0.0015 - val_q_ex_hat_loss: 0.0056 - val_q_or_hat_loss: 0.0053 - val_v_or_hat_loss: 2.1253e-05 - val_v_ex_hat_loss: 2.8999e-05\n",
      "Epoch 46/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0171 - a_or_hat_loss: 0.0027 - a_ex_hat_loss: 0.0027 - p_or_hat_loss: 0.0014 - p_ex_hat_loss: 0.0015 - q_ex_hat_loss: 0.0048 - q_or_hat_loss: 0.0040 - v_or_hat_loss: 2.9207e-05 - v_ex_hat_loss: 3.8063e-05 - val_loss: 0.0226 - val_a_or_hat_loss: 0.0031 - val_a_ex_hat_loss: 0.0031 - val_p_or_hat_loss: 0.0021 - val_p_ex_hat_loss: 0.0019 - val_q_ex_hat_loss: 0.0068 - val_q_or_hat_loss: 0.0055 - val_v_or_hat_loss: 4.2219e-05 - val_v_ex_hat_loss: 5.9475e-05\n",
      "Epoch 47/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0169 - a_or_hat_loss: 0.0027 - a_ex_hat_loss: 0.0026 - p_or_hat_loss: 0.0014 - p_ex_hat_loss: 0.0014 - q_ex_hat_loss: 0.0047 - q_or_hat_loss: 0.0040 - v_or_hat_loss: 3.0216e-05 - v_ex_hat_loss: 3.9937e-05 - val_loss: 0.0197 - val_a_or_hat_loss: 0.0030 - val_a_ex_hat_loss: 0.0030 - val_p_or_hat_loss: 0.0018 - val_p_ex_hat_loss: 0.0017 - val_q_ex_hat_loss: 0.0056 - val_q_or_hat_loss: 0.0046 - val_v_or_hat_loss: 3.0122e-05 - val_v_ex_hat_loss: 2.8901e-05\n",
      "Epoch 48/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0169 - a_or_hat_loss: 0.0026 - a_ex_hat_loss: 0.0026 - p_or_hat_loss: 0.0015 - p_ex_hat_loss: 0.0015 - q_ex_hat_loss: 0.0047 - q_or_hat_loss: 0.0040 - v_or_hat_loss: 2.8277e-05 - v_ex_hat_loss: 3.7364e-05 - val_loss: 0.0210 - val_a_or_hat_loss: 0.0030 - val_a_ex_hat_loss: 0.0033 - val_p_or_hat_loss: 0.0020 - val_p_ex_hat_loss: 0.0019 - val_q_ex_hat_loss: 0.0060 - val_q_or_hat_loss: 0.0049 - val_v_or_hat_loss: 2.2020e-05 - val_v_ex_hat_loss: 2.9880e-05\n",
      "Epoch 49/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0167 - a_or_hat_loss: 0.0026 - a_ex_hat_loss: 0.0026 - p_or_hat_loss: 0.0015 - p_ex_hat_loss: 0.0015 - q_ex_hat_loss: 0.0046 - q_or_hat_loss: 0.0039 - v_or_hat_loss: 3.0125e-05 - v_ex_hat_loss: 3.9277e-05 - val_loss: 0.0192 - val_a_or_hat_loss: 0.0029 - val_a_ex_hat_loss: 0.0030 - val_p_or_hat_loss: 0.0016 - val_p_ex_hat_loss: 0.0016 - val_q_ex_hat_loss: 0.0056 - val_q_or_hat_loss: 0.0045 - val_v_or_hat_loss: 3.6192e-05 - val_v_ex_hat_loss: 3.9754e-05\n",
      "Epoch 50/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0162 - a_or_hat_loss: 0.0025 - a_ex_hat_loss: 0.0025 - p_or_hat_loss: 0.0014 - p_ex_hat_loss: 0.0014 - q_ex_hat_loss: 0.0046 - q_or_hat_loss: 0.0038 - v_or_hat_loss: 2.7946e-05 - v_ex_hat_loss: 3.5953e-05 - val_loss: 0.0192 - val_a_or_hat_loss: 0.0030 - val_a_ex_hat_loss: 0.0030 - val_p_or_hat_loss: 0.0016 - val_p_ex_hat_loss: 0.0015 - val_q_ex_hat_loss: 0.0053 - val_q_or_hat_loss: 0.0046 - val_v_or_hat_loss: 9.0263e-05 - val_v_ex_hat_loss: 7.8743e-05\n",
      "Epoch 51/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0159 - a_or_hat_loss: 0.0025 - a_ex_hat_loss: 0.0025 - p_or_hat_loss: 0.0013 - p_ex_hat_loss: 0.0013 - q_ex_hat_loss: 0.0045 - q_or_hat_loss: 0.0037 - v_or_hat_loss: 2.7881e-05 - v_ex_hat_loss: 3.7145e-05 - val_loss: 0.0182 - val_a_or_hat_loss: 0.0028 - val_a_ex_hat_loss: 0.0028 - val_p_or_hat_loss: 0.0017 - val_p_ex_hat_loss: 0.0015 - val_q_ex_hat_loss: 0.0051 - val_q_or_hat_loss: 0.0042 - val_v_or_hat_loss: 1.8645e-05 - val_v_ex_hat_loss: 2.9249e-05\n",
      "Epoch 52/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0154 - a_or_hat_loss: 0.0025 - a_ex_hat_loss: 0.0024 - p_or_hat_loss: 0.0013 - p_ex_hat_loss: 0.0013 - q_ex_hat_loss: 0.0043 - q_or_hat_loss: 0.0036 - v_or_hat_loss: 2.5888e-05 - v_ex_hat_loss: 3.3933e-05 - val_loss: 0.0191 - val_a_or_hat_loss: 0.0029 - val_a_ex_hat_loss: 0.0029 - val_p_or_hat_loss: 0.0018 - val_p_ex_hat_loss: 0.0019 - val_q_ex_hat_loss: 0.0050 - val_q_or_hat_loss: 0.0045 - val_v_or_hat_loss: 3.3967e-05 - val_v_ex_hat_loss: 3.5272e-05\n",
      "Epoch 53/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0155 - a_or_hat_loss: 0.0024 - a_ex_hat_loss: 0.0024 - p_or_hat_loss: 0.0013 - p_ex_hat_loss: 0.0013 - q_ex_hat_loss: 0.0043 - q_or_hat_loss: 0.0036 - v_or_hat_loss: 2.6485e-05 - v_ex_hat_loss: 3.4459e-05 - val_loss: 0.0195 - val_a_or_hat_loss: 0.0027 - val_a_ex_hat_loss: 0.0028 - val_p_or_hat_loss: 0.0018 - val_p_ex_hat_loss: 0.0018 - val_q_ex_hat_loss: 0.0055 - val_q_or_hat_loss: 0.0048 - val_v_or_hat_loss: 4.6124e-05 - val_v_ex_hat_loss: 4.2599e-05\n",
      "Epoch 54/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0153 - a_or_hat_loss: 0.0024 - a_ex_hat_loss: 0.0024 - p_or_hat_loss: 0.0013 - p_ex_hat_loss: 0.0013 - q_ex_hat_loss: 0.0043 - q_or_hat_loss: 0.0036 - v_or_hat_loss: 2.5740e-05 - v_ex_hat_loss: 3.2624e-05 - val_loss: 0.0186 - val_a_or_hat_loss: 0.0027 - val_a_ex_hat_loss: 0.0029 - val_p_or_hat_loss: 0.0016 - val_p_ex_hat_loss: 0.0014 - val_q_ex_hat_loss: 0.0052 - val_q_or_hat_loss: 0.0048 - val_v_or_hat_loss: 2.4840e-05 - val_v_ex_hat_loss: 3.0141e-05\n",
      "Epoch 55/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0149 - a_or_hat_loss: 0.0024 - a_ex_hat_loss: 0.0023 - p_or_hat_loss: 0.0013 - p_ex_hat_loss: 0.0012 - q_ex_hat_loss: 0.0042 - q_or_hat_loss: 0.0035 - v_or_hat_loss: 2.2979e-05 - v_ex_hat_loss: 2.9745e-05 - val_loss: 0.0178 - val_a_or_hat_loss: 0.0026 - val_a_ex_hat_loss: 0.0029 - val_p_or_hat_loss: 0.0018 - val_p_ex_hat_loss: 0.0015 - val_q_ex_hat_loss: 0.0049 - val_q_or_hat_loss: 0.0041 - val_v_or_hat_loss: 2.7724e-05 - val_v_ex_hat_loss: 3.2775e-05\n",
      "Epoch 56/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0150 - a_or_hat_loss: 0.0024 - a_ex_hat_loss: 0.0023 - p_or_hat_loss: 0.0013 - p_ex_hat_loss: 0.0013 - q_ex_hat_loss: 0.0042 - q_or_hat_loss: 0.0035 - v_or_hat_loss: 2.5438e-05 - v_ex_hat_loss: 3.2249e-05 - val_loss: 0.0175 - val_a_or_hat_loss: 0.0026 - val_a_ex_hat_loss: 0.0026 - val_p_or_hat_loss: 0.0016 - val_p_ex_hat_loss: 0.0014 - val_q_ex_hat_loss: 0.0051 - val_q_or_hat_loss: 0.0042 - val_v_or_hat_loss: 2.0295e-05 - val_v_ex_hat_loss: 3.0310e-05\n",
      "Epoch 57/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0146 - a_or_hat_loss: 0.0023 - a_ex_hat_loss: 0.0023 - p_or_hat_loss: 0.0013 - p_ex_hat_loss: 0.0012 - q_ex_hat_loss: 0.0040 - q_or_hat_loss: 0.0034 - v_or_hat_loss: 2.3266e-05 - v_ex_hat_loss: 3.1295e-05 - val_loss: 0.0175 - val_a_or_hat_loss: 0.0026 - val_a_ex_hat_loss: 0.0030 - val_p_or_hat_loss: 0.0015 - val_p_ex_hat_loss: 0.0015 - val_q_ex_hat_loss: 0.0045 - val_q_or_hat_loss: 0.0043 - val_v_or_hat_loss: 1.6398e-05 - val_v_ex_hat_loss: 2.4029e-05\n",
      "Epoch 58/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0145 - a_or_hat_loss: 0.0023 - a_ex_hat_loss: 0.0023 - p_or_hat_loss: 0.0012 - p_ex_hat_loss: 0.0012 - q_ex_hat_loss: 0.0041 - q_or_hat_loss: 0.0034 - v_or_hat_loss: 2.0176e-05 - v_ex_hat_loss: 2.6482e-05 - val_loss: 0.0182 - val_a_or_hat_loss: 0.0026 - val_a_ex_hat_loss: 0.0027 - val_p_or_hat_loss: 0.0018 - val_p_ex_hat_loss: 0.0017 - val_q_ex_hat_loss: 0.0050 - val_q_or_hat_loss: 0.0043 - val_v_or_hat_loss: 3.5337e-05 - val_v_ex_hat_loss: 3.8651e-05\n",
      "Epoch 59/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0145 - a_or_hat_loss: 0.0023 - a_ex_hat_loss: 0.0022 - p_or_hat_loss: 0.0012 - p_ex_hat_loss: 0.0012 - q_ex_hat_loss: 0.0040 - q_or_hat_loss: 0.0034 - v_or_hat_loss: 2.6462e-05 - v_ex_hat_loss: 3.4223e-05 - val_loss: 0.0165 - val_a_or_hat_loss: 0.0026 - val_a_ex_hat_loss: 0.0027 - val_p_or_hat_loss: 0.0015 - val_p_ex_hat_loss: 0.0014 - val_q_ex_hat_loss: 0.0045 - val_q_or_hat_loss: 0.0038 - val_v_or_hat_loss: 2.1646e-05 - val_v_ex_hat_loss: 2.4951e-05\n",
      "Epoch 60/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0144 - a_or_hat_loss: 0.0023 - a_ex_hat_loss: 0.0022 - p_or_hat_loss: 0.0012 - p_ex_hat_loss: 0.0012 - q_ex_hat_loss: 0.0040 - q_or_hat_loss: 0.0034 - v_or_hat_loss: 2.3419e-05 - v_ex_hat_loss: 3.0030e-05 - val_loss: 0.0179 - val_a_or_hat_loss: 0.0027 - val_a_ex_hat_loss: 0.0027 - val_p_or_hat_loss: 0.0016 - val_p_ex_hat_loss: 0.0017 - val_q_ex_hat_loss: 0.0053 - val_q_or_hat_loss: 0.0038 - val_v_or_hat_loss: 2.3225e-05 - val_v_ex_hat_loss: 2.6809e-05\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0141 - a_or_hat_loss: 0.0022 - a_ex_hat_loss: 0.0022 - p_or_hat_loss: 0.0012 - p_ex_hat_loss: 0.0012 - q_ex_hat_loss: 0.0039 - q_or_hat_loss: 0.0033 - v_or_hat_loss: 2.4072e-05 - v_ex_hat_loss: 3.1155e-05 - val_loss: 0.0162 - val_a_or_hat_loss: 0.0025 - val_a_ex_hat_loss: 0.0025 - val_p_or_hat_loss: 0.0014 - val_p_ex_hat_loss: 0.0014 - val_q_ex_hat_loss: 0.0047 - val_q_or_hat_loss: 0.0037 - val_v_or_hat_loss: 1.8158e-05 - val_v_ex_hat_loss: 2.3364e-05\n",
      "Epoch 62/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0139 - a_or_hat_loss: 0.0022 - a_ex_hat_loss: 0.0022 - p_or_hat_loss: 0.0012 - p_ex_hat_loss: 0.0011 - q_ex_hat_loss: 0.0039 - q_or_hat_loss: 0.0033 - v_or_hat_loss: 2.1107e-05 - v_ex_hat_loss: 2.7852e-05 - val_loss: 0.0159 - val_a_or_hat_loss: 0.0024 - val_a_ex_hat_loss: 0.0025 - val_p_or_hat_loss: 0.0013 - val_p_ex_hat_loss: 0.0013 - val_q_ex_hat_loss: 0.0046 - val_q_or_hat_loss: 0.0037 - val_v_or_hat_loss: 1.7134e-05 - val_v_ex_hat_loss: 2.2511e-05\n",
      "Epoch 63/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0136 - a_or_hat_loss: 0.0022 - a_ex_hat_loss: 0.0021 - p_or_hat_loss: 0.0012 - p_ex_hat_loss: 0.0011 - q_ex_hat_loss: 0.0038 - q_or_hat_loss: 0.0032 - v_or_hat_loss: 2.2796e-05 - v_ex_hat_loss: 2.9694e-05 - val_loss: 0.0158 - val_a_or_hat_loss: 0.0025 - val_a_ex_hat_loss: 0.0025 - val_p_or_hat_loss: 0.0015 - val_p_ex_hat_loss: 0.0014 - val_q_ex_hat_loss: 0.0043 - val_q_or_hat_loss: 0.0036 - val_v_or_hat_loss: 1.7563e-05 - val_v_ex_hat_loss: 2.2859e-05\n",
      "Epoch 64/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0135 - a_or_hat_loss: 0.0021 - a_ex_hat_loss: 0.0021 - p_or_hat_loss: 0.0011 - p_ex_hat_loss: 0.0011 - q_ex_hat_loss: 0.0038 - q_or_hat_loss: 0.0032 - v_or_hat_loss: 2.0416e-05 - v_ex_hat_loss: 2.6133e-05 - val_loss: 0.0169 - val_a_or_hat_loss: 0.0026 - val_a_ex_hat_loss: 0.0027 - val_p_or_hat_loss: 0.0014 - val_p_ex_hat_loss: 0.0013 - val_q_ex_hat_loss: 0.0046 - val_q_or_hat_loss: 0.0043 - val_v_or_hat_loss: 1.9940e-05 - val_v_ex_hat_loss: 2.5286e-05\n",
      "Epoch 65/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0135 - a_or_hat_loss: 0.0021 - a_ex_hat_loss: 0.0021 - p_or_hat_loss: 0.0011 - p_ex_hat_loss: 0.0011 - q_ex_hat_loss: 0.0038 - q_or_hat_loss: 0.0032 - v_or_hat_loss: 2.3082e-05 - v_ex_hat_loss: 3.0350e-05 - val_loss: 0.0158 - val_a_or_hat_loss: 0.0023 - val_a_ex_hat_loss: 0.0025 - val_p_or_hat_loss: 0.0016 - val_p_ex_hat_loss: 0.0013 - val_q_ex_hat_loss: 0.0044 - val_q_or_hat_loss: 0.0037 - val_v_or_hat_loss: 1.6810e-05 - val_v_ex_hat_loss: 2.5361e-05\n",
      "Epoch 66/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0133 - a_or_hat_loss: 0.0021 - a_ex_hat_loss: 0.0021 - p_or_hat_loss: 0.0011 - p_ex_hat_loss: 0.0011 - q_ex_hat_loss: 0.0037 - q_or_hat_loss: 0.0031 - v_or_hat_loss: 2.1565e-05 - v_ex_hat_loss: 2.8721e-05 - val_loss: 0.0158 - val_a_or_hat_loss: 0.0025 - val_a_ex_hat_loss: 0.0025 - val_p_or_hat_loss: 0.0014 - val_p_ex_hat_loss: 0.0015 - val_q_ex_hat_loss: 0.0044 - val_q_or_hat_loss: 0.0036 - val_v_or_hat_loss: 2.7846e-05 - val_v_ex_hat_loss: 3.7825e-05\n",
      "Epoch 67/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0132 - a_or_hat_loss: 0.0020 - a_ex_hat_loss: 0.0021 - p_or_hat_loss: 0.0011 - p_ex_hat_loss: 0.0011 - q_ex_hat_loss: 0.0037 - q_or_hat_loss: 0.0031 - v_or_hat_loss: 2.0536e-05 - v_ex_hat_loss: 2.6487e-05 - val_loss: 0.0154 - val_a_or_hat_loss: 0.0023 - val_a_ex_hat_loss: 0.0024 - val_p_or_hat_loss: 0.0013 - val_p_ex_hat_loss: 0.0012 - val_q_ex_hat_loss: 0.0041 - val_q_or_hat_loss: 0.0040 - val_v_or_hat_loss: 2.2432e-05 - val_v_ex_hat_loss: 2.8252e-05\n",
      "Epoch 68/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0131 - a_or_hat_loss: 0.0020 - a_ex_hat_loss: 0.0020 - p_or_hat_loss: 0.0011 - p_ex_hat_loss: 0.0011 - q_ex_hat_loss: 0.0036 - q_or_hat_loss: 0.0032 - v_or_hat_loss: 1.9539e-05 - v_ex_hat_loss: 2.5541e-05 - val_loss: 0.0154 - val_a_or_hat_loss: 0.0022 - val_a_ex_hat_loss: 0.0023 - val_p_or_hat_loss: 0.0013 - val_p_ex_hat_loss: 0.0013 - val_q_ex_hat_loss: 0.0045 - val_q_or_hat_loss: 0.0038 - val_v_or_hat_loss: 1.5649e-05 - val_v_ex_hat_loss: 1.9899e-05\n",
      "Epoch 69/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0128 - a_or_hat_loss: 0.0020 - a_ex_hat_loss: 0.0020 - p_or_hat_loss: 0.0011 - p_ex_hat_loss: 0.0010 - q_ex_hat_loss: 0.0035 - q_or_hat_loss: 0.0031 - v_or_hat_loss: 1.8863e-05 - v_ex_hat_loss: 2.5342e-05 - val_loss: 0.0158 - val_a_or_hat_loss: 0.0024 - val_a_ex_hat_loss: 0.0025 - val_p_or_hat_loss: 0.0014 - val_p_ex_hat_loss: 0.0014 - val_q_ex_hat_loss: 0.0044 - val_q_or_hat_loss: 0.0037 - val_v_or_hat_loss: 1.8515e-05 - val_v_ex_hat_loss: 1.9663e-05\n",
      "Epoch 70/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0128 - a_or_hat_loss: 0.0020 - a_ex_hat_loss: 0.0020 - p_or_hat_loss: 0.0011 - p_ex_hat_loss: 0.0011 - q_ex_hat_loss: 0.0036 - q_or_hat_loss: 0.0030 - v_or_hat_loss: 2.0000e-05 - v_ex_hat_loss: 2.6739e-05 - val_loss: 0.0155 - val_a_or_hat_loss: 0.0022 - val_a_ex_hat_loss: 0.0025 - val_p_or_hat_loss: 0.0013 - val_p_ex_hat_loss: 0.0014 - val_q_ex_hat_loss: 0.0046 - val_q_or_hat_loss: 0.0035 - val_v_or_hat_loss: 1.3330e-05 - val_v_ex_hat_loss: 1.9170e-05\n",
      "Epoch 71/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0126 - a_or_hat_loss: 0.0020 - a_ex_hat_loss: 0.0020 - p_or_hat_loss: 0.0010 - p_ex_hat_loss: 0.0010 - q_ex_hat_loss: 0.0036 - q_or_hat_loss: 0.0030 - v_or_hat_loss: 1.8782e-05 - v_ex_hat_loss: 2.5302e-05 - val_loss: 0.0149 - val_a_or_hat_loss: 0.0021 - val_a_ex_hat_loss: 0.0023 - val_p_or_hat_loss: 0.0012 - val_p_ex_hat_loss: 0.0012 - val_q_ex_hat_loss: 0.0041 - val_q_or_hat_loss: 0.0039 - val_v_or_hat_loss: 3.7512e-05 - val_v_ex_hat_loss: 5.1056e-05\n",
      "Epoch 72/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0124 - a_or_hat_loss: 0.0019 - a_ex_hat_loss: 0.0020 - p_or_hat_loss: 0.0010 - p_ex_hat_loss: 0.0010 - q_ex_hat_loss: 0.0034 - q_or_hat_loss: 0.0030 - v_or_hat_loss: 1.9957e-05 - v_ex_hat_loss: 2.6751e-05 - val_loss: 0.0165 - val_a_or_hat_loss: 0.0025 - val_a_ex_hat_loss: 0.0025 - val_p_or_hat_loss: 0.0016 - val_p_ex_hat_loss: 0.0016 - val_q_ex_hat_loss: 0.0043 - val_q_or_hat_loss: 0.0039 - val_v_or_hat_loss: 2.8228e-05 - val_v_ex_hat_loss: 3.1119e-05\n",
      "Epoch 73/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0128 - a_or_hat_loss: 0.0020 - a_ex_hat_loss: 0.0020 - p_or_hat_loss: 0.0011 - p_ex_hat_loss: 0.0011 - q_ex_hat_loss: 0.0036 - q_or_hat_loss: 0.0031 - v_or_hat_loss: 1.9215e-05 - v_ex_hat_loss: 2.4852e-05 - val_loss: 0.0143 - val_a_or_hat_loss: 0.0022 - val_a_ex_hat_loss: 0.0022 - val_p_or_hat_loss: 0.0012 - val_p_ex_hat_loss: 0.0012 - val_q_ex_hat_loss: 0.0040 - val_q_or_hat_loss: 0.0035 - val_v_or_hat_loss: 1.3353e-05 - val_v_ex_hat_loss: 1.9603e-05\n",
      "Epoch 74/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0123 - a_or_hat_loss: 0.0019 - a_ex_hat_loss: 0.0019 - p_or_hat_loss: 9.8996e-04 - p_ex_hat_loss: 9.6315e-04 - q_ex_hat_loss: 0.0035 - q_or_hat_loss: 0.0029 - v_or_hat_loss: 1.7430e-05 - v_ex_hat_loss: 2.4101e-05 - val_loss: 0.0161 - val_a_or_hat_loss: 0.0024 - val_a_ex_hat_loss: 0.0023 - val_p_or_hat_loss: 0.0014 - val_p_ex_hat_loss: 0.0016 - val_q_ex_hat_loss: 0.0049 - val_q_or_hat_loss: 0.0036 - val_v_or_hat_loss: 1.7387e-05 - val_v_ex_hat_loss: 2.7326e-05\n",
      "Epoch 75/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0124 - a_or_hat_loss: 0.0019 - a_ex_hat_loss: 0.0020 - p_or_hat_loss: 0.0010 - p_ex_hat_loss: 0.0011 - q_ex_hat_loss: 0.0035 - q_or_hat_loss: 0.0029 - v_or_hat_loss: 1.9347e-05 - v_ex_hat_loss: 2.6880e-05 - val_loss: 0.0144 - val_a_or_hat_loss: 0.0021 - val_a_ex_hat_loss: 0.0023 - val_p_or_hat_loss: 0.0015 - val_p_ex_hat_loss: 0.0013 - val_q_ex_hat_loss: 0.0038 - val_q_or_hat_loss: 0.0033 - val_v_or_hat_loss: 1.5677e-05 - val_v_ex_hat_loss: 2.3180e-05\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0121 - a_or_hat_loss: 0.0019 - a_ex_hat_loss: 0.0019 - p_or_hat_loss: 9.7525e-04 - p_ex_hat_loss: 9.5273e-04 - q_ex_hat_loss: 0.0034 - q_or_hat_loss: 0.0029 - v_or_hat_loss: 1.8404e-05 - v_ex_hat_loss: 2.4719e-05 - val_loss: 0.0147 - val_a_or_hat_loss: 0.0023 - val_a_ex_hat_loss: 0.0024 - val_p_or_hat_loss: 0.0012 - val_p_ex_hat_loss: 0.0012 - val_q_ex_hat_loss: 0.0041 - val_q_or_hat_loss: 0.0035 - val_v_or_hat_loss: 1.5988e-05 - val_v_ex_hat_loss: 2.0215e-05\n",
      "Epoch 77/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0119 - a_or_hat_loss: 0.0019 - a_ex_hat_loss: 0.0019 - p_or_hat_loss: 9.8895e-04 - p_ex_hat_loss: 9.7281e-04 - q_ex_hat_loss: 0.0033 - q_or_hat_loss: 0.0029 - v_or_hat_loss: 1.6743e-05 - v_ex_hat_loss: 2.2683e-05 - val_loss: 0.0152 - val_a_or_hat_loss: 0.0022 - val_a_ex_hat_loss: 0.0023 - val_p_or_hat_loss: 0.0012 - val_p_ex_hat_loss: 0.0015 - val_q_ex_hat_loss: 0.0043 - val_q_or_hat_loss: 0.0037 - val_v_or_hat_loss: 5.2366e-05 - val_v_ex_hat_loss: 6.9733e-05\n",
      "Epoch 78/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0119 - a_or_hat_loss: 0.0019 - a_ex_hat_loss: 0.0019 - p_or_hat_loss: 9.6458e-04 - p_ex_hat_loss: 0.0010 - q_ex_hat_loss: 0.0033 - q_or_hat_loss: 0.0028 - v_or_hat_loss: 1.7082e-05 - v_ex_hat_loss: 2.3504e-05 - val_loss: 0.0143 - val_a_or_hat_loss: 0.0021 - val_a_ex_hat_loss: 0.0022 - val_p_or_hat_loss: 0.0014 - val_p_ex_hat_loss: 0.0012 - val_q_ex_hat_loss: 0.0039 - val_q_or_hat_loss: 0.0035 - val_v_or_hat_loss: 1.3464e-05 - val_v_ex_hat_loss: 2.1338e-05\n",
      "Epoch 79/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0118 - a_or_hat_loss: 0.0019 - a_ex_hat_loss: 0.0019 - p_or_hat_loss: 9.8679e-04 - p_ex_hat_loss: 9.6415e-04 - q_ex_hat_loss: 0.0033 - q_or_hat_loss: 0.0028 - v_or_hat_loss: 1.6954e-05 - v_ex_hat_loss: 2.3836e-05 - val_loss: 0.0136 - val_a_or_hat_loss: 0.0021 - val_a_ex_hat_loss: 0.0022 - val_p_or_hat_loss: 0.0012 - val_p_ex_hat_loss: 0.0012 - val_q_ex_hat_loss: 0.0037 - val_q_or_hat_loss: 0.0032 - val_v_or_hat_loss: 1.4439e-05 - val_v_ex_hat_loss: 1.7300e-05\n",
      "Epoch 80/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0116 - a_or_hat_loss: 0.0018 - a_ex_hat_loss: 0.0018 - p_or_hat_loss: 9.6491e-04 - p_ex_hat_loss: 9.7514e-04 - q_ex_hat_loss: 0.0032 - q_or_hat_loss: 0.0027 - v_or_hat_loss: 1.6609e-05 - v_ex_hat_loss: 2.2174e-05 - val_loss: 0.0141 - val_a_or_hat_loss: 0.0023 - val_a_ex_hat_loss: 0.0023 - val_p_or_hat_loss: 0.0011 - val_p_ex_hat_loss: 0.0012 - val_q_ex_hat_loss: 0.0038 - val_q_or_hat_loss: 0.0033 - val_v_or_hat_loss: 1.0721e-05 - val_v_ex_hat_loss: 1.7390e-05\n",
      "Epoch 81/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0116 - a_or_hat_loss: 0.0018 - a_ex_hat_loss: 0.0018 - p_or_hat_loss: 9.4743e-04 - p_ex_hat_loss: 9.4543e-04 - q_ex_hat_loss: 0.0032 - q_or_hat_loss: 0.0027 - v_or_hat_loss: 1.5888e-05 - v_ex_hat_loss: 2.2367e-05 - val_loss: 0.0133 - val_a_or_hat_loss: 0.0020 - val_a_ex_hat_loss: 0.0021 - val_p_or_hat_loss: 0.0010 - val_p_ex_hat_loss: 0.0011 - val_q_ex_hat_loss: 0.0040 - val_q_or_hat_loss: 0.0031 - val_v_or_hat_loss: 1.1419e-05 - val_v_ex_hat_loss: 1.7066e-05\n",
      "Epoch 82/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0113 - a_or_hat_loss: 0.0018 - a_ex_hat_loss: 0.0018 - p_or_hat_loss: 9.6037e-04 - p_ex_hat_loss: 9.4145e-04 - q_ex_hat_loss: 0.0031 - q_or_hat_loss: 0.0027 - v_or_hat_loss: 1.6900e-05 - v_ex_hat_loss: 2.2944e-05 - val_loss: 0.0146 - val_a_or_hat_loss: 0.0022 - val_a_ex_hat_loss: 0.0023 - val_p_or_hat_loss: 0.0013 - val_p_ex_hat_loss: 0.0013 - val_q_ex_hat_loss: 0.0040 - val_q_or_hat_loss: 0.0034 - val_v_or_hat_loss: 2.3371e-05 - val_v_ex_hat_loss: 2.5177e-05\n",
      "Epoch 83/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0115 - a_or_hat_loss: 0.0018 - a_ex_hat_loss: 0.0018 - p_or_hat_loss: 9.5507e-04 - p_ex_hat_loss: 9.6019e-04 - q_ex_hat_loss: 0.0032 - q_or_hat_loss: 0.0028 - v_or_hat_loss: 1.7193e-05 - v_ex_hat_loss: 2.3593e-05 - val_loss: 0.0139 - val_a_or_hat_loss: 0.0021 - val_a_ex_hat_loss: 0.0021 - val_p_or_hat_loss: 0.0013 - val_p_ex_hat_loss: 0.0013 - val_q_ex_hat_loss: 0.0037 - val_q_or_hat_loss: 0.0034 - val_v_or_hat_loss: 1.5418e-05 - val_v_ex_hat_loss: 1.7361e-05\n",
      "Epoch 84/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0111 - a_or_hat_loss: 0.0017 - a_ex_hat_loss: 0.0018 - p_or_hat_loss: 8.8533e-04 - p_ex_hat_loss: 8.7054e-04 - q_ex_hat_loss: 0.0031 - q_or_hat_loss: 0.0027 - v_or_hat_loss: 1.5621e-05 - v_ex_hat_loss: 2.1348e-05 - val_loss: 0.0133 - val_a_or_hat_loss: 0.0021 - val_a_ex_hat_loss: 0.0022 - val_p_or_hat_loss: 0.0011 - val_p_ex_hat_loss: 0.0010 - val_q_ex_hat_loss: 0.0037 - val_q_or_hat_loss: 0.0032 - val_v_or_hat_loss: 1.6898e-05 - val_v_ex_hat_loss: 2.1903e-05\n",
      "Epoch 85/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0109 - a_or_hat_loss: 0.0017 - a_ex_hat_loss: 0.0018 - p_or_hat_loss: 8.3474e-04 - p_ex_hat_loss: 8.3704e-04 - q_ex_hat_loss: 0.0030 - q_or_hat_loss: 0.0026 - v_or_hat_loss: 1.3220e-05 - v_ex_hat_loss: 1.9216e-05 - val_loss: 0.0136 - val_a_or_hat_loss: 0.0020 - val_a_ex_hat_loss: 0.0022 - val_p_or_hat_loss: 0.0011 - val_p_ex_hat_loss: 0.0012 - val_q_ex_hat_loss: 0.0037 - val_q_or_hat_loss: 0.0032 - val_v_or_hat_loss: 1.4781e-05 - val_v_ex_hat_loss: 1.7797e-05\n",
      "Epoch 86/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0112 - a_or_hat_loss: 0.0018 - a_ex_hat_loss: 0.0018 - p_or_hat_loss: 9.0382e-04 - p_ex_hat_loss: 9.0279e-04 - q_ex_hat_loss: 0.0031 - q_or_hat_loss: 0.0027 - v_or_hat_loss: 1.5980e-05 - v_ex_hat_loss: 2.2106e-05 - val_loss: 0.0149 - val_a_or_hat_loss: 0.0020 - val_a_ex_hat_loss: 0.0024 - val_p_or_hat_loss: 0.0012 - val_p_ex_hat_loss: 0.0012 - val_q_ex_hat_loss: 0.0043 - val_q_or_hat_loss: 0.0037 - val_v_or_hat_loss: 4.0431e-05 - val_v_ex_hat_loss: 4.8354e-05\n",
      "Epoch 87/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0113 - a_or_hat_loss: 0.0018 - a_ex_hat_loss: 0.0018 - p_or_hat_loss: 9.3520e-04 - p_ex_hat_loss: 9.2938e-04 - q_ex_hat_loss: 0.0032 - q_or_hat_loss: 0.0027 - v_or_hat_loss: 1.6811e-05 - v_ex_hat_loss: 2.2985e-05 - val_loss: 0.0143 - val_a_or_hat_loss: 0.0020 - val_a_ex_hat_loss: 0.0022 - val_p_or_hat_loss: 0.0014 - val_p_ex_hat_loss: 0.0019 - val_q_ex_hat_loss: 0.0036 - val_q_or_hat_loss: 0.0032 - val_v_or_hat_loss: 1.1776e-05 - val_v_ex_hat_loss: 1.8368e-05\n",
      "Epoch 88/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0110 - a_or_hat_loss: 0.0017 - a_ex_hat_loss: 0.0018 - p_or_hat_loss: 8.9172e-04 - p_ex_hat_loss: 9.4802e-04 - q_ex_hat_loss: 0.0030 - q_or_hat_loss: 0.0026 - v_or_hat_loss: 1.3634e-05 - v_ex_hat_loss: 2.0197e-05 - val_loss: 0.0136 - val_a_or_hat_loss: 0.0020 - val_a_ex_hat_loss: 0.0022 - val_p_or_hat_loss: 0.0012 - val_p_ex_hat_loss: 0.0012 - val_q_ex_hat_loss: 0.0038 - val_q_or_hat_loss: 0.0032 - val_v_or_hat_loss: 1.0843e-05 - val_v_ex_hat_loss: 1.5011e-05\n",
      "Epoch 89/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0108 - a_or_hat_loss: 0.0017 - a_ex_hat_loss: 0.0017 - p_or_hat_loss: 8.7769e-04 - p_ex_hat_loss: 8.9761e-04 - q_ex_hat_loss: 0.0030 - q_or_hat_loss: 0.0026 - v_or_hat_loss: 1.4909e-05 - v_ex_hat_loss: 2.1134e-05 - val_loss: 0.0138 - val_a_or_hat_loss: 0.0021 - val_a_ex_hat_loss: 0.0021 - val_p_or_hat_loss: 0.0012 - val_p_ex_hat_loss: 0.0011 - val_q_ex_hat_loss: 0.0040 - val_q_or_hat_loss: 0.0032 - val_v_or_hat_loss: 1.6416e-05 - val_v_ex_hat_loss: 2.4531e-05\n",
      "Epoch 90/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0107 - a_or_hat_loss: 0.0017 - a_ex_hat_loss: 0.0017 - p_or_hat_loss: 8.5861e-04 - p_ex_hat_loss: 8.5875e-04 - q_ex_hat_loss: 0.0030 - q_or_hat_loss: 0.0026 - v_or_hat_loss: 1.5432e-05 - v_ex_hat_loss: 2.1027e-05 - val_loss: 0.0120 - val_a_or_hat_loss: 0.0019 - val_a_ex_hat_loss: 0.0020 - val_p_or_hat_loss: 9.6704e-04 - val_p_ex_hat_loss: 9.3162e-04 - val_q_ex_hat_loss: 0.0033 - val_q_or_hat_loss: 0.0029 - val_v_or_hat_loss: 1.5736e-05 - val_v_ex_hat_loss: 2.3655e-05\n",
      "Epoch 91/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0108 - a_or_hat_loss: 0.0017 - a_ex_hat_loss: 0.0017 - p_or_hat_loss: 9.2716e-04 - p_ex_hat_loss: 8.7985e-04 - q_ex_hat_loss: 0.0030 - q_or_hat_loss: 0.0026 - v_or_hat_loss: 1.5008e-05 - v_ex_hat_loss: 2.1338e-05 - val_loss: 0.0142 - val_a_or_hat_loss: 0.0019 - val_a_ex_hat_loss: 0.0022 - val_p_or_hat_loss: 0.0010 - val_p_ex_hat_loss: 0.0013 - val_q_ex_hat_loss: 0.0042 - val_q_or_hat_loss: 0.0036 - val_v_or_hat_loss: 8.9414e-06 - val_v_ex_hat_loss: 1.4676e-05\n",
      "Epoch 92/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0106 - a_or_hat_loss: 0.0017 - a_ex_hat_loss: 0.0017 - p_or_hat_loss: 8.5485e-04 - p_ex_hat_loss: 8.7497e-04 - q_ex_hat_loss: 0.0030 - q_or_hat_loss: 0.0026 - v_or_hat_loss: 1.5470e-05 - v_ex_hat_loss: 2.1117e-05 - val_loss: 0.0127 - val_a_or_hat_loss: 0.0019 - val_a_ex_hat_loss: 0.0021 - val_p_or_hat_loss: 0.0011 - val_p_ex_hat_loss: 9.8991e-04 - val_q_ex_hat_loss: 0.0033 - val_q_or_hat_loss: 0.0032 - val_v_or_hat_loss: 3.1697e-05 - val_v_ex_hat_loss: 4.1271e-05\n",
      "Epoch 93/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0107 - a_or_hat_loss: 0.0017 - a_ex_hat_loss: 0.0017 - p_or_hat_loss: 8.7005e-04 - p_ex_hat_loss: 8.8056e-04 - q_ex_hat_loss: 0.0029 - q_or_hat_loss: 0.0026 - v_or_hat_loss: 1.4417e-05 - v_ex_hat_loss: 1.9678e-05 - val_loss: 0.0126 - val_a_or_hat_loss: 0.0019 - val_a_ex_hat_loss: 0.0020 - val_p_or_hat_loss: 0.0012 - val_p_ex_hat_loss: 0.0011 - val_q_ex_hat_loss: 0.0036 - val_q_or_hat_loss: 0.0028 - val_v_or_hat_loss: 1.5104e-05 - val_v_ex_hat_loss: 2.3726e-05\n",
      "Epoch 94/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0104 - a_or_hat_loss: 0.0017 - a_ex_hat_loss: 0.0017 - p_or_hat_loss: 8.3977e-04 - p_ex_hat_loss: 8.4480e-04 - q_ex_hat_loss: 0.0029 - q_or_hat_loss: 0.0024 - v_or_hat_loss: 1.5228e-05 - v_ex_hat_loss: 2.0370e-05 - val_loss: 0.0131 - val_a_or_hat_loss: 0.0019 - val_a_ex_hat_loss: 0.0020 - val_p_or_hat_loss: 0.0012 - val_p_ex_hat_loss: 0.0011 - val_q_ex_hat_loss: 0.0037 - val_q_or_hat_loss: 0.0033 - val_v_or_hat_loss: 1.3886e-05 - val_v_ex_hat_loss: 1.8240e-05\n",
      "Epoch 95/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0104 - a_or_hat_loss: 0.0016 - a_ex_hat_loss: 0.0017 - p_or_hat_loss: 8.4655e-04 - p_ex_hat_loss: 8.4178e-04 - q_ex_hat_loss: 0.0029 - q_or_hat_loss: 0.0025 - v_or_hat_loss: 1.4045e-05 - v_ex_hat_loss: 1.9645e-05 - val_loss: 0.0119 - val_a_or_hat_loss: 0.0019 - val_a_ex_hat_loss: 0.0020 - val_p_or_hat_loss: 9.2914e-04 - val_p_ex_hat_loss: 9.5239e-04 - val_q_ex_hat_loss: 0.0034 - val_q_or_hat_loss: 0.0027 - val_v_or_hat_loss: 9.8868e-06 - val_v_ex_hat_loss: 1.3865e-05\n",
      "Epoch 96/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0101 - a_or_hat_loss: 0.0016 - a_ex_hat_loss: 0.0016 - p_or_hat_loss: 8.1304e-04 - p_ex_hat_loss: 8.2454e-04 - q_ex_hat_loss: 0.0028 - q_or_hat_loss: 0.0024 - v_or_hat_loss: 1.3117e-05 - v_ex_hat_loss: 1.8389e-05 - val_loss: 0.0128 - val_a_or_hat_loss: 0.0020 - val_a_ex_hat_loss: 0.0020 - val_p_or_hat_loss: 0.0010 - val_p_ex_hat_loss: 0.0010 - val_q_ex_hat_loss: 0.0036 - val_q_or_hat_loss: 0.0031 - val_v_or_hat_loss: 8.8919e-06 - val_v_ex_hat_loss: 1.4202e-05\n",
      "Epoch 97/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0102 - a_or_hat_loss: 0.0016 - a_ex_hat_loss: 0.0017 - p_or_hat_loss: 8.0936e-04 - p_ex_hat_loss: 8.0849e-04 - q_ex_hat_loss: 0.0028 - q_or_hat_loss: 0.0024 - v_or_hat_loss: 1.2410e-05 - v_ex_hat_loss: 1.6934e-05 - val_loss: 0.0124 - val_a_or_hat_loss: 0.0018 - val_a_ex_hat_loss: 0.0021 - val_p_or_hat_loss: 0.0010 - val_p_ex_hat_loss: 0.0011 - val_q_ex_hat_loss: 0.0034 - val_q_or_hat_loss: 0.0029 - val_v_or_hat_loss: 1.0845e-05 - val_v_ex_hat_loss: 1.7056e-05\n",
      "Epoch 98/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0100 - a_or_hat_loss: 0.0016 - a_ex_hat_loss: 0.0016 - p_or_hat_loss: 8.1329e-04 - p_ex_hat_loss: 8.2008e-04 - q_ex_hat_loss: 0.0027 - q_or_hat_loss: 0.0024 - v_or_hat_loss: 1.4190e-05 - v_ex_hat_loss: 1.9720e-05 - val_loss: 0.0120 - val_a_or_hat_loss: 0.0019 - val_a_ex_hat_loss: 0.0020 - val_p_or_hat_loss: 9.6886e-04 - val_p_ex_hat_loss: 9.8322e-04 - val_q_ex_hat_loss: 0.0032 - val_q_or_hat_loss: 0.0029 - val_v_or_hat_loss: 1.0909e-05 - val_v_ex_hat_loss: 1.9215e-05\n",
      "Epoch 99/200\n",
      "782/782 [==============================] - 14s 19ms/step - loss: 0.0102 - a_or_hat_loss: 0.0016 - a_ex_hat_loss: 0.0016 - p_or_hat_loss: 8.2324e-04 - p_ex_hat_loss: 8.4599e-04 - q_ex_hat_loss: 0.0028 - q_or_hat_loss: 0.0024 - v_or_hat_loss: 1.4338e-05 - v_ex_hat_loss: 1.9721e-05 - val_loss: 0.0121 - val_a_or_hat_loss: 0.0019 - val_a_ex_hat_loss: 0.0022 - val_p_or_hat_loss: 8.5375e-04 - val_p_ex_hat_loss: 9.6376e-04 - val_q_ex_hat_loss: 0.0034 - val_q_or_hat_loss: 0.0028 - val_v_or_hat_loss: 1.8499e-05 - val_v_ex_hat_loss: 2.3643e-05\n",
      "Epoch 100/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0100 - a_or_hat_loss: 0.0016 - a_ex_hat_loss: 0.0016 - p_or_hat_loss: 8.0797e-04 - p_ex_hat_loss: 8.0451e-04 - q_ex_hat_loss: 0.0028 - q_or_hat_loss: 0.0024 - v_or_hat_loss: 1.3374e-05 - v_ex_hat_loss: 1.8683e-05 - val_loss: 0.0121 - val_a_or_hat_loss: 0.0019 - val_a_ex_hat_loss: 0.0019 - val_p_or_hat_loss: 0.0011 - val_p_ex_hat_loss: 0.0011 - val_q_ex_hat_loss: 0.0031 - val_q_or_hat_loss: 0.0029 - val_v_or_hat_loss: 1.1841e-05 - val_v_ex_hat_loss: 1.5947e-05\n",
      "Epoch 101/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0100 - a_or_hat_loss: 0.0016 - a_ex_hat_loss: 0.0016 - p_or_hat_loss: 7.8270e-04 - p_ex_hat_loss: 7.9772e-04 - q_ex_hat_loss: 0.0028 - q_or_hat_loss: 0.0024 - v_or_hat_loss: 1.5341e-05 - v_ex_hat_loss: 2.0907e-05 - val_loss: 0.0127 - val_a_or_hat_loss: 0.0019 - val_a_ex_hat_loss: 0.0020 - val_p_or_hat_loss: 0.0015 - val_p_ex_hat_loss: 0.0011 - val_q_ex_hat_loss: 0.0032 - val_q_or_hat_loss: 0.0029 - val_v_or_hat_loss: 1.0081e-05 - val_v_ex_hat_loss: 1.3078e-05\n",
      "Epoch 102/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0099 - a_or_hat_loss: 0.0016 - a_ex_hat_loss: 0.0016 - p_or_hat_loss: 8.3255e-04 - p_ex_hat_loss: 8.2172e-04 - q_ex_hat_loss: 0.0027 - q_or_hat_loss: 0.0023 - v_or_hat_loss: 1.2627e-05 - v_ex_hat_loss: 1.8006e-05 - val_loss: 0.0117 - val_a_or_hat_loss: 0.0017 - val_a_ex_hat_loss: 0.0019 - val_p_or_hat_loss: 9.7430e-04 - val_p_ex_hat_loss: 0.0010 - val_q_ex_hat_loss: 0.0033 - val_q_or_hat_loss: 0.0028 - val_v_or_hat_loss: 8.4345e-06 - val_v_ex_hat_loss: 1.1903e-05\n",
      "Epoch 103/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0096 - a_or_hat_loss: 0.0015 - a_ex_hat_loss: 0.0016 - p_or_hat_loss: 7.5711e-04 - p_ex_hat_loss: 7.6254e-04 - q_ex_hat_loss: 0.0026 - q_or_hat_loss: 0.0023 - v_or_hat_loss: 1.1613e-05 - v_ex_hat_loss: 1.6959e-05 - val_loss: 0.0130 - val_a_or_hat_loss: 0.0018 - val_a_ex_hat_loss: 0.0019 - val_p_or_hat_loss: 8.8916e-04 - val_p_ex_hat_loss: 0.0011 - val_q_ex_hat_loss: 0.0039 - val_q_or_hat_loss: 0.0034 - val_v_or_hat_loss: 8.4689e-06 - val_v_ex_hat_loss: 1.3252e-05\n",
      "Epoch 104/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0097 - a_or_hat_loss: 0.0015 - a_ex_hat_loss: 0.0016 - p_or_hat_loss: 7.5136e-04 - p_ex_hat_loss: 7.6323e-04 - q_ex_hat_loss: 0.0027 - q_or_hat_loss: 0.0024 - v_or_hat_loss: 1.1896e-05 - v_ex_hat_loss: 1.6788e-05 - val_loss: 0.0132 - val_a_or_hat_loss: 0.0019 - val_a_ex_hat_loss: 0.0021 - val_p_or_hat_loss: 0.0013 - val_p_ex_hat_loss: 0.0016 - val_q_ex_hat_loss: 0.0033 - val_q_or_hat_loss: 0.0029 - val_v_or_hat_loss: 9.2465e-06 - val_v_ex_hat_loss: 1.6177e-05\n",
      "Epoch 105/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0098 - a_or_hat_loss: 0.0016 - a_ex_hat_loss: 0.0016 - p_or_hat_loss: 7.9919e-04 - p_ex_hat_loss: 8.0696e-04 - q_ex_hat_loss: 0.0027 - q_or_hat_loss: 0.0023 - v_or_hat_loss: 1.2723e-05 - v_ex_hat_loss: 1.7739e-05 - val_loss: 0.0119 - val_a_or_hat_loss: 0.0017 - val_a_ex_hat_loss: 0.0019 - val_p_or_hat_loss: 9.5805e-04 - val_p_ex_hat_loss: 0.0011 - val_q_ex_hat_loss: 0.0035 - val_q_or_hat_loss: 0.0027 - val_v_or_hat_loss: 1.1255e-05 - val_v_ex_hat_loss: 1.5721e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0094 - a_or_hat_loss: 0.0015 - a_ex_hat_loss: 0.0016 - p_or_hat_loss: 7.2353e-04 - p_ex_hat_loss: 7.3704e-04 - q_ex_hat_loss: 0.0026 - q_or_hat_loss: 0.0023 - v_or_hat_loss: 1.2254e-05 - v_ex_hat_loss: 1.7461e-05 - val_loss: 0.0135 - val_a_or_hat_loss: 0.0018 - val_a_ex_hat_loss: 0.0020 - val_p_or_hat_loss: 8.6315e-04 - val_p_ex_hat_loss: 9.4711e-04 - val_q_ex_hat_loss: 0.0041 - val_q_or_hat_loss: 0.0038 - val_v_or_hat_loss: 2.1169e-05 - val_v_ex_hat_loss: 2.9708e-05\n",
      "Epoch 107/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0096 - a_or_hat_loss: 0.0015 - a_ex_hat_loss: 0.0016 - p_or_hat_loss: 7.5867e-04 - p_ex_hat_loss: 7.4126e-04 - q_ex_hat_loss: 0.0026 - q_or_hat_loss: 0.0023 - v_or_hat_loss: 1.2920e-05 - v_ex_hat_loss: 1.7706e-05 - val_loss: 0.0128 - val_a_or_hat_loss: 0.0019 - val_a_ex_hat_loss: 0.0019 - val_p_or_hat_loss: 0.0011 - val_p_ex_hat_loss: 0.0012 - val_q_ex_hat_loss: 0.0037 - val_q_or_hat_loss: 0.0029 - val_v_or_hat_loss: 1.0705e-05 - val_v_ex_hat_loss: 1.5309e-05\n",
      "Epoch 108/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0096 - a_or_hat_loss: 0.0015 - a_ex_hat_loss: 0.0016 - p_or_hat_loss: 7.7354e-04 - p_ex_hat_loss: 7.8530e-04 - q_ex_hat_loss: 0.0026 - q_or_hat_loss: 0.0023 - v_or_hat_loss: 1.2835e-05 - v_ex_hat_loss: 1.7964e-05 - val_loss: 0.0123 - val_a_or_hat_loss: 0.0020 - val_a_ex_hat_loss: 0.0019 - val_p_or_hat_loss: 0.0010 - val_p_ex_hat_loss: 9.2033e-04 - val_q_ex_hat_loss: 0.0035 - val_q_or_hat_loss: 0.0029 - val_v_or_hat_loss: 9.5558e-06 - val_v_ex_hat_loss: 1.4666e-05\n",
      "Epoch 109/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0096 - a_or_hat_loss: 0.0015 - a_ex_hat_loss: 0.0016 - p_or_hat_loss: 7.6903e-04 - p_ex_hat_loss: 7.8440e-04 - q_ex_hat_loss: 0.0026 - q_or_hat_loss: 0.0023 - v_or_hat_loss: 1.2122e-05 - v_ex_hat_loss: 1.7523e-05 - val_loss: 0.0111 - val_a_or_hat_loss: 0.0018 - val_a_ex_hat_loss: 0.0019 - val_p_or_hat_loss: 8.3155e-04 - val_p_ex_hat_loss: 8.4032e-04 - val_q_ex_hat_loss: 0.0031 - val_q_or_hat_loss: 0.0027 - val_v_or_hat_loss: 1.3372e-05 - val_v_ex_hat_loss: 2.2707e-05\n",
      "Epoch 110/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0093 - a_or_hat_loss: 0.0015 - a_ex_hat_loss: 0.0015 - p_or_hat_loss: 7.4214e-04 - p_ex_hat_loss: 7.5758e-04 - q_ex_hat_loss: 0.0025 - q_or_hat_loss: 0.0022 - v_or_hat_loss: 1.1251e-05 - v_ex_hat_loss: 1.6615e-05 - val_loss: 0.0110 - val_a_or_hat_loss: 0.0018 - val_a_ex_hat_loss: 0.0019 - val_p_or_hat_loss: 8.1873e-04 - val_p_ex_hat_loss: 8.5521e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0026 - val_v_or_hat_loss: 1.3881e-05 - val_v_ex_hat_loss: 2.0238e-05\n",
      "Epoch 111/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0093 - a_or_hat_loss: 0.0015 - a_ex_hat_loss: 0.0015 - p_or_hat_loss: 7.3341e-04 - p_ex_hat_loss: 7.3976e-04 - q_ex_hat_loss: 0.0026 - q_or_hat_loss: 0.0022 - v_or_hat_loss: 1.1268e-05 - v_ex_hat_loss: 1.6417e-05 - val_loss: 0.0121 - val_a_or_hat_loss: 0.0018 - val_a_ex_hat_loss: 0.0019 - val_p_or_hat_loss: 0.0011 - val_p_ex_hat_loss: 0.0010 - val_q_ex_hat_loss: 0.0033 - val_q_or_hat_loss: 0.0029 - val_v_or_hat_loss: 9.4014e-06 - val_v_ex_hat_loss: 1.5687e-05\n",
      "Epoch 112/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0092 - a_or_hat_loss: 0.0015 - a_ex_hat_loss: 0.0015 - p_or_hat_loss: 7.2331e-04 - p_ex_hat_loss: 7.2273e-04 - q_ex_hat_loss: 0.0025 - q_or_hat_loss: 0.0022 - v_or_hat_loss: 1.1557e-05 - v_ex_hat_loss: 1.6848e-05 - val_loss: 0.0124 - val_a_or_hat_loss: 0.0019 - val_a_ex_hat_loss: 0.0022 - val_p_or_hat_loss: 0.0011 - val_p_ex_hat_loss: 0.0011 - val_q_ex_hat_loss: 0.0032 - val_q_or_hat_loss: 0.0029 - val_v_or_hat_loss: 1.6261e-05 - val_v_ex_hat_loss: 2.3134e-05\n",
      "Epoch 113/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0095 - a_or_hat_loss: 0.0015 - a_ex_hat_loss: 0.0015 - p_or_hat_loss: 8.1064e-04 - p_ex_hat_loss: 7.7378e-04 - q_ex_hat_loss: 0.0026 - q_or_hat_loss: 0.0022 - v_or_hat_loss: 1.2455e-05 - v_ex_hat_loss: 1.7256e-05 - val_loss: 0.0111 - val_a_or_hat_loss: 0.0017 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 9.8472e-04 - val_p_ex_hat_loss: 9.6035e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0027 - val_v_or_hat_loss: 9.8349e-06 - val_v_ex_hat_loss: 1.4893e-05\n",
      "Epoch 114/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0093 - a_or_hat_loss: 0.0015 - a_ex_hat_loss: 0.0015 - p_or_hat_loss: 7.4996e-04 - p_ex_hat_loss: 7.3413e-04 - q_ex_hat_loss: 0.0026 - q_or_hat_loss: 0.0022 - v_or_hat_loss: 1.1401e-05 - v_ex_hat_loss: 1.6449e-05 - val_loss: 0.0113 - val_a_or_hat_loss: 0.0017 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 8.2296e-04 - val_p_ex_hat_loss: 8.5569e-04 - val_q_ex_hat_loss: 0.0033 - val_q_or_hat_loss: 0.0028 - val_v_or_hat_loss: 1.3142e-05 - val_v_ex_hat_loss: 1.4594e-05\n",
      "Epoch 115/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0091 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0015 - p_or_hat_loss: 7.2743e-04 - p_ex_hat_loss: 7.1496e-04 - q_ex_hat_loss: 0.0025 - q_or_hat_loss: 0.0022 - v_or_hat_loss: 1.0613e-05 - v_ex_hat_loss: 1.5696e-05 - val_loss: 0.0116 - val_a_or_hat_loss: 0.0017 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 0.0012 - val_p_ex_hat_loss: 0.0010 - val_q_ex_hat_loss: 0.0031 - val_q_or_hat_loss: 0.0028 - val_v_or_hat_loss: 1.2243e-05 - val_v_ex_hat_loss: 1.4970e-05\n",
      "Epoch 116/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0090 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0015 - p_or_hat_loss: 7.0352e-04 - p_ex_hat_loss: 7.0617e-04 - q_ex_hat_loss: 0.0025 - q_or_hat_loss: 0.0022 - v_or_hat_loss: 1.0318e-05 - v_ex_hat_loss: 1.5198e-05 - val_loss: 0.0106 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 7.9127e-04 - val_p_ex_hat_loss: 8.6184e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0026 - val_v_or_hat_loss: 7.8277e-06 - val_v_ex_hat_loss: 1.0953e-05\n",
      "Epoch 117/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0092 - a_or_hat_loss: 0.0015 - a_ex_hat_loss: 0.0015 - p_or_hat_loss: 7.2837e-04 - p_ex_hat_loss: 7.3806e-04 - q_ex_hat_loss: 0.0025 - q_or_hat_loss: 0.0022 - v_or_hat_loss: 1.1739e-05 - v_ex_hat_loss: 1.6800e-05 - val_loss: 0.0109 - val_a_or_hat_loss: 0.0017 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 8.6009e-04 - val_p_ex_hat_loss: 7.9801e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0028 - val_v_or_hat_loss: 6.7026e-06 - val_v_ex_hat_loss: 1.4736e-05\n",
      "Epoch 118/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0090 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0015 - p_or_hat_loss: 7.0974e-04 - p_ex_hat_loss: 7.0435e-04 - q_ex_hat_loss: 0.0025 - q_or_hat_loss: 0.0022 - v_or_hat_loss: 9.8023e-06 - v_ex_hat_loss: 1.4408e-05 - val_loss: 0.0111 - val_a_or_hat_loss: 0.0018 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 8.5421e-04 - val_p_ex_hat_loss: 9.9982e-04 - val_q_ex_hat_loss: 0.0030 - val_q_or_hat_loss: 0.0026 - val_v_or_hat_loss: 1.4528e-05 - val_v_ex_hat_loss: 2.1539e-05\n",
      "Epoch 119/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0088 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0015 - p_or_hat_loss: 6.8444e-04 - p_ex_hat_loss: 6.9574e-04 - q_ex_hat_loss: 0.0024 - q_or_hat_loss: 0.0021 - v_or_hat_loss: 9.8172e-06 - v_ex_hat_loss: 1.4534e-05 - val_loss: 0.0111 - val_a_or_hat_loss: 0.0017 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 8.8275e-04 - val_p_ex_hat_loss: 8.5848e-04 - val_q_ex_hat_loss: 0.0030 - val_q_or_hat_loss: 0.0028 - val_v_or_hat_loss: 1.2968e-05 - val_v_ex_hat_loss: 1.6948e-05\n",
      "Epoch 120/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0087 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0015 - p_or_hat_loss: 6.7152e-04 - p_ex_hat_loss: 6.8393e-04 - q_ex_hat_loss: 0.0024 - q_or_hat_loss: 0.0021 - v_or_hat_loss: 1.0217e-05 - v_ex_hat_loss: 1.5093e-05 - val_loss: 0.0109 - val_a_or_hat_loss: 0.0017 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 8.8516e-04 - val_p_ex_hat_loss: 9.2494e-04 - val_q_ex_hat_loss: 0.0032 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 1.3902e-05 - val_v_ex_hat_loss: 2.1398e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0089 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0015 - p_or_hat_loss: 6.9709e-04 - p_ex_hat_loss: 6.9412e-04 - q_ex_hat_loss: 0.0024 - q_or_hat_loss: 0.0021 - v_or_hat_loss: 1.1462e-05 - v_ex_hat_loss: 1.6072e-05 - val_loss: 0.0109 - val_a_or_hat_loss: 0.0017 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 9.3526e-04 - val_p_ex_hat_loss: 0.0010 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 7.2543e-06 - val_v_ex_hat_loss: 1.3006e-05\n",
      "Epoch 122/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0087 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 6.7503e-04 - p_ex_hat_loss: 6.8671e-04 - q_ex_hat_loss: 0.0024 - q_or_hat_loss: 0.0021 - v_or_hat_loss: 9.3065e-06 - v_ex_hat_loss: 1.3795e-05 - val_loss: 0.0141 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 0.0015 - val_p_ex_hat_loss: 0.0015 - val_q_ex_hat_loss: 0.0043 - val_q_or_hat_loss: 0.0033 - val_v_or_hat_loss: 7.0159e-06 - val_v_ex_hat_loss: 1.0513e-05\n",
      "Epoch 123/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0091 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0015 - p_or_hat_loss: 7.4736e-04 - p_ex_hat_loss: 7.4645e-04 - q_ex_hat_loss: 0.0025 - q_or_hat_loss: 0.0022 - v_or_hat_loss: 1.2231e-05 - v_ex_hat_loss: 1.6803e-05 - val_loss: 0.0112 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 9.7099e-04 - val_p_ex_hat_loss: 9.6858e-04 - val_q_ex_hat_loss: 0.0032 - val_q_or_hat_loss: 0.0026 - val_v_or_hat_loss: 7.6918e-06 - val_v_ex_hat_loss: 1.1492e-05\n",
      "Epoch 124/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0088 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 6.8720e-04 - p_ex_hat_loss: 7.0849e-04 - q_ex_hat_loss: 0.0024 - q_or_hat_loss: 0.0021 - v_or_hat_loss: 9.5816e-06 - v_ex_hat_loss: 1.3965e-05 - val_loss: 0.0107 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 8.9776e-04 - val_p_ex_hat_loss: 8.6136e-04 - val_q_ex_hat_loss: 0.0030 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 8.9157e-06 - val_v_ex_hat_loss: 1.1477e-05\n",
      "Epoch 125/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0088 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 7.0997e-04 - p_ex_hat_loss: 7.1742e-04 - q_ex_hat_loss: 0.0024 - q_or_hat_loss: 0.0021 - v_or_hat_loss: 1.0077e-05 - v_ex_hat_loss: 1.4525e-05 - val_loss: 0.0115 - val_a_or_hat_loss: 0.0017 - val_a_ex_hat_loss: 0.0019 - val_p_or_hat_loss: 9.1840e-04 - val_p_ex_hat_loss: 0.0011 - val_q_ex_hat_loss: 0.0031 - val_q_or_hat_loss: 0.0028 - val_v_or_hat_loss: 2.6265e-05 - val_v_ex_hat_loss: 3.5993e-05\n",
      "Epoch 126/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0087 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 6.9342e-04 - p_ex_hat_loss: 6.8109e-04 - q_ex_hat_loss: 0.0024 - q_or_hat_loss: 0.0021 - v_or_hat_loss: 1.0327e-05 - v_ex_hat_loss: 1.4731e-05 - val_loss: 0.0117 - val_a_or_hat_loss: 0.0017 - val_a_ex_hat_loss: 0.0019 - val_p_or_hat_loss: 0.0010 - val_p_ex_hat_loss: 9.8195e-04 - val_q_ex_hat_loss: 0.0033 - val_q_or_hat_loss: 0.0028 - val_v_or_hat_loss: 8.6886e-06 - val_v_ex_hat_loss: 1.2104e-05\n",
      "Epoch 127/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0085 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 6.4185e-04 - p_ex_hat_loss: 6.4044e-04 - q_ex_hat_loss: 0.0023 - q_or_hat_loss: 0.0021 - v_or_hat_loss: 9.9183e-06 - v_ex_hat_loss: 1.4135e-05 - val_loss: 0.0118 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0017 - val_p_or_hat_loss: 0.0013 - val_p_ex_hat_loss: 0.0014 - val_q_ex_hat_loss: 0.0031 - val_q_or_hat_loss: 0.0026 - val_v_or_hat_loss: 6.2054e-06 - val_v_ex_hat_loss: 1.4786e-05\n",
      "Epoch 128/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0087 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 6.8603e-04 - p_ex_hat_loss: 7.0049e-04 - q_ex_hat_loss: 0.0024 - q_or_hat_loss: 0.0021 - v_or_hat_loss: 1.1281e-05 - v_ex_hat_loss: 1.5648e-05 - val_loss: 0.0107 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0017 - val_p_or_hat_loss: 9.6246e-04 - val_p_ex_hat_loss: 9.1017e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0026 - val_v_or_hat_loss: 2.3963e-05 - val_v_ex_hat_loss: 3.0756e-05\n",
      "Epoch 129/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0084 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 6.4621e-04 - p_ex_hat_loss: 6.4203e-04 - q_ex_hat_loss: 0.0023 - q_or_hat_loss: 0.0020 - v_or_hat_loss: 9.8137e-06 - v_ex_hat_loss: 1.3936e-05 - val_loss: 0.0112 - val_a_or_hat_loss: 0.0017 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 9.3392e-04 - val_p_ex_hat_loss: 9.3474e-04 - val_q_ex_hat_loss: 0.0032 - val_q_or_hat_loss: 0.0026 - val_v_or_hat_loss: 1.3666e-05 - val_v_ex_hat_loss: 1.6945e-05\n",
      "Epoch 130/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0084 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 6.4954e-04 - p_ex_hat_loss: 6.4274e-04 - q_ex_hat_loss: 0.0023 - q_or_hat_loss: 0.0020 - v_or_hat_loss: 9.6812e-06 - v_ex_hat_loss: 1.3990e-05 - val_loss: 0.0110 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 8.9169e-04 - val_p_ex_hat_loss: 0.0014 - val_q_ex_hat_loss: 0.0028 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 6.7270e-06 - val_v_ex_hat_loss: 9.6891e-06\n",
      "Epoch 131/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0085 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 6.6688e-04 - p_ex_hat_loss: 6.9649e-04 - q_ex_hat_loss: 0.0023 - q_or_hat_loss: 0.0020 - v_or_hat_loss: 1.0563e-05 - v_ex_hat_loss: 1.4619e-05 - val_loss: 0.0098 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 7.3141e-04 - val_p_ex_hat_loss: 7.9071e-04 - val_q_ex_hat_loss: 0.0027 - val_q_or_hat_loss: 0.0024 - val_v_or_hat_loss: 8.3174e-06 - val_v_ex_hat_loss: 1.0127e-05\n",
      "Epoch 132/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0084 - a_or_hat_loss: 0.0014 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 6.5709e-04 - p_ex_hat_loss: 6.5626e-04 - q_ex_hat_loss: 0.0023 - q_or_hat_loss: 0.0020 - v_or_hat_loss: 9.9481e-06 - v_ex_hat_loss: 1.4060e-05 - val_loss: 0.0105 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0017 - val_p_or_hat_loss: 9.1028e-04 - val_p_ex_hat_loss: 9.2226e-04 - val_q_ex_hat_loss: 0.0028 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 1.4021e-05 - val_v_ex_hat_loss: 2.1270e-05\n",
      "Epoch 133/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0085 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 6.6367e-04 - p_ex_hat_loss: 6.6202e-04 - q_ex_hat_loss: 0.0024 - q_or_hat_loss: 0.0021 - v_or_hat_loss: 9.4689e-06 - v_ex_hat_loss: 1.3411e-05 - val_loss: 0.0104 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 7.6882e-04 - val_p_ex_hat_loss: 8.5260e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 8.1706e-06 - val_v_ex_hat_loss: 1.3754e-05\n",
      "Epoch 134/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0083 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 6.4855e-04 - p_ex_hat_loss: 6.6318e-04 - q_ex_hat_loss: 0.0023 - q_or_hat_loss: 0.0020 - v_or_hat_loss: 1.0223e-05 - v_ex_hat_loss: 1.4275e-05 - val_loss: 0.0103 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0017 - val_p_or_hat_loss: 7.9459e-04 - val_p_ex_hat_loss: 7.6373e-04 - val_q_ex_hat_loss: 0.0028 - val_q_or_hat_loss: 0.0026 - val_v_or_hat_loss: 1.5319e-05 - val_v_ex_hat_loss: 2.3472e-05\n",
      "Epoch 135/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0082 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 6.2604e-04 - p_ex_hat_loss: 6.3899e-04 - q_ex_hat_loss: 0.0023 - q_or_hat_loss: 0.0020 - v_or_hat_loss: 8.6563e-06 - v_ex_hat_loss: 1.2600e-05 - val_loss: 0.0103 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0017 - val_p_or_hat_loss: 9.8845e-04 - val_p_ex_hat_loss: 8.7241e-04 - val_q_ex_hat_loss: 0.0028 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 1.3277e-05 - val_v_ex_hat_loss: 1.4781e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0085 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0014 - p_or_hat_loss: 6.8268e-04 - p_ex_hat_loss: 6.8169e-04 - q_ex_hat_loss: 0.0024 - q_or_hat_loss: 0.0020 - v_or_hat_loss: 9.8735e-06 - v_ex_hat_loss: 1.3722e-05 - val_loss: 0.0100 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 8.2617e-04 - val_p_ex_hat_loss: 7.4809e-04 - val_q_ex_hat_loss: 0.0028 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 8.3442e-06 - val_v_ex_hat_loss: 1.3857e-05\n",
      "Epoch 137/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0082 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.4586e-04 - p_ex_hat_loss: 6.5034e-04 - q_ex_hat_loss: 0.0023 - q_or_hat_loss: 0.0020 - v_or_hat_loss: 8.9795e-06 - v_ex_hat_loss: 1.2930e-05 - val_loss: 0.0106 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0018 - val_p_or_hat_loss: 8.8506e-04 - val_p_ex_hat_loss: 9.5463e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0024 - val_v_or_hat_loss: 1.2637e-05 - val_v_ex_hat_loss: 1.5777e-05\n",
      "Epoch 138/200\n",
      "782/782 [==============================] - 14s 19ms/step - loss: 0.0080 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.0769e-04 - p_ex_hat_loss: 6.0837e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 9.0233e-06 - v_ex_hat_loss: 1.2743e-05 - val_loss: 0.0109 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0017 - val_p_or_hat_loss: 0.0011 - val_p_ex_hat_loss: 0.0010 - val_q_ex_hat_loss: 0.0030 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 1.2205e-05 - val_v_ex_hat_loss: 1.6612e-05\n",
      "Epoch 139/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0081 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.3014e-04 - p_ex_hat_loss: 6.2992e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0020 - v_or_hat_loss: 1.0251e-05 - v_ex_hat_loss: 1.4133e-05 - val_loss: 0.0099 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 7.8234e-04 - val_p_ex_hat_loss: 8.2054e-04 - val_q_ex_hat_loss: 0.0027 - val_q_or_hat_loss: 0.0024 - val_v_or_hat_loss: 7.2098e-06 - val_v_ex_hat_loss: 1.0045e-05\n",
      "Epoch 140/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0081 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.4342e-04 - p_ex_hat_loss: 6.4752e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 9.7637e-06 - v_ex_hat_loss: 1.3416e-05 - val_loss: 0.0099 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 7.3896e-04 - val_p_ex_hat_loss: 7.3952e-04 - val_q_ex_hat_loss: 0.0027 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 8.0467e-06 - val_v_ex_hat_loss: 1.2998e-05\n",
      "Epoch 141/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0080 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.0558e-04 - p_ex_hat_loss: 6.1317e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 9.6477e-06 - v_ex_hat_loss: 1.3524e-05 - val_loss: 0.0099 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0017 - val_p_or_hat_loss: 7.5818e-04 - val_p_ex_hat_loss: 8.6724e-04 - val_q_ex_hat_loss: 0.0027 - val_q_or_hat_loss: 0.0024 - val_v_or_hat_loss: 4.7801e-06 - val_v_ex_hat_loss: 7.8490e-06\n",
      "Epoch 142/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0079 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.1369e-04 - p_ex_hat_loss: 6.2992e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 8.6825e-06 - v_ex_hat_loss: 1.2095e-05 - val_loss: 0.0107 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0017 - val_p_or_hat_loss: 9.5493e-04 - val_p_ex_hat_loss: 8.9270e-04 - val_q_ex_hat_loss: 0.0030 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 7.9401e-06 - val_v_ex_hat_loss: 9.6318e-06\n",
      "Epoch 143/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0080 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.2502e-04 - p_ex_hat_loss: 6.2118e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 9.5641e-06 - v_ex_hat_loss: 1.2926e-05 - val_loss: 0.0106 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0017 - val_p_or_hat_loss: 0.0010 - val_p_ex_hat_loss: 9.4959e-04 - val_q_ex_hat_loss: 0.0030 - val_q_or_hat_loss: 0.0024 - val_v_or_hat_loss: 1.0474e-05 - val_v_ex_hat_loss: 1.1997e-05\n",
      "Epoch 144/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0078 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.0794e-04 - p_ex_hat_loss: 6.0355e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 8.5290e-06 - v_ex_hat_loss: 1.2048e-05 - val_loss: 0.0098 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0017 - val_p_or_hat_loss: 8.7429e-04 - val_p_ex_hat_loss: 8.7885e-04 - val_q_ex_hat_loss: 0.0026 - val_q_or_hat_loss: 0.0023 - val_v_or_hat_loss: 6.8347e-06 - val_v_ex_hat_loss: 1.1348e-05\n",
      "Epoch 145/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0080 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.3068e-04 - p_ex_hat_loss: 6.3517e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 8.9419e-06 - v_ex_hat_loss: 1.2579e-05 - val_loss: 0.0112 - val_a_or_hat_loss: 0.0018 - val_a_ex_hat_loss: 0.0020 - val_p_or_hat_loss: 9.3326e-04 - val_p_ex_hat_loss: 9.4396e-04 - val_q_ex_hat_loss: 0.0030 - val_q_or_hat_loss: 0.0026 - val_v_or_hat_loss: 2.7913e-05 - val_v_ex_hat_loss: 2.8437e-05\n",
      "Epoch 146/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0080 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.1949e-04 - p_ex_hat_loss: 6.3319e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 9.7640e-06 - v_ex_hat_loss: 1.3145e-05 - val_loss: 0.0102 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 6.9112e-04 - val_p_ex_hat_loss: 7.7162e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0026 - val_v_or_hat_loss: 8.2531e-06 - val_v_ex_hat_loss: 1.3937e-05\n",
      "Epoch 147/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0077 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 5.7826e-04 - p_ex_hat_loss: 5.8441e-04 - q_ex_hat_loss: 0.0021 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 8.1788e-06 - v_ex_hat_loss: 1.1462e-05 - val_loss: 0.0097 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 9.0478e-04 - val_p_ex_hat_loss: 7.5101e-04 - val_q_ex_hat_loss: 0.0026 - val_q_or_hat_loss: 0.0024 - val_v_or_hat_loss: 5.7378e-06 - val_v_ex_hat_loss: 9.5170e-06\n",
      "Epoch 148/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0079 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.2924e-04 - p_ex_hat_loss: 6.2891e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 9.1242e-06 - v_ex_hat_loss: 1.2494e-05 - val_loss: 0.0108 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0017 - val_p_or_hat_loss: 8.8957e-04 - val_p_ex_hat_loss: 9.1601e-04 - val_q_ex_hat_loss: 0.0034 - val_q_or_hat_loss: 0.0023 - val_v_or_hat_loss: 1.4218e-05 - val_v_ex_hat_loss: 1.5070e-05\n",
      "Epoch 149/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0078 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 5.9633e-04 - p_ex_hat_loss: 5.9162e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 9.6261e-06 - v_ex_hat_loss: 1.3219e-05 - val_loss: 0.0103 - val_a_or_hat_loss: 0.0016 - val_a_ex_hat_loss: 0.0017 - val_p_or_hat_loss: 8.7083e-04 - val_p_ex_hat_loss: 8.9168e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0023 - val_v_or_hat_loss: 1.1395e-05 - val_v_ex_hat_loss: 1.7417e-05\n",
      "Epoch 150/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0079 - a_or_hat_loss: 0.0013 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.1330e-04 - p_ex_hat_loss: 6.0919e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 9.6802e-06 - v_ex_hat_loss: 1.3190e-05 - val_loss: 0.0101 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 7.4548e-04 - val_p_ex_hat_loss: 0.0010 - val_q_ex_hat_loss: 0.0027 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 7.1933e-06 - val_v_ex_hat_loss: 1.0328e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0078 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 5.9185e-04 - p_ex_hat_loss: 6.2233e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 8.2434e-06 - v_ex_hat_loss: 1.1259e-05 - val_loss: 0.0097 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 8.3501e-04 - val_p_ex_hat_loss: 7.7785e-04 - val_q_ex_hat_loss: 0.0026 - val_q_or_hat_loss: 0.0023 - val_v_or_hat_loss: 5.0908e-06 - val_v_ex_hat_loss: 8.2949e-06\n",
      "Epoch 152/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0077 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 5.9716e-04 - p_ex_hat_loss: 6.0762e-04 - q_ex_hat_loss: 0.0021 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 9.3644e-06 - v_ex_hat_loss: 1.2703e-05 - val_loss: 0.0097 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 7.8062e-04 - val_p_ex_hat_loss: 7.8945e-04 - val_q_ex_hat_loss: 0.0027 - val_q_or_hat_loss: 0.0023 - val_v_or_hat_loss: 6.4008e-06 - val_v_ex_hat_loss: 8.9391e-06\n",
      "Epoch 153/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0078 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.1178e-04 - p_ex_hat_loss: 6.1733e-04 - q_ex_hat_loss: 0.0021 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 9.6077e-06 - v_ex_hat_loss: 1.2649e-05 - val_loss: 0.0090 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 6.2065e-04 - val_p_ex_hat_loss: 6.7085e-04 - val_q_ex_hat_loss: 0.0025 - val_q_or_hat_loss: 0.0023 - val_v_or_hat_loss: 5.6835e-06 - val_v_ex_hat_loss: 8.9909e-06\n",
      "Epoch 154/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0077 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 5.8248e-04 - p_ex_hat_loss: 5.8256e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 8.5035e-06 - v_ex_hat_loss: 1.1799e-05 - val_loss: 0.0098 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 7.9852e-04 - val_p_ex_hat_loss: 7.5411e-04 - val_q_ex_hat_loss: 0.0027 - val_q_or_hat_loss: 0.0023 - val_v_or_hat_loss: 7.7943e-06 - val_v_ex_hat_loss: 1.0021e-05\n",
      "Epoch 155/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0078 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.1290e-04 - p_ex_hat_loss: 6.1420e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 8.5058e-06 - v_ex_hat_loss: 1.1588e-05 - val_loss: 0.0114 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 8.0689e-04 - val_p_ex_hat_loss: 0.0012 - val_q_ex_hat_loss: 0.0032 - val_q_or_hat_loss: 0.0030 - val_v_or_hat_loss: 1.0043e-05 - val_v_ex_hat_loss: 1.2305e-05\n",
      "Epoch 156/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0076 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.5922e-04 - p_ex_hat_loss: 5.8283e-04 - q_ex_hat_loss: 0.0021 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 8.7080e-06 - v_ex_hat_loss: 1.1846e-05 - val_loss: 0.0095 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 8.9887e-04 - val_p_ex_hat_loss: 8.3282e-04 - val_q_ex_hat_loss: 0.0025 - val_q_or_hat_loss: 0.0022 - val_v_or_hat_loss: 7.0132e-06 - val_v_ex_hat_loss: 1.0037e-05\n",
      "Epoch 157/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0074 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.4694e-04 - p_ex_hat_loss: 5.5481e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0018 - v_or_hat_loss: 8.4515e-06 - v_ex_hat_loss: 1.1605e-05 - val_loss: 0.0092 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 7.4424e-04 - val_p_ex_hat_loss: 6.8252e-04 - val_q_ex_hat_loss: 0.0025 - val_q_or_hat_loss: 0.0022 - val_v_or_hat_loss: 4.9594e-06 - val_v_ex_hat_loss: 7.6664e-06\n",
      "Epoch 158/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0076 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 5.9465e-04 - p_ex_hat_loss: 5.9944e-04 - q_ex_hat_loss: 0.0021 - q_or_hat_loss: 0.0018 - v_or_hat_loss: 8.6017e-06 - v_ex_hat_loss: 1.1800e-05 - val_loss: 0.0105 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0017 - val_p_or_hat_loss: 7.0625e-04 - val_p_ex_hat_loss: 7.4665e-04 - val_q_ex_hat_loss: 0.0035 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 7.4983e-06 - val_v_ex_hat_loss: 9.0316e-06\n",
      "Epoch 159/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0076 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 5.7555e-04 - p_ex_hat_loss: 5.7087e-04 - q_ex_hat_loss: 0.0021 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 8.7096e-06 - v_ex_hat_loss: 1.1696e-05 - val_loss: 0.0093 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 6.5899e-04 - val_p_ex_hat_loss: 7.1275e-04 - val_q_ex_hat_loss: 0.0025 - val_q_or_hat_loss: 0.0024 - val_v_or_hat_loss: 1.0947e-05 - val_v_ex_hat_loss: 1.3976e-05\n",
      "Epoch 160/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0078 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0013 - p_or_hat_loss: 6.1905e-04 - p_ex_hat_loss: 6.3150e-04 - q_ex_hat_loss: 0.0022 - q_or_hat_loss: 0.0019 - v_or_hat_loss: 1.0236e-05 - v_ex_hat_loss: 1.3201e-05 - val_loss: 0.0100 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 6.2910e-04 - val_p_ex_hat_loss: 6.4452e-04 - val_q_ex_hat_loss: 0.0027 - val_q_or_hat_loss: 0.0030 - val_v_or_hat_loss: 1.0419e-05 - val_v_ex_hat_loss: 1.4525e-05\n",
      "Epoch 161/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0074 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.4656e-04 - p_ex_hat_loss: 5.5642e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0018 - v_or_hat_loss: 9.0366e-06 - v_ex_hat_loss: 1.1908e-05 - val_loss: 0.0095 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 8.8233e-04 - val_p_ex_hat_loss: 9.1115e-04 - val_q_ex_hat_loss: 0.0023 - val_q_or_hat_loss: 0.0024 - val_v_or_hat_loss: 5.8323e-06 - val_v_ex_hat_loss: 9.9426e-06\n",
      "Epoch 162/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0076 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.9160e-04 - p_ex_hat_loss: 6.1049e-04 - q_ex_hat_loss: 0.0021 - q_or_hat_loss: 0.0018 - v_or_hat_loss: 8.0851e-06 - v_ex_hat_loss: 1.0869e-05 - val_loss: 0.0098 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 7.2911e-04 - val_p_ex_hat_loss: 8.7447e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0024 - val_v_or_hat_loss: 6.9213e-06 - val_v_ex_hat_loss: 1.1425e-05\n",
      "Epoch 163/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0074 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.6017e-04 - p_ex_hat_loss: 5.6720e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0018 - v_or_hat_loss: 8.8039e-06 - v_ex_hat_loss: 1.1878e-05 - val_loss: 0.0090 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 7.1029e-04 - val_p_ex_hat_loss: 7.0678e-04 - val_q_ex_hat_loss: 0.0026 - val_q_or_hat_loss: 0.0021 - val_v_or_hat_loss: 8.3561e-06 - val_v_ex_hat_loss: 1.3102e-05\n",
      "Epoch 164/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0072 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.3465e-04 - p_ex_hat_loss: 5.4253e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0018 - v_or_hat_loss: 7.8065e-06 - v_ex_hat_loss: 1.0760e-05 - val_loss: 0.0097 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 8.4244e-04 - val_p_ex_hat_loss: 7.6983e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0022 - val_v_or_hat_loss: 1.1210e-05 - val_v_ex_hat_loss: 1.8580e-05\n",
      "Epoch 165/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0073 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.4817e-04 - p_ex_hat_loss: 5.6283e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0018 - v_or_hat_loss: 9.0058e-06 - v_ex_hat_loss: 1.1798e-05 - val_loss: 0.0085 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 6.7106e-04 - val_p_ex_hat_loss: 6.3463e-04 - val_q_ex_hat_loss: 0.0023 - val_q_or_hat_loss: 0.0020 - val_v_or_hat_loss: 4.5468e-06 - val_v_ex_hat_loss: 7.5195e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0073 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.6921e-04 - p_ex_hat_loss: 5.6897e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0018 - v_or_hat_loss: 7.6104e-06 - v_ex_hat_loss: 1.0487e-05 - val_loss: 0.0088 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 6.5367e-04 - val_p_ex_hat_loss: 6.4305e-04 - val_q_ex_hat_loss: 0.0025 - val_q_or_hat_loss: 0.0022 - val_v_or_hat_loss: 4.7312e-06 - val_v_ex_hat_loss: 7.0199e-06\n",
      "Epoch 167/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0073 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.5227e-04 - p_ex_hat_loss: 5.5631e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0018 - v_or_hat_loss: 8.0249e-06 - v_ex_hat_loss: 1.0742e-05 - val_loss: 0.0101 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 8.2535e-04 - val_p_ex_hat_loss: 9.4415e-04 - val_q_ex_hat_loss: 0.0028 - val_q_or_hat_loss: 0.0026 - val_v_or_hat_loss: 9.6644e-06 - val_v_ex_hat_loss: 1.1860e-05\n",
      "Epoch 168/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0073 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.5665e-04 - p_ex_hat_loss: 5.5782e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0018 - v_or_hat_loss: 8.4866e-06 - v_ex_hat_loss: 1.1270e-05 - val_loss: 0.0095 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 6.9727e-04 - val_p_ex_hat_loss: 7.5959e-04 - val_q_ex_hat_loss: 0.0024 - val_q_or_hat_loss: 0.0026 - val_v_or_hat_loss: 1.7333e-05 - val_v_ex_hat_loss: 1.8117e-05\n",
      "Epoch 169/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0072 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.4904e-04 - p_ex_hat_loss: 5.6692e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0018 - v_or_hat_loss: 8.7492e-06 - v_ex_hat_loss: 1.1594e-05 - val_loss: 0.0099 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 9.0657e-04 - val_p_ex_hat_loss: 8.4635e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0023 - val_v_or_hat_loss: 5.0831e-06 - val_v_ex_hat_loss: 7.3316e-06\n",
      "Epoch 170/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0071 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.1981e-04 - p_ex_hat_loss: 5.2144e-04 - q_ex_hat_loss: 0.0019 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 8.6685e-06 - v_ex_hat_loss: 1.1513e-05 - val_loss: 0.0095 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 7.9850e-04 - val_p_ex_hat_loss: 7.3269e-04 - val_q_ex_hat_loss: 0.0026 - val_q_or_hat_loss: 0.0022 - val_v_or_hat_loss: 1.7657e-05 - val_v_ex_hat_loss: 2.2790e-05\n",
      "Epoch 171/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0071 - a_or_hat_loss: 0.0012 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.2847e-04 - p_ex_hat_loss: 5.2764e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 7.7534e-06 - v_ex_hat_loss: 1.0576e-05 - val_loss: 0.0092 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 6.4947e-04 - val_p_ex_hat_loss: 8.4125e-04 - val_q_ex_hat_loss: 0.0025 - val_q_or_hat_loss: 0.0023 - val_v_or_hat_loss: 6.9311e-06 - val_v_ex_hat_loss: 8.8895e-06\n",
      "Epoch 172/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0071 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.2664e-04 - p_ex_hat_loss: 5.3882e-04 - q_ex_hat_loss: 0.0019 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 8.3935e-06 - v_ex_hat_loss: 1.1172e-05 - val_loss: 0.0092 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 8.0914e-04 - val_p_ex_hat_loss: 8.1141e-04 - val_q_ex_hat_loss: 0.0025 - val_q_or_hat_loss: 0.0021 - val_v_or_hat_loss: 1.1553e-05 - val_v_ex_hat_loss: 1.8197e-05\n",
      "Epoch 173/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0071 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.2396e-04 - p_ex_hat_loss: 5.2420e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 7.8998e-06 - v_ex_hat_loss: 1.0743e-05 - val_loss: 0.0089 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 7.0375e-04 - val_p_ex_hat_loss: 7.4687e-04 - val_q_ex_hat_loss: 0.0026 - val_q_or_hat_loss: 0.0021 - val_v_or_hat_loss: 8.3103e-06 - val_v_ex_hat_loss: 1.3690e-05\n",
      "Epoch 174/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0071 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.3821e-04 - p_ex_hat_loss: 5.3198e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 8.1971e-06 - v_ex_hat_loss: 1.1075e-05 - val_loss: 0.0089 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 7.5796e-04 - val_p_ex_hat_loss: 6.5300e-04 - val_q_ex_hat_loss: 0.0028 - val_q_or_hat_loss: 0.0020 - val_v_or_hat_loss: 6.7732e-06 - val_v_ex_hat_loss: 8.5542e-06\n",
      "Epoch 175/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0071 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.4785e-04 - p_ex_hat_loss: 5.5968e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 7.7541e-06 - v_ex_hat_loss: 1.0439e-05 - val_loss: 0.0090 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 6.7234e-04 - val_p_ex_hat_loss: 7.1740e-04 - val_q_ex_hat_loss: 0.0025 - val_q_or_hat_loss: 0.0022 - val_v_or_hat_loss: 6.7846e-06 - val_v_ex_hat_loss: 8.3755e-06\n",
      "Epoch 176/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0070 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.1899e-04 - p_ex_hat_loss: 5.1749e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 7.8321e-06 - v_ex_hat_loss: 1.0378e-05 - val_loss: 0.0088 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 6.5426e-04 - val_p_ex_hat_loss: 6.7715e-04 - val_q_ex_hat_loss: 0.0025 - val_q_or_hat_loss: 0.0021 - val_v_or_hat_loss: 7.0943e-06 - val_v_ex_hat_loss: 1.0605e-05\n",
      "Epoch 177/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0070 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.2119e-04 - p_ex_hat_loss: 5.2210e-04 - q_ex_hat_loss: 0.0019 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 7.8671e-06 - v_ex_hat_loss: 1.0512e-05 - val_loss: 0.0084 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 5.8799e-04 - val_p_ex_hat_loss: 5.9963e-04 - val_q_ex_hat_loss: 0.0024 - val_q_or_hat_loss: 0.0021 - val_v_or_hat_loss: 5.1441e-06 - val_v_ex_hat_loss: 7.4677e-06\n",
      "Epoch 178/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0069 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 5.1440e-04 - p_ex_hat_loss: 5.1825e-04 - q_ex_hat_loss: 0.0019 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 7.1626e-06 - v_ex_hat_loss: 9.7779e-06 - val_loss: 0.0085 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 7.0571e-04 - val_p_ex_hat_loss: 6.7228e-04 - val_q_ex_hat_loss: 0.0023 - val_q_or_hat_loss: 0.0020 - val_v_or_hat_loss: 6.1551e-06 - val_v_ex_hat_loss: 8.6168e-06\n",
      "Epoch 179/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0072 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.7408e-04 - p_ex_hat_loss: 5.7987e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 8.1773e-06 - v_ex_hat_loss: 1.0723e-05 - val_loss: 0.0099 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 7.4480e-04 - val_p_ex_hat_loss: 8.5857e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0024 - val_v_or_hat_loss: 6.2158e-06 - val_v_ex_hat_loss: 9.7114e-06\n",
      "Epoch 180/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0070 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 5.3730e-04 - p_ex_hat_loss: 5.3487e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 7.1206e-06 - v_ex_hat_loss: 9.7229e-06 - val_loss: 0.0083 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 6.9950e-04 - val_p_ex_hat_loss: 6.6575e-04 - val_q_ex_hat_loss: 0.0023 - val_q_or_hat_loss: 0.0019 - val_v_or_hat_loss: 4.0923e-06 - val_v_ex_hat_loss: 6.3673e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0069 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 5.2100e-04 - p_ex_hat_loss: 5.3773e-04 - q_ex_hat_loss: 0.0019 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 8.3807e-06 - v_ex_hat_loss: 1.1151e-05 - val_loss: 0.0087 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 6.0880e-04 - val_p_ex_hat_loss: 6.0302e-04 - val_q_ex_hat_loss: 0.0024 - val_q_or_hat_loss: 0.0022 - val_v_or_hat_loss: 2.2743e-05 - val_v_ex_hat_loss: 2.2303e-05\n",
      "Epoch 182/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0069 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 5.0941e-04 - p_ex_hat_loss: 5.0694e-04 - q_ex_hat_loss: 0.0019 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 8.2231e-06 - v_ex_hat_loss: 1.0605e-05 - val_loss: 0.0092 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 6.6995e-04 - val_p_ex_hat_loss: 7.2137e-04 - val_q_ex_hat_loss: 0.0026 - val_q_or_hat_loss: 0.0023 - val_v_or_hat_loss: 9.7534e-06 - val_v_ex_hat_loss: 1.5547e-05\n",
      "Epoch 183/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0071 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0012 - p_or_hat_loss: 5.3721e-04 - p_ex_hat_loss: 5.5110e-04 - q_ex_hat_loss: 0.0020 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 7.9966e-06 - v_ex_hat_loss: 1.0686e-05 - val_loss: 0.0090 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 9.3120e-04 - val_p_ex_hat_loss: 7.5390e-04 - val_q_ex_hat_loss: 0.0024 - val_q_or_hat_loss: 0.0021 - val_v_or_hat_loss: 7.5805e-06 - val_v_ex_hat_loss: 1.1023e-05\n",
      "Epoch 184/200\n",
      "782/782 [==============================] - 14s 19ms/step - loss: 0.0069 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 5.1179e-04 - p_ex_hat_loss: 5.2152e-04 - q_ex_hat_loss: 0.0019 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 7.4446e-06 - v_ex_hat_loss: 9.9240e-06 - val_loss: 0.0084 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 6.5652e-04 - val_p_ex_hat_loss: 6.1043e-04 - val_q_ex_hat_loss: 0.0022 - val_q_or_hat_loss: 0.0022 - val_v_or_hat_loss: 5.2024e-06 - val_v_ex_hat_loss: 7.6837e-06\n",
      "Epoch 185/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0067 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 4.9366e-04 - p_ex_hat_loss: 5.0018e-04 - q_ex_hat_loss: 0.0018 - q_or_hat_loss: 0.0016 - v_or_hat_loss: 7.3496e-06 - v_ex_hat_loss: 9.8034e-06 - val_loss: 0.0086 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 6.9299e-04 - val_p_ex_hat_loss: 6.6224e-04 - val_q_ex_hat_loss: 0.0024 - val_q_or_hat_loss: 0.0021 - val_v_or_hat_loss: 5.1289e-06 - val_v_ex_hat_loss: 8.8295e-06\n",
      "Epoch 186/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0069 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 5.0947e-04 - p_ex_hat_loss: 5.2195e-04 - q_ex_hat_loss: 0.0019 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 7.8350e-06 - v_ex_hat_loss: 1.0343e-05 - val_loss: 0.0083 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 6.1641e-04 - val_p_ex_hat_loss: 6.4365e-04 - val_q_ex_hat_loss: 0.0023 - val_q_or_hat_loss: 0.0020 - val_v_or_hat_loss: 4.0232e-06 - val_v_ex_hat_loss: 6.1863e-06\n",
      "Epoch 187/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0067 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 4.8978e-04 - p_ex_hat_loss: 5.1432e-04 - q_ex_hat_loss: 0.0018 - q_or_hat_loss: 0.0016 - v_or_hat_loss: 6.9719e-06 - v_ex_hat_loss: 9.3844e-06 - val_loss: 0.0085 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0013 - val_p_or_hat_loss: 7.7744e-04 - val_p_ex_hat_loss: 7.0654e-04 - val_q_ex_hat_loss: 0.0023 - val_q_or_hat_loss: 0.0022 - val_v_or_hat_loss: 9.2059e-06 - val_v_ex_hat_loss: 1.4088e-05\n",
      "Epoch 188/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0068 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 5.0969e-04 - p_ex_hat_loss: 5.2024e-04 - q_ex_hat_loss: 0.0019 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 7.8994e-06 - v_ex_hat_loss: 1.0320e-05 - val_loss: 0.0101 - val_a_or_hat_loss: 0.0015 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 6.9576e-04 - val_p_ex_hat_loss: 8.8570e-04 - val_q_ex_hat_loss: 0.0032 - val_q_or_hat_loss: 0.0022 - val_v_or_hat_loss: 1.1232e-05 - val_v_ex_hat_loss: 1.4308e-05\n",
      "Epoch 189/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0067 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 4.9939e-04 - p_ex_hat_loss: 5.0919e-04 - q_ex_hat_loss: 0.0019 - q_or_hat_loss: 0.0016 - v_or_hat_loss: 8.5111e-06 - v_ex_hat_loss: 1.1062e-05 - val_loss: 0.0104 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0016 - val_p_or_hat_loss: 8.1626e-04 - val_p_ex_hat_loss: 8.3289e-04 - val_q_ex_hat_loss: 0.0032 - val_q_or_hat_loss: 0.0026 - val_v_or_hat_loss: 1.0398e-05 - val_v_ex_hat_loss: 1.7478e-05\n",
      "Epoch 190/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0068 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 5.1261e-04 - p_ex_hat_loss: 5.3566e-04 - q_ex_hat_loss: 0.0019 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 7.1762e-06 - v_ex_hat_loss: 9.6412e-06 - val_loss: 0.0082 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 6.4776e-04 - val_p_ex_hat_loss: 6.1999e-04 - val_q_ex_hat_loss: 0.0023 - val_q_or_hat_loss: 0.0020 - val_v_or_hat_loss: 1.1585e-05 - val_v_ex_hat_loss: 1.5156e-05\n",
      "Epoch 191/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0066 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 4.8295e-04 - p_ex_hat_loss: 4.8782e-04 - q_ex_hat_loss: 0.0018 - q_or_hat_loss: 0.0016 - v_or_hat_loss: 7.6018e-06 - v_ex_hat_loss: 9.9802e-06 - val_loss: 0.0092 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 7.2117e-04 - val_p_ex_hat_loss: 7.5566e-04 - val_q_ex_hat_loss: 0.0031 - val_q_or_hat_loss: 0.0020 - val_v_or_hat_loss: 5.2992e-06 - val_v_ex_hat_loss: 7.2482e-06\n",
      "Epoch 192/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0067 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 5.0057e-04 - p_ex_hat_loss: 5.0765e-04 - q_ex_hat_loss: 0.0019 - q_or_hat_loss: 0.0016 - v_or_hat_loss: 6.8638e-06 - v_ex_hat_loss: 9.1894e-06 - val_loss: 0.0081 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 6.4819e-04 - val_p_ex_hat_loss: 6.6270e-04 - val_q_ex_hat_loss: 0.0022 - val_q_or_hat_loss: 0.0019 - val_v_or_hat_loss: 8.0356e-06 - val_v_ex_hat_loss: 1.0546e-05\n",
      "Epoch 193/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0065 - a_or_hat_loss: 0.0010 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 4.8434e-04 - p_ex_hat_loss: 5.0958e-04 - q_ex_hat_loss: 0.0018 - q_or_hat_loss: 0.0016 - v_or_hat_loss: 7.0528e-06 - v_ex_hat_loss: 9.3643e-06 - val_loss: 0.0095 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 6.7259e-04 - val_p_ex_hat_loss: 8.1569e-04 - val_q_ex_hat_loss: 0.0029 - val_q_or_hat_loss: 0.0025 - val_v_or_hat_loss: 9.4310e-06 - val_v_ex_hat_loss: 1.0216e-05\n",
      "Epoch 194/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0068 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 5.1470e-04 - p_ex_hat_loss: 5.3034e-04 - q_ex_hat_loss: 0.0019 - q_or_hat_loss: 0.0017 - v_or_hat_loss: 7.6929e-06 - v_ex_hat_loss: 9.9394e-06 - val_loss: 0.0081 - val_a_or_hat_loss: 0.0012 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 5.6249e-04 - val_p_ex_hat_loss: 5.9194e-04 - val_q_ex_hat_loss: 0.0023 - val_q_or_hat_loss: 0.0020 - val_v_or_hat_loss: 6.9591e-06 - val_v_ex_hat_loss: 8.5300e-06\n",
      "Epoch 195/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0066 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 4.9643e-04 - p_ex_hat_loss: 4.9215e-04 - q_ex_hat_loss: 0.0018 - q_or_hat_loss: 0.0016 - v_or_hat_loss: 7.1362e-06 - v_ex_hat_loss: 9.4860e-06 - val_loss: 0.0081 - val_a_or_hat_loss: 0.0012 - val_a_ex_hat_loss: 0.0013 - val_p_or_hat_loss: 6.7679e-04 - val_p_ex_hat_loss: 6.0459e-04 - val_q_ex_hat_loss: 0.0022 - val_q_or_hat_loss: 0.0020 - val_v_or_hat_loss: 4.4498e-06 - val_v_ex_hat_loss: 6.1959e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0066 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 4.7715e-04 - p_ex_hat_loss: 4.9043e-04 - q_ex_hat_loss: 0.0018 - q_or_hat_loss: 0.0016 - v_or_hat_loss: 6.7131e-06 - v_ex_hat_loss: 8.8004e-06 - val_loss: 0.0087 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 7.0048e-04 - val_p_ex_hat_loss: 7.5164e-04 - val_q_ex_hat_loss: 0.0026 - val_q_or_hat_loss: 0.0020 - val_v_or_hat_loss: 7.7013e-06 - val_v_ex_hat_loss: 1.2036e-05\n",
      "Epoch 197/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0066 - a_or_hat_loss: 0.0011 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 4.9058e-04 - p_ex_hat_loss: 5.0718e-04 - q_ex_hat_loss: 0.0018 - q_or_hat_loss: 0.0016 - v_or_hat_loss: 7.3712e-06 - v_ex_hat_loss: 9.6928e-06 - val_loss: 0.0084 - val_a_or_hat_loss: 0.0014 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 7.0736e-04 - val_p_ex_hat_loss: 6.8021e-04 - val_q_ex_hat_loss: 0.0023 - val_q_or_hat_loss: 0.0019 - val_v_or_hat_loss: 9.2729e-06 - val_v_ex_hat_loss: 1.2142e-05\n",
      "Epoch 198/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0065 - a_or_hat_loss: 0.0010 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 4.8124e-04 - p_ex_hat_loss: 4.9559e-04 - q_ex_hat_loss: 0.0018 - q_or_hat_loss: 0.0016 - v_or_hat_loss: 7.6565e-06 - v_ex_hat_loss: 9.9936e-06 - val_loss: 0.0082 - val_a_or_hat_loss: 0.0012 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 5.7157e-04 - val_p_ex_hat_loss: 6.0970e-04 - val_q_ex_hat_loss: 0.0023 - val_q_or_hat_loss: 0.0021 - val_v_or_hat_loss: 4.0700e-06 - val_v_ex_hat_loss: 7.1342e-06\n",
      "Epoch 199/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0064 - a_or_hat_loss: 0.0010 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 4.6229e-04 - p_ex_hat_loss: 4.8706e-04 - q_ex_hat_loss: 0.0018 - q_or_hat_loss: 0.0015 - v_or_hat_loss: 7.4310e-06 - v_ex_hat_loss: 9.9798e-06 - val_loss: 0.0087 - val_a_or_hat_loss: 0.0012 - val_a_ex_hat_loss: 0.0015 - val_p_or_hat_loss: 7.5610e-04 - val_p_ex_hat_loss: 6.9988e-04 - val_q_ex_hat_loss: 0.0023 - val_q_or_hat_loss: 0.0022 - val_v_or_hat_loss: 8.6914e-06 - val_v_ex_hat_loss: 1.3447e-05\n",
      "Epoch 200/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0065 - a_or_hat_loss: 0.0010 - a_ex_hat_loss: 0.0011 - p_or_hat_loss: 4.9092e-04 - p_ex_hat_loss: 4.9373e-04 - q_ex_hat_loss: 0.0018 - q_or_hat_loss: 0.0016 - v_or_hat_loss: 8.7965e-06 - v_ex_hat_loss: 1.1103e-05 - val_loss: 0.0083 - val_a_or_hat_loss: 0.0013 - val_a_ex_hat_loss: 0.0014 - val_p_or_hat_loss: 6.5435e-04 - val_p_ex_hat_loss: 7.2251e-04 - val_q_ex_hat_loss: 0.0022 - val_q_or_hat_loss: 0.0021 - val_v_or_hat_loss: 4.3700e-06 - val_v_ex_hat_loss: 6.9485e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10a8b919220>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leapNet.train(nb_iter=200,\n",
    "              train_dataset=neurips_benchmark2.train_dataset,\n",
    "              val_dataset=neurips_benchmark2.val_dataset\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f9b4b",
   "metadata": {},
   "source": [
    "save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fa76653",
   "metadata": {},
   "outputs": [],
   "source": [
    "leapNet.save(path_save)\n",
    "leapNet.save_metadata(path_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ead58",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d40eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A log file including some verifications is created at root directory with the name logs.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milad\\Anaconda3\\envs\\RTE\\lib\\site-packages\\lightsim2grid\\_aux_add_trafo.py:33: UserWarning: There were some Nan in the pp_net.trafo[\"tap_neutral\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_neutral\\\"], they have been replaced by 0\")\n",
      "C:\\Users\\Milad\\Anaconda3\\envs\\RTE\\lib\\site-packages\\lightsim2grid\\_aux_add_trafo.py:41: UserWarning: There were some Nan in the pp_net.trafo[\"tap_step_percent\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_step_percent\\\"], they have been replaced by 0\")\n",
      "C:\\Users\\Milad\\Anaconda3\\envs\\RTE\\lib\\site-packages\\lightsim2grid\\_aux_add_trafo.py:46: UserWarning: There were some Nan in the pp_net.trafo[\"tap_pos\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_pos\\\"], they have been replaced by 0\")\n"
     ]
    }
   ],
   "source": [
    "leapNet_metrics_per_dataset = neurips_benchmark2.evaluate_augmented_simulator(leapNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8321db64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE90\n",
      "{'a_ex': 0.00813160707427244,\n",
      " 'a_or': 0.007979104685545902,\n",
      " 'p_ex': 0.008166597600051348,\n",
      " 'p_or': 0.007834906470320726,\n",
      " 'q_ex': 0.019858207009851324,\n",
      " 'q_or': 0.024461410089700493,\n",
      " 'v_ex': 0.002699542428681486,\n",
      " 'v_or': 0.0019494662495950477}\n",
      "MAPE\n",
      "{'a_ex': 0.02127385212817802,\n",
      " 'a_or': 0.01952513799981881,\n",
      " 'p_ex': 0.01680612547798812,\n",
      " 'p_or': 0.0163945119652645,\n",
      " 'q_ex': 0.019394653354758803,\n",
      " 'q_or': 0.02161757008069546,\n",
      " 'v_ex': 0.001953868020788054,\n",
      " 'v_or': 0.0015720658390373377}\n",
      "MAE\n",
      "{'a_ex': 4.521700859069824,\n",
      " 'a_or': 3.0080628395080566,\n",
      " 'p_ex': 0.192916139960289,\n",
      " 'p_or': 0.19933074712753296,\n",
      " 'q_ex': 0.10750918090343475,\n",
      " 'q_or': 0.12753885984420776,\n",
      " 'v_ex': 0.12099482119083405,\n",
      " 'v_or': 0.11611560732126236}\n",
      "NRMSE\n",
      "{'a_ex': 0.0046461401507258415,\n",
      " 'a_or': 0.004516526125371456,\n",
      " 'p_ex': 0.002776965033262968,\n",
      " 'p_or': 0.0027368508744984865,\n",
      " 'q_ex': 0.003519367892295122,\n",
      " 'q_or': 0.004789002705365419,\n",
      " 'v_ex': 0.0034246505238115788,\n",
      " 'v_or': 0.0036250713746994734}\n"
     ]
    }
   ],
   "source": [
    "ML_metrics = 0\n",
    "print(\"MAPE90\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test\"][ML_metrics][\"mape90\"])\n",
    "print(\"MAPE\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test\"][ML_metrics][\"mape_avg\"])\n",
    "print(\"MAE\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test\"][ML_metrics][\"MAE_avg\"])\n",
    "print(\"NRMSE\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test\"][ML_metrics][\"NRMSE_avg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14ab98bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pos\n",
      "0.000275\n",
      "0.000475\n",
      "voltage pos\n",
      "loss\n",
      "0.303455\n",
      "line_status\n",
      "{'a_ex_not_null': 0.0,\n",
      " 'a_or_not_null': 0.0,\n",
      " 'a_violations': 0.0,\n",
      " 'p_ex_not_null': 0.0,\n",
      " 'p_or_not_null': 0,\n",
      " 'p_violations': 0.0,\n",
      " 'q_ex_not_null': 0.0,\n",
      " 'q_or_not_null': 0.0,\n",
      " 'q_violations': 0.0}\n"
     ]
    }
   ],
   "source": [
    "PhysicCompliances = 1\n",
    "print(\"current pos\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test\"][PhysicCompliances][\"BasicVerifications\"][\"currents\"][\"a_or\"][\"Violation_proportion\"])\n",
    "pprint(leapNet_metrics_per_dataset[\"test\"][PhysicCompliances][\"BasicVerifications\"][\"currents\"][\"a_ex\"][\"Violation_proportion\"])\n",
    "print(\"voltage pos\")\n",
    "#pprint(leapNet_metrics_per_dataset[\"test\"][1][\"BasicVerifications\"][\"voltages\"][\"v_or\"][\"Violation_proportion\"])\n",
    "#pprint(leapNet_metrics_per_dataset[\"test\"][1][\"BasicVerifications\"][\"voltages\"][\"v_ex\"][\"Violation_proportion\"])\n",
    "\n",
    "print(\"loss\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test\"][PhysicCompliances][\"BasicVerifications\"][\"loss\"][\"violation_proportion\"])\n",
    "print(\"line_status\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test\"][PhysicCompliances][\"BasicVerifications\"][\"line_status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d587670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE90\n",
      "{'a_ex': 0.1382224791906293,\n",
      " 'a_or': 0.11992010528588419,\n",
      " 'p_ex': 0.14609953917458304,\n",
      " 'p_or': 0.171091161287175,\n",
      " 'q_ex': 0.32924807814673646,\n",
      " 'q_or': 0.28565893688075833,\n",
      " 'v_ex': 0.013703405666633545,\n",
      " 'v_or': 0.009956155747893181}\n",
      "MAPE\n",
      "{'a_ex': 0.247116519280815,\n",
      " 'a_or': 0.24847870687428136,\n",
      " 'p_ex': 0.3008344258064579,\n",
      " 'p_or': 0.26384691793750564,\n",
      " 'q_ex': 0.37309383561380977,\n",
      " 'q_or': 0.3830213923133384,\n",
      " 'v_ex': 0.009438520209545163,\n",
      " 'v_or': 0.0074657264398096685}\n",
      "MAE\n",
      "{'a_ex': 68.26426696777344,\n",
      " 'a_or': 44.68369674682617,\n",
      " 'p_ex': 3.4000396728515625,\n",
      " 'p_or': 3.157888889312744,\n",
      " 'q_ex': 1.9745080471038818,\n",
      " 'q_or': 1.9132009744644165,\n",
      " 'v_ex': 0.5168433785438538,\n",
      " 'v_or': 0.5385361313819885}\n",
      "NRMSE\n",
      "{'a_ex': 0.06371472030878067,\n",
      " 'a_or': 0.062345314770936966,\n",
      " 'p_ex': 0.04826875030994415,\n",
      " 'p_or': 0.04869859665632248,\n",
      " 'q_ex': 0.0653589740395546,\n",
      " 'q_or': 0.06821189820766449,\n",
      " 'v_ex': 0.020316803827881813,\n",
      " 'v_or': 0.02507242187857628}\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPE90\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test_ood_topo\"][ML_metrics][\"mape90\"])\n",
    "print(\"MAPE\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test_ood_topo\"][ML_metrics][\"mape_avg\"])\n",
    "print(\"MAE\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test_ood_topo\"][ML_metrics][\"MAE_avg\"])\n",
    "print(\"NRMSE\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test_ood_topo\"][ML_metrics][\"NRMSE_avg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a1a7c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pos\n",
      "0.004475\n",
      "0.013125\n",
      "voltage pos\n",
      "loss\n",
      "0.457525\n",
      "line_status\n",
      "{'a_ex_not_null': 0.0,\n",
      " 'a_or_not_null': 0.0,\n",
      " 'a_violations': 0.0,\n",
      " 'p_ex_not_null': 0.0,\n",
      " 'p_or_not_null': 0,\n",
      " 'p_violations': 0.0,\n",
      " 'q_ex_not_null': 0.0,\n",
      " 'q_or_not_null': 0.0,\n",
      " 'q_violations': 0.0}\n"
     ]
    }
   ],
   "source": [
    "PhysicCompliances = 1\n",
    "print(\"current pos\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test_ood_topo\"][PhysicCompliances][\"BasicVerifications\"][\"currents\"][\"a_or\"][\"Violation_proportion\"])\n",
    "pprint(leapNet_metrics_per_dataset[\"test_ood_topo\"][PhysicCompliances][\"BasicVerifications\"][\"currents\"][\"a_ex\"][\"Violation_proportion\"])\n",
    "print(\"voltage pos\")\n",
    "#pprint(leapNet_metrics_per_dataset[\"test_ood_topo\"][1][\"BasicVerifications\"][\"voltages\"][\"v_or\"][\"Violation_proportion\"])\n",
    "#pprint(leapNet_metrics_per_dataset[\"test_ood_topo\"][1][\"BasicVerifications\"][\"voltages\"][\"v_ex\"][\"Violation_proportion\"])\n",
    "\n",
    "print(\"loss\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test_ood_topo\"][PhysicCompliances][\"BasicVerifications\"][\"loss\"][\"violation_proportion\"])\n",
    "print(\"line_status\")\n",
    "pprint(leapNet_metrics_per_dataset[\"test_ood_topo\"][PhysicCompliances][\"BasicVerifications\"][\"line_status\"])\n",
    "#pprint(dc_metrics_per_dataset[\"test_ood_topo\"][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RTE",
   "language": "python",
   "name": "rte"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
