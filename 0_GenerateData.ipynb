{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains how to generate the datasets used for the \"Scenario 1\" of the proposed benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_benchmark = os.path.join(\"reference_data\")\n",
    "if not os.path.exists(path_benchmark):\n",
    "    os.mkdir(path_benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIPS : Learning Industrial physical simulationbenchmark suite: the power grid case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmarking platform (as it can be observed in the Figure below) is composed of three main modules: Data, Benchmarking and Evaluation. As the names suggest, each module is specialized with its own set of functions for a specific task. The Data module is the one that helps to generate different dataset and distributions providing a high flexibility using a set of parameters. Once the datasets are generated, the benchmarking platform will take as entry the generated dataset and perform a required benchmark on it. Each benchmark could be precised via the inputs and outputs required for modeling and prediction. Finally, in order to evaluate the performance of each benchmark, the Evaluation module provides a set of functions to inspect the models from different point of views (physics compliances, machine learning metrics, adaptability and readiness.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below presents the scheme of benchmarking platform which is composed of three distinct parts : \n",
    "* DataSet: Generate some dataset for training and evaluation \n",
    "* Benchmark: coordinates between different parts and allows train and evaluate an agumented simulator \n",
    "* Evaluation: once the benchmark done, it allow to evaluate the performance with respect to various point of views\n",
    "\n",
    "![title](img/Benchmarking_scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 1 : Risk assessment through contingency screening\n",
    "The first power grid benchmark tackles the step of risk assessment in power grids. The problem is to anticipate potential threats on the power grid (several hours ahead) and warn the operators accordingly. It simulates incidents (aka contingencies) involving various elements of the grid (such as the disconnection of a line/production unit), one by one. For each contingency, a risk (weakness of the grid) is detected when overloads are detected by the simulation engine on some lines. On a real grid, this scenario means running several dozens of thousands of simulations, thereby, computation time is critical, especially since this risk assessment is refreshed every 15 minutes. In this benchmark, the main physical variable we are interested in is the value of electric current in the lines (in amperes), because an overload is detected if this value exceeds a line-dependent threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create the associated benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.neurips_benchmark import NeuripsBenchmark1\n",
    "neurips_benchmark1 = NeuripsBenchmark1(path_benchmark=path_benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to generate the data, note that this step needs to be done only once (by the person in charge of releasing the benchmark).\n",
    "\n",
    "People that want to use this benchmark are after free to load the data (see the next notebook for an example)\n",
    "\n",
    "**NB** To generate the datasets, you need extra libraries (for example `grid2op`, `lightsim2grid` or `leap_net`). These libraries are not required to use this platform once the data are generated.\n",
    "\n",
    "**NB** This process takes a lot of time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting path reference_data/NeuripsBenchmark1 that might contain previous runs\n",
      "Creating path reference_data/NeuripsBenchmark1 to save the current data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 100000/100000 [11:33<00:00, 144.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the path /home/donnotben/Documents/lips/reference_data/NeuripsBenchmark1/train to store the dataset name train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 10000/10000 [00:53<00:00, 187.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the path /home/donnotben/Documents/lips/reference_data/NeuripsBenchmark1/val to store the dataset name val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 10000/10000 [01:39<00:00, 100.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the path /home/donnotben/Documents/lips/reference_data/NeuripsBenchmark1/test to store the dataset name test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_ood_topo: 100%|██████████| 10000/10000 [05:28<00:00, 30.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the path /home/donnotben/Documents/lips/reference_data/NeuripsBenchmark1/test_ood_topo to store the dataset name test_ood_topo\n"
     ]
    }
   ],
   "source": [
    "neurips_benchmark1.generate(nb_sample_train=int(1e5),\n",
    "                            nb_sample_val=int(1e4),\n",
    "                            nb_sample_test=int(1e4),\n",
    "                            nb_sample_test_ood_topo=int(1e4),\n",
    "                           )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
